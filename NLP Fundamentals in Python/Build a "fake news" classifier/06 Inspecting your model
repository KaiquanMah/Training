#part 1
Improving the model
What are possible next steps you could take to improve the model?
Tweaking alpha levels.
Trying a new classification model.
Training on a larger dataset.
Improving text preprocessing.



#part 2
#Improving your model
#Your job in this exercise is to test a few different alpha levels using the Tfidf vectors to determine if there is a better performing combination.
#The training and test sets have been created, and tfidf_vectorizer, tfidf_train, and tfidf_test have been computed.


#Create a list of alphas to try using np.arange(). Values should range from 0 to 1 with steps of 0.1.
# Create the list of alphas: alphas
alphas = np.arange(0, 1, 0.1)


#Create a function train_and_predict() that takes in one argument: alpha. The function should:
# Define train_and_predict()
def train_and_predict(alpha):
    # Instantiate the classifier: nb_classifier
    nb_classifier = MultinomialNB(alpha=alpha)
    # Fit to the training data
    nb_classifier.fit(tfidf_train, y_train)
    # Predict the labels: pred
    pred = nb_classifier.predict(tfidf_test)
    # Compute accuracy: score
    score = metrics.accuracy_score(y_test, pred)
    return score

# Iterate over the alphas and print the corresponding score
for alpha in alphas:
    print('Alpha: ', alpha)
    print('Score: ', train_and_predict(alpha))
    print('\n')

<script.py> output:
    Alpha:  0.0
    Score:  0.8813964610234337
    
    
    Alpha:  0.1
    Score:  0.8976566236250598
    
    
    Alpha:  0.2
    Score:  0.8938307030129125
    
    
    Alpha:  0.30000000000000004
    Score:  0.8900047824007652
    
    
    Alpha:  0.4
    Score:  0.8857006217120995
    
    
    Alpha:  0.5
    Score:  0.8842659014825442
    
    
    Alpha:  0.6000000000000001
    Score:  0.874701099952176
    
    
    Alpha:  0.7000000000000001
    Score:  0.8703969392635102
    
    
    Alpha:  0.8
    Score:  0.8660927785748446
    
    
    Alpha:  0.9
    Score:  0.8589191774270684
    




#part 3
#Inspecting your model
Now that you have built a "fake news" classifier, you'll investigate what it has learned. You can map the important vector weights back to actual words using some simple inspection techniques.
You have your well performing tfidf Naive Bayes classifier available as nb_classifier, and the vectors as tfidf_vectorizer.

#Save the class labels as class_labels by accessing the .classes_ attribute of nb_classifier.
# Get the class labels: class_labels
class_labels = nb_classifier.classes_
#array(['FAKE', 'REAL'], dtype='<U4')


#Extract the features using the .get_feature_names() method of tfidf_vectorizer.
# Extract the features: feature_names
feature_names = tfidf_vectorizer.get_feature_names()



#Create a zipped array of the classifier coefficients with the feature names and sort them by the coefficients. To do this, first use zip() with the arguments nb_classifier.coef_[0] and feature_names. Then, use sorted() on this.
# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights
feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))

[(-12.641778440826338, '0000'),
 (-12.641778440826338, '000035'),
 (-12.641778440826338, '0001'),
 (-12.641778440826338, '0001pt'),
 (-12.641778440826338, '000km'),
...,
 (-12.641778440826338, '9ucbbewnz2'),
 (-12.641778440826338, '_000_17b2908ff9fa45188d243fd49aaeeb2dhrcofficecom_'),
 (-12.641778440826338, '____'),
 (-12.641778440826338, '_blank'),
 (-12.641778440826338, '_derosa'),
 (-12.641778440826338, '_future'),
 (-12.641778440826338, '_nytimes'),
 (-12.641778440826338, '_obama_says_he_didn'),
 (-12.641778440826338, '_rt_op_edge'),
 (-12.641778440826338, '_strauss'),
 (-12.641778440826338, '_work'),
 (-12.641778440826338, 'a04c'),
 (-12.641778440826338, 'a092299ryazanbomb'),
 (-12.641778440826338, 'a0eyximi8g'),
 (-12.641778440826338, 'a1mleov0kl'),
 (-12.641778440826338, 'a2'),
 (-12.641778440826338, 'a3'),
...,
(-12.641778440826338, 'adaptogen'),
 ...]



# Print the first class label and the top 20 feat_with_weights entries
print(class_labels[0], feat_with_weights[:20])

<script.py> output:
    FAKE [(-12.641778440826338, '0000'), (-12.641778440826338, '000035'), (-12.641778440826338, '0001'), (-12.641778440826338, '0001pt'), (-12.641778440826338, '000km'), (-12.641778440826338, '0011'), (-12.641778440826338, '006s'), (-12.641778440826338, '007'), (-12.641778440826338, '007s'), (-12.641778440826338, '008s'), (-12.641778440826338, '0099'), (-12.641778440826338, '00am'), (-12.641778440826338, '00p'), (-12.641778440826338, '00pm'), (-12.641778440826338, '014'), (-12.641778440826338, '015'), (-12.641778440826338, '018'), (-12.641778440826338, '01am'), (-12.641778440826338, '020'), (-12.641778440826338, '023')]


# Print the second class label and the bottom 20 feat_with_weights entries
print(class_labels[1], feat_with_weights[-20:])

<script.py> output:
    REAL [(-6.790929954967984, 'states'), (-6.765360557845786, 'rubio'), (-6.751044290367751, 'voters'), (-6.701050756752027, 'house'), (-6.695547793099875, 'republicans'), (-6.6701912490429685, 'bush'), (-6.661945235816139, 'percent'), (-6.589623788689862, 'people'), (-6.559670340096453, 'new'), (-6.489892292073901, 'party'), (-6.452319082422527, 'cruz'), (-6.452076515575875, 'state'), (-6.397696648238072, 'republican'), (-6.376343060363355, 'campaign'), (-6.324397735392007, 'president'), (-6.2546017970213645, 'sanders'), (-6.144621899738043, 'obama'), (-5.756817248152807, 'clinton'), (-5.596085785733112, 'said'), (-5.357523914504495, 'trump')]
    
    
