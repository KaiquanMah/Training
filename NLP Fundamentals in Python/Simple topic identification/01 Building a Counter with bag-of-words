#part 1
#Bag-of-words picker
#It's time for a quick check on your understanding of bag-of-words. Which of the below options, with basic nltk tokenization, map the bag-of-words for the following text?
#"The cat is in the box. The cat box."

#('The', 2), ('box', 2), ('.', 2), ('cat', 2), ('is', 1), ('in', 1), ('the', 1)


#part 2
#Building a Counter with bag-of-words
#In this exercise, you'll build your first (in this course) bag-of-words counter using a Wikipedia article, which has been pre-loaded as article. Try doing the bag-of-words without looking at the full article text, and guessing what the topic is! If you'd like to peek at the title at the end, we've included it as article_title. Note that this article text has had very little preprocessing from the raw Wikipedia database entry.
#word_tokenize has been imported for you.


# Import Counter from collections
from collections import Counter

#Use word_tokenize() to split the article into tokens.
# Tokenize the article: tokens
tokens = word_tokenize(article)

#Use a list comprehension with t as the iterator variable to convert all the tokens into lowercase. The .lower() method converts text into lowercase.
# Convert the tokens into lowercase: lower_tokens
lower_tokens = [t.lower() for t in tokens]

# Create a Counter with the lowercase tokens: bow_simple
bow_simple = Counter(lower_tokens)

#Use the .most_common() method of bow_simple to print the 10 most common tokens.
# Print the 10 most common tokens
print(bow_simple.most_common(10))
