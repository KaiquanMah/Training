#source - https://docs.microsoft.com/en-us/learn/modules/register-and-deploy-model-with-amls/2-deploy-model
#Deploying a model as a real-time service
#deploy a model as a real-time web service to several kinds of compute target, 
#including local compute, 
#an Azure Machine Learning compute instance, 
#an Azure Container Instance (ACI), 
#an Azure Kubernetes Service (AKS) cluster, 
#an Azure Function, or 
#an Internet of Things (IoT) module. 

#Azure Machine Learning uses containers as a deployment mechanism, packaging the model and the code to use it as an image that can be deployed to a container in your chosen compute target.
#Deployment to a local service, a compute instance, or an ACI is a good choice for testing and development. 
#For production, you should deploy to a target that meets the specific performance, scalability, and security needs of your application architecture.

#To deploy a model as a real-time inferencing service, you must perform the following tasks:
1. Register a trained model
2. Define an inference configuration
3. Define a deployment configuration
4. Deploy the model











1. Register a trained model
#After successfully training a model, you must register it in your Azure Machine Learning workspace. 
#Your real-time service will then be able to load the model when required.
#To register a model from a local file, you can use the register method of the Model object as shown here:
from azureml.core import Model

classification_model = Model.register(workspace=ws,
                       model_name='classification_model',
                       model_path='model.pkl', # local path
                       description='A classification model')





#Alternatively, if you have a reference to the Run used to train the model, you can use its register_model method as shown here:
run.register_model( model_name='classification_model',
                    model_path='outputs/model.pkl', # run outputs path
                    description='A classification model')











2. Define an inference configuration
#The model will be deployed as a service that consists of:
-A script to load the model and return predictions for submitted data.
-An environment in which the script will be run.
#You must therefore define the script and environment for the service.


#step 1
#Creating an entry script
#Create the entry script (sometimes referred to as the scoring script) for the service as a Python (.py) file. 

#It must include two functions:
-init(): Called when the service is initialized.
-run(raw_data): Called when new data is submitted to the service.
#Typically, you use the init function to load the model from the model registry, and 
#use the run function to generate predictions from the input data. 

import json
import joblib
import numpy as np
from azureml.core.model import Model

# Called when the service is loaded
def init():
    global model
    # Get the path to the registered model file and load it
    model_path = Model.get_model_path('classification_model')
    model = joblib.load(model_path)

# Called when a request is received
def run(raw_data):
    # Get the input data as a numpy array
    data = np.array(json.loads(raw_data)['data'])
    # Get a prediction from the model
    predictions = model.predict(data)
    # Return the predictions as any JSON serializable format
    return predictions.tolist()








#step 2
#Creating an environment
#Your service requires a Python environment in which to run the entry script, which you can configure using Conda configuration file. 
#An easy way to create this file is to use a CondaDependencies class to create a default environment (which includes the azureml-defaults package and commonly-used packages like numpy and pandas), 
#add any other required packages, and then 
#serialize the environment to a string and save it:
from azureml.core.conda_dependencies import CondaDependencies

# Add the dependencies for your model
myenv = CondaDependencies()
myenv.add_conda_package("scikit-learn")

# Save the environment config as a .yml file
env_file = 'service_files/env.yml'
with open(env_file,"w") as f:
    #serialize_to_string
    f.write(myenv.serialize_to_string())
print("Saved dependency info in", env_file)




#step 3
#Combining the script and environment in an InferenceConfig
#After creating the entry script and environment configuration file, you can combine them in an InferenceConfig for the service like this:
from azureml.core.model import InferenceConfig

classifier_inference_config = InferenceConfig(runtime= "python",
                                              source_directory = 'service_files',
                                              entry_script="score.py",
                                              conda_file="env.yml")
























3. Define a deployment configuration
#Now that you have the entry script and environment, you need to configure the compute to which the service will be deployed. 
#If you are deploying to an AKS cluster, you must create the cluster and a compute target for it before deploying:
from azureml.core.compute import ComputeTarget, AksCompute

cluster_name = 'aks-cluster'
compute_config = AksCompute.provisioning_configuration(location='eastus')
production_cluster = ComputeTarget.create(ws, cluster_name, compute_config)
production_cluster.wait_for_completion(show_output=True)





#With the compute target created, you can now define the deployment configuration, 
#which sets the target-specific compute specification for the containerized deployment:
from azureml.core.webservice import AksWebservice

classifier_deploy_config = AksWebservice.deploy_configuration(cpu_cores = 1,
                                                              memory_gb = 1)
                                                              
                                                              
                                                              

#ACI = AZ CONTAINER INSTANCE
#The code to configure an ACI deployment is similar, 
#except that you do not need to explicitly create an ACI compute target, and 
#you must use the deploy_configuration class from the azureml.core.webservice.AciWebservice namespace. 

#LOCAL DOCKER
#Similarly, you can use the azureml.core.webservice.LocalWebservice namespace to configure a local Docker-based service.

#AZ FUNCTION
#To deploy a model to an Azure Function, you do not need to create a deployment configuration. 
#Instead, you need to package the model based on the type of function trigger you want to use. 
#EXTRA READING - https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-functions
























4. Deploy the model
#After all of the configuration is prepared, you can deploy the model. 
#The easiest way to do this is to call the deploy method of the Model class, like this:
from azureml.core.model import Model

model = ws.models['classification_model']
service = Model.deploy(workspace=ws,
                       name = 'classifier-service',
                       models = [model],
                       inference_config = classifier_inference_config,
                       deployment_config = classifier_deploy_config,
                       #For ACI or local services, you can omit the deployment_target parameter (or set it to None).
                       deployment_target = production_cluster)
service.wait_for_deployment(show_output = True)
#EXTRA READING - https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where?tabs=azcli













































#Consuming a real-time inferencing service
#After deploying a real-time service, you can consume it from client applications to predict labels for new data cases.

#method 1
Using the Azure Machine Learning SDK
#For testing, you can use the Azure Machine Learning SDK to call a web service through 
#the run method of a WebService object 
#that references the deployed service. 

#Typically, you SEND INPUT DATA to the run method in JSON format with the following structure:
#JSON
{
  "data":[
      [0.1,2.3,4.1,2.0], // 1st case
      [0.2,1.8,3.9,2.1],  // 2nd case,
      ...
  ]
}




#The response from the run method is a JSON collection with a prediction for each case that was submitted in the data. 
#The following code sample calls a service and displays the response:
#Python
import json

#INPUT DATA
# An array of new data cases
x_new = [[0.1,2.3,4.1,2.0],
         [0.2,1.8,3.9,2.1]]

#ARRAY TO JSON
# Convert the array to a serializable list in a JSON document
json_data = json.dumps({"data": x_new})

# Call the web service, passing the input data
response = service.run(input_data = json_data)

# Get the predictions
predictions = json.loads(response)

# Print the predicted class for each case.
for i in range(len(x_new)):
    print (x_new[i]), predictions[i] )
    
    
    
    
    
    
    
    
    
    
    
    
#method 2   
#Using a REST endpoint
#In production, most client applications will not include the Azure Machine Learning SDK, 
#and will consume the service through its REST interface. 

#You can determine the endpoint of a deployed service in Azure machine Learning studio, or 
#by retrieving the scoring_uri property of the Webservice object in the SDK, like this:
endpoint = service.scoring_uri
print(endpoint)

#With the endpoint known, you can 
#use a HTTP POST request with JSON data to call the service.
#Python
import requests
import json

#INPUT DATA
# An array of new data cases
x_new = [[0.1,2.3,4.1,2.0],
         [0.2,1.8,3.9,2.1]]

#ARRAY TO JSON
# Convert the array to a serializable list in a JSON document
json_data = json.dumps({"data": x_new})

#DIFF FROM METHOD 1
# Set the content type in the request headers
request_headers = { 'Content-Type':'application/json' }

#DIFF FROM METHOD 1
#HTTP POST REQUEST
# Call the service
response = requests.post(url = endpoint,
                         data = json_data,
                         headers = request_headers)

# Get the predictions from the JSON response
predictions = json.loads(response.json())

# Print the predicted class for each case.
for i in range(len(x_new)):
    print (x_new[i]), predictions[i] )
    
    








#TOPUP METHOD 2
#Authentication
#In production, you will likely want to restrict access to your services by applying authentication. 

#There are two kinds of authentication you can use:
Key: Requests are authenticated by specifying the key associated with the service.
Token: Requests are authenticated by providing a JSON Web Token (JWT).

#By default, authentication is disabled for ACI services, and 
#set to key-based authentication for AKS services (for which primary and secondary keys are automatically generated). 
#You can optionally configure an AKS service to use token-based authentication (which is not supported for ACI services).
#SO AZ CONTAINER INSTANCES DONT SUPPORT AUTHENTICATION AT ALL
#AZ KUBERNETES SERVICES SUPPORTS BOTH KEY + (JAVA WEB) TOKEN AUTHENTICATION

#KEY
#Assuming you have an authenticated session established with the workspace, you can 
#retrieve the keys for a service by using the get_keys method of the WebService object associated with the service:
#Python
primary_key, secondary_key = service.get_keys()

#TOKEN
#For token-based authentication, your client application needs to use service-principal authentication to verify its identity through Azure Active Directory (Azure AD) and 
#call the get_token method of the service to retrieve a time-limited token.

#To make an authenticated call to the service's REST endpoint, you must include the key or token in the request header like this:
Python
import requests
import json

#INPUT DATA
# An array of new data cases
x_new = [[0.1,2.3,4.1,2.0],
         [0.2,1.8,3.9,2.1]]

#ARRAY TO JSON
# Convert the array to a serializable list in a JSON document
json_data = json.dumps({"data": x_new})

#INCLUDE KEY/TOKEN IN REQUEST HEADERS
# Set the content type in the request headers
request_headers = { "Content-Type":"application/json",
                    "Authorization":"Bearer " + key_or_token }

# Call the service
response = requests.post(url = endpoint,
                         data = json_data,
                         headers = request_headers)

# Get the predictions from the JSON response
predictions = json.loads(response.json())

# Print the predicted class for each case.
for i in range(len(x_new)):
    print (x_new[i]), predictions[i] )





































#Troubleshooting service deployment
#EG. trained model, 
#the runtime environment configuration, 
#the scoring script, 
#the container image, and 
#the container host

#Check the service state
#As an initial troubleshooting step, you can check the status of a service by examining its state:
#Python
#To view the state of a service, you must use the compute-specific service type (for example AksWebservice) and not a generic WebService object.
from azureml.core.webservice import AksWebservice

# Get the deployed service
service = AksWebservice(name='classifier-service', workspace=ws)

# Check its state
#For an operational service, the state should be Healthy.
print(service.state)







#Review service logs
#If a service is not healthy, or you are experiencing errors when using it, you can review its logs:
#The logs include detailed information about the provisioning of the service, and 
#the requests it has processed
#Python
print(service.get_logs())




#Deploy to a local container
#Deployment and runtime errors can be easier to diagnose by 
#deploying the service as a container in a local Docker instance, like this:
#Python
from azureml.core.webservice import LocalWebservice

deployment_config = LocalWebservice.deploy_configuration(port=8890)
service = Model.deploy(ws, 'test-svc', [model], inference_config, deployment_config)





#You can then test the locally deployed service using the SDK:
#Python
print(service.run(input_data = json_data))



#You can then troubleshoot RUNTIME ISSUES by 
#making changes to the scoring file that is referenced in the inference configuration, and 
#reloading the service without redeploying it (something you can only do with a local service):
#Python
service.reload()
print(service.run(input_data = json_data))


















