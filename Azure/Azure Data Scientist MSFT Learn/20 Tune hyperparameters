#source - https://docs.microsoft.com/en-us/learn/modules/tune-hyperparameters-with-azure-machine-learning/2-search-space
#Define a hyperparameter search space.
#Configure hyperparameter sampling.
#Select an early-termination policy.
#Run a hyperparameter tuning experiment.






#Defining a search space
#To define a search space for hyperparameter tuning, create a dictionary with the appropriate parameter expression for each named hyperparameter. 
#For example, the following search space indicates that the batch_size hyperparameter can have the value 16, 32, or 64, and 
#the learning_rate hyperparameter can have any value from a normal distribution with a mean of 10 and a standard deviation of 3.
Python
from azureml.train.hyperdrive import choice, normal

param_space = {
                 '--batch_size': choice(16, 32, 64),
                 '--learning_rate': normal(10, 3)
              }


















#Configuring sampling
#The specific values used in a hyperparameter tuning run depend on the type of sampling used.

#method 1 - DISCRETE HYPERPARAM DISTRIBUTIONS ONLY
#Grid sampling
#Grid sampling can only be employed when all hyperparameters are discrete, and is 
#used to try every possible combination of parameters in the search space.

#For example, in the following code example, grid sampling is used to try every possible combination of discrete batch_size and learning_rate value:
#Python
from azureml.train.hyperdrive import GridParameterSampling, choice

param_space = {
                 '--batch_size': choice(16, 32, 64),
                 '--learning_rate': choice(0.01, 0.1, 1.0)
              }

param_sampling = GridParameterSampling(param_space)








#method 2 - DISCRETE ONLY, CONTINUOUS ONLY OR DISCRETE + CONTINUOUS HYPERPARAM DISTRIBUTIONS
#Random sampling
#Random sampling is used to randomly select a value for each hyperparameter, which can be a mix of discrete and continuous values as shown in the following code example:
#Python
from azureml.train.hyperdrive import RandomParameterSampling, choice, normal

param_space = {
                 '--batch_size': choice(16, 32, 64),
                 '--learning_rate': normal(10, 3)
              }

param_sampling = RandomParameterSampling(param_space)







#method 3 - CHOICE, UNIFORM, QUNIFORM HYPERPARAM DISTRIBUTIONS
#You can only use Bayesian sampling with choice, uniform, and quniform parameter expressions, and you can't combine it with an early-termination policy.
#Bayesian sampling
#Bayesian sampling chooses hyperparameter values based on the Bayesian optimization algorithm, which 
#tries to select parameter combinations that will result in improved performance from the previous selection. 
#The following code example shows how to configure Bayesian sampling:
#Python
from azureml.train.hyperdrive import BayesianParameterSampling, choice, uniform

param_space = {
                 '--batch_size': choice(16, 32, 64),
                 '--learning_rate': uniform(0.5, 0.1)
              }

param_sampling = BayesianParameterSampling(param_space)






























#Configuring early termination policy
#With a sufficiently large hyperparameter search space, it could take many iterations (child runs) to try every possible combination. 
#Typically, you set a maximum number of iterations, but this could still result in a large number of runs that don't result in a better model than a combination that has already been tried.
#To help prevent wasting time, you can set an early termination policy that abandons runs that are unlikely to produce a better result than previously completed runs. 
#The early termination policy is evaluated at an evaluation_interval you specify, based on each time the target performance metric is logged. 
#You can also set a delay_evaluation parameter to avoid evaluating the policy until a minimum number of iterations have been completed.
#Early termination is particularly useful for deep learning scenarios where a deep neural network (DNN) is trained iteratively over a number of epochs. The training script can report the target metric after each epoch, and if the run is significantly underperforming previous runs after the same number of intervals, it can be abandoned.



#type 1
#Bandit policy
#You can use a bandit policy to stop a run 
#if the target performance metric UNDERPERFORMS THE BEST RUN so far BY A SPECIFIED MARGIN.

#This example applies the policy for every iteration after the first five, and 
#abandons runs where the reported target metric is 0.2 or more worse than the best performing run after the same number of intervals.
Python
from azureml.train.hyperdrive import BanditPolicy

#MARGIN = slack_amount
early_termination_policy = BanditPolicy(slack_amount = 0.2,
                                        #after 1st 5 runs, evaluate each run
                                        evaluation_interval=1,
                                        #evaluate only after the 1st 5 runs
                                        delay_evaluation=5)
                                        

#You can also apply a bandit policy using a slack factor, which compares the performance metric as a ratio 
#rather than an absolute value.








#type 2
#Median stopping policy
#A median stopping policy abandons runs where the target performance metric is WORSE THAN THE MEDIAN OF THE RUNNING AVERAGES FOR ALL RUNS.

#3-step process
#step 1 - take the CUMULATIVE/RUNNING AVG at each run
#step 2 - get the MEDIAN RUNNING AVG across all runs
#step 3 - compare CURRENT RUNNING AVG vs MEDIAN RUNNING AVG
#extra reading - https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.medianstoppingpolicy?view=azure-ml-py

#Python
from azureml.train.hyperdrive import MedianStoppingPolicy

early_termination_policy = MedianStoppingPolicy(evaluation_interval=1,
                                                delay_evaluation=5)






#Truncation selection policy
#A truncation selection policy CANCELS THE LOWEST PERFORMING X% OF RUNS AT EACH EVALUATION INTERVAL 
#based on the truncation_percentage value you specify for X.
#Python
from azureml.train.hyperdrive import TruncationSelectionPolicy

early_termination_policy = TruncationSelectionPolicy(truncation_percentage=10,
                                                     evaluation_interval=1,
                                                     delay_evaluation=5)
































#Running a hyperparameter tuning experiment
#In Azure Machine Learning, you can tune hyperparameters by running a hyperdrive experiment.



#Creating a training script for hyperparameter tuning
#To run a hyperdrive experiment, you need to create a training script just the way you would do for any other training experiment, except that your script must:
-Include AN ARGUMENT FOR EACH HYPERPARAMETER you want to vary.
-LOG the target performance METRIC. This enables the hyperdrive run to evaluate the performance of the child runs it initiates, and identify the one that produces the best performing model.

#For example, the following example script trains a logistic regression model using a --regularization argument to set the regularization rate hyperparameter, and 
#logs the accuracy metric with the name Accuracy:
#wPython
import argparse
import joblib
from azureml.core import Run
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

# Get regularization hyperparameter
parser = argparse.ArgumentParser()
parser.add_argument('--regularization', type=float, dest='reg_rate', default=0.01)
args = parser.parse_args()
reg = args.reg_rate

# Get the experiment run context
run = Run.get_context()

# load the training dataset
data = run.input_datasets['training_data'].to_pandas_dataframe()

# Separate features and labels, and split for training/validatiom
X = data[['feature1','feature2','feature3','feature4']].values
y = data['label'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)

# Train a logistic regression model with the reg hyperparameter
#Note that in the Scikit-Learn LogisticRegression class, C is the inverse of the regularization rate; hence C=1/reg.
model = LogisticRegression(C=1/reg, solver="liblinear").fit(X_train, y_train)

# calculate and log accuracy
y_hat = model.predict(X_test)
acc = np.average(y_hat == y_test)
run.log('Accuracy', np.float(acc))

# Save the trained model
os.makedirs('outputs', exist_ok=True)
joblib.dump(value=model, filename='outputs/model.pkl')

run.complete()
 Note











#Configuring and running a hyperdrive experiment
#To prepare the hyperdrive experiment, you must use a HyperDriveConfig object to configure the experiment run, as shown in the following example code:
#Python
from azureml.core import Experiment
from azureml.train.hyperdrive import HyperDriveConfig, PrimaryMetricGoal

# Assumes ws, script_config and param_sampling are already defined
hyperdrive = HyperDriveConfig(run_config=script_config,
                              hyperparameter_sampling=param_sampling,
                              policy=None,
                              primary_metric_name='Accuracy',
                              primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,
                              max_total_runs=6,
                              max_concurrent_runs=4)

experiment = Experiment(workspace = ws, name = 'hyperdrive_training')
hyperdrive_run = experiment.submit(config=hyperdrive)







#Monitoring and reviewing hyperdrive runs
#You can monitor hyperdrive experiments in Azure Machine Learning studio, or by 
#using the Jupyter Notebook's RunDetails widget.

#The experiment will initiate a child run for each hyperparameter combination to be tried, and you can retrieve the logged metrics these runs using the following code:
#Python
for child_run in run.get_children():
    print(child_run.id, child_run.get_metrics())


#You can also list all runs in DESCENDING ORDER of performance like this:
#Python
for child_run in hyperdrive_.get_children_sorted_by_primary_metric():
    print(child_run)


#To retrieve the BEST PERFORMING RUN, you can use the following code:
#Python
best_run = hyperdrive_run.get_best_run_by_primary_metric()




