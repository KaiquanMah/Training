#source - https://docs.microsoft.com/en-us/learn/modules/detect-mitigate-unfairness-models-with-azure-machine-learning/2-consider-model-fairness

#1 Measuring disparity in PREDICTIONS
One way to start evaluating the fairness of a model is to COMPARE PREDICTIONS FOR EACH GROUP WITHIN A SENSITIVE FEATURE. 
For the loan approval model, AGE is a sensitive feature that we care about, so we could split the data into subsets for each age group and compare the selection rate (the proportion of positive predictions) for each group.

Four out of eleven applicants aged 25 or younger are predicted to repay, and seven out of eleven applicants aged over 25 are predicted to repay.
Let's say we find that the model predicts that 36% of applicants aged 25 or younger will repay a loan, but it predicts successful repayments for 54% of applicants aged over 25. There's a disparity in predictions of 18%.
At first glance, this comparison seems to confirm that there's bias in the model that discriminates against younger applicants. 

However, WHEN YOU CONSIDER THE POPULATION AS A WHOLE, 
it may be that younger people generally earn less than people more established in their careers, 
have lower levels of savings and assets, and 
have a higher rate of defaulting on loans.

The important point to consider here is that just because we want to ensure fairness regarding age, 
it doesn't necessarily follow that age SHOULD NOT BE a factor in loan repayment probability. 
It's possible that in general, younger people are less likely to repay a loan than older people. 
To get the full picture, we need to look a little deeper into the predictive performance of the model for each subset of the population.








#2 Measuring disparity in PREDICTION PERFORMANCE/METRICS
#e.g. ARE THERE MORE FALSE NEGATIVES FOR 1 GROUP OF PPL
When you train a machine learning model using a supervised technique, like regression or classification, you use metrics achieved 
against hold-out validation data to evaluate the overall predictive performance of the model. 
For example, you might evaluate a classification model based on 
ACCURACY, 
PRECISION, OR 
RECALL.

To evaluate the fairness of a model, you can apply the same predictive performance metric to subsets of the data, based on the sensitive features on which your population is grouped, and measure the disparity in those metrics across the subgroups.

For example, suppose the loan approval model exhibits an overall recall metric of 0.67 - in other words, it correctly identifies 67% of cases where the applicant repaid the loan. 
The question is whether or not the model provides a similar rate of correct predictions for different age groups.
To find out, we group the data based on the sensitive feature (Age) and measure the predictive performance metric (recall) for those groups. 
Then we can compare the metric scores to determine the disparity between them.
Three out of six applicants aged 25 or younger who repaid are predicted to do so, while 
five out of six applicants aged over 25 who repaid are predicted to do so. 
Both groups include one false positive where an applicant was predicted to repay but didn't.


Let's say that we find that the recall for validation cases where the applicant is 25 or younger is 0.50, and 
recall for cases where the applicant is over 25 is 0.83. 
In other words, the model correctly identified 50% of the people in the 25 or younger age group who successfully repaid a loan (and therefore misclassified 50% of them as loan defaulters), but 
found 83% of loan repayers in the older age group (misclassifying only 17% of them). 
The disparity in prediction performance between the groups is 33%, with the model predicting significantly MORE FALSE NEGATIVES for the younger age group.


















#Potential causes of disparity
When you find a disparity between prediction rates or prediction performance metrics across sensitive feature groups, it's worth considering potential causes. These might include:
1 Data imbalance. Some groups may be OVERREPRESENTED in the training data, or the data may be skewed so that cases within a specific group aren't representative of the overall population.
2 MULTI-COLLINEARITY. Indirect correlation. The sensitive feature itself may not be predictive of the label, but there may be a hidden CORRELATION BETWEEN THE SENSITIVE FEATURE AND SOME OTHER FEATURE THAT INFLUENCES THE PREDICTION. For example, there's likely a correlation between age and credit history, and there's likely a correlation between credit history and loan defaults. If the credit history feature is not included in the training data, the training algorithm may assign a predictive weight to age without accounting for credit history, which might make a difference to loan repayment probability.
3 Societal biases. Subconscious biases in the DATA COLLECTION, PREPARATION, OR MODELING process may have influenced feature selection or other aspects of model design.










#Mitigating bias
Optimizing for fairness in a machine learning model is a sociotechnical challenge. In other words, it's not always something you can achieve purely by applying technical corrections to a training algorithm. However, there are some strategies you can adopt to mitigate bias, including:
1 Balance training and validation data. You can apply OVER-SAMPLING OR UNDER-SAMPLING techniques to balance data and use STRATIFIED SPLITTING ALGORITHMS to maintain REPRESENTATIVE PROPORTIONS FOR TRAINING AND VALIDATION.
2 Perform EXTENSIVE FEATURE SELECTION AND ENGINEERING analysis. Make sure you FULLY EXPLORE THE INTERCONNECTED CORRELATIONS in your data to try to differentiate features that are directly predictive from features that encapsulate more complex, nuanced relationships. You can use the model interpretability support in Azure Machine Learning to understand how individual features influence predictions.
#QN: WHAT IS UR MEANING OF SIGNIFICANT FEATURES? 
#QN: HOW DOES "SIGNIFICANT FEATURES" RELATE TO QUANTIFYING BIAS?
3 Evaluate models for disparity based on SIGNIFICANT FEATURES. You can't easily address the bias in a model if you can't quantify it.
4 TRADE-OFF OVERALL PREDICTIVE PERFORMANCE for the LOWER DISPARITY in predictive performance between sensitive feature groups. A model that is 99.5% accurate with comparable performance across all groups is often more desirable than a model that is 99.9% accurate but discriminates against a particular subset of cases.

#The rest of this module explores the Fairlearn package - a Python package that you can use to evaluate and mitigate unfairness in machine learning models.
  


































#Mitigate unfairness with Fairlearn
#create alternative models that apply parity constraints to produce comparable metrics across sensitive feature groups
Technique	              Description	                    Model type support
1-Exponentiated Gradient	-A reduction technique that applies a COST-MINIMIZATION approach to learning the OPTIMAL TRADE-OFF OF OVERALL PREDICTIVE PERFORMANCE AND FAIRNESS DISPARITY	              -Binary classification and regression
#WHAT IS THE COST? FALSE POS/NEG ERROR RATE?
2-Grid Search	          -A SIMPLIFIED version of the Exponentiated Gradient algorithm that works efficiently with SMALL NUMBERS OF CONSTRAINTS	                                                         -Binary classification and regression
3-Threshold Optimizer	  -A post-processing technique that APPLIES A CONSTRAINT TO AN EXISTING CLASSIFIER, transforming the prediction as appropriate	                                                   -Binary classification
#POST WHICH STEP? POST DATA TRANSFORMATION + PRE-PROCESSING? POST INITIAL MODEL CREATION + RUN?


Constraints in Fairlearn include:
-Demographic parity: Use this constraint with any of the mitigation algorithms to minimize disparity in the selection rate across SENSITIVE FEATURE GROUPS. For example, in a binary classification scenario, this constraint tries to ensure that an EQUAL NUMBER OF POSITIVE PREDICTIONS ARE MADE IN EACH GROUP.
-True positive rate parity: Use this constraint with any of the mitigation algorithms to minimize disparity in true positive rate across sensitive feature groups. For example, in a binary classification scenario, this constraint tries to ensure that each group contains a COMPARABLE RATIO OF TRUE POSITIVE PREDICTIONS.
#QN: DEMOGRAPHIC PARITY LOOKS AT ABSOLUTE TRUE POS NUMBER?
#THEN TRUE POS RATE PARITY LOOKS AT RATE OF TRUE POS wrt TOTAL POS?
-False-positive rate parity: Use this constraint with any of the mitigation algorithms to minimize disparity in false_positive_rate across sensitive feature groups. For example, in a binary classification scenario, this constraint tries to ensure that each group contains a COMPARABLE RATIO OF FALSE-POSITIVE PREDICTIONS.
-Equalized odds: Use this constraint with any of the mitigation algorithms to minimize disparity in COMBINED TRUE POSITIVE RATE AND FALSE_POSITIVE_RATE across sensitive feature groups. For example, in a binary classification scenario, this constraint tries to ensure that each group contains a comparable ratio of true positive and false-positive predictions.
-ERROR RATE parity: Use this constraint with any of the reduction-based mitigation algorithms (Exponentiated Gradient and Grid Search) to ensure that the error for each SENSITIVE FEATURE GROUP does not deviate from the OVERALL error rate by more than a specified amount.
-Bounded group loss: Use this constraint with any of the reduction-based mitigation algorithms to RESTRICT THE LOSS FOR EACH SENSITIVE FEATURE GROUP in a regression model.
#QN: WHAT DO U MEAN BY LOSS?





