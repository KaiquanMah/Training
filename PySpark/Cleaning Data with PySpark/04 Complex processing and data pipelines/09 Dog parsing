Dog parsing
You've done a considerable amount of cleanup on the initial dataset, but now need to analyze the data a bit deeper. There are several questions that have now come up about the type of dogs seen in an image and some details regarding the images. You realize that to answer these questions, you need to process the data into a specific type. Before you can use it, you'll need to create a schema / type to represent the dog details.
The joined_df DataFrame is as you last defined it, and the pyspark.sql.types have all been imported.


Select the column representing the dog details from the DataFrame and show the first 10 un-truncated rows.
Create a new schema as you've done before, using breed, start_x, start_y, end_x, and end_y as the names. Make sure to specify the proper data types for each field in the schema (any number value is an integer).


In [1]: joined_df.printSchema()
root
 |-- folder: string (nullable = true)
 |-- filename: string (nullable = true)
 |-- width: string (nullable = true)
 |-- height: string (nullable = true)
 |-- dog_list: array (nullable = true)
 |    |-- element: string (containsNull = true)
 
 

In [3]: print(joined_df.select('dog_list').show(truncate=False))
+--------------------------------------------------------+
|dog_list                                                |
+--------------------------------------------------------+
|[affenpinscher,0,9,173,298]                             |
|[Border_terrier,73,127,341,335]                         |
|[kuvasz,0,0,499,327]                                    |
|[Great_Pyrenees,124,225,403,374]                        |
|[schipperke,146,29,416,309]                             |
|[groenendael,168,0,469,374]                             |
|[Bedlington_terrier,10,12,462,332]                      |
|[Lhasa,39,1,499,373]                                    |
|[Kerry_blue_terrier,17,16,300,482]                      |
|[vizsla,112,93,276,236]                                 |
|[Eskimo_dog,43,20,472,461]                              |
|[cairn,71,2,319,302]                                    |
|[EntleBucher,307,94,515,448, EntleBucher,101,33,330,448]|
|[Japanese_spaniel,23,0,598,435]                         |
|[Great_Dane,51,36,355,332]                              |
|[Siberian_husky,7,2,235,498]                            |
|[Blenheim_spaniel,25,66,401,387]                        |
|[cairn,82,2,472,369]                                    |
|[Lhasa,141,40,423,185]                                  |
|[giant_schnauzer,227,130,339,367]                       |
+--------------------------------------------------------+
only showing top 20 rows

None

In [4]: print(joined_df.select('dog_list').show(truncate=True))
+--------------------+
|            dog_list|
+--------------------+
|[affenpinscher,0,...|
|[Border_terrier,7...|
|[kuvasz,0,0,499,327]|
|[Great_Pyrenees,1...|
|[schipperke,146,2...|
|[groenendael,168,...|
|[Bedlington_terri...|
|[Lhasa,39,1,499,373]|
|[Kerry_blue_terri...|
|[vizsla,112,93,27...|
|[Eskimo_dog,43,20...|
|[cairn,71,2,319,302]|
|[EntleBucher,307,...|
|[Japanese_spaniel...|
|[Great_Dane,51,36...|
|[Siberian_husky,7...|
|[Blenheim_spaniel...|
|[cairn,82,2,472,369]|
|[Lhasa,141,40,423...|
|[giant_schnauzer,...|
+--------------------+
only showing top 20 rows

None










# Select the dog details and show 10 untruncated rows
print(joined_df.select('dog_list').show(truncate=False))


# Define a schema type for the details in the dog list
DogType = StructType([
	StructField("breed", StringType(), False),
    StructField("start_x", IntegerType(), False),
    StructField("start_y", IntegerType(), False),
    StructField("end_x", IntegerType(), False),
    StructField("end_y", IntegerType(), False)
])



Nicely done - you'll use this schema soon to determine some details about the dogs in the data. As you've just seen, schemas can be used for importing data, but they can also be used to simplify accessing information within pre-parsed data. If you're wondering why we didn't just define a full schema for the import, the Spark CSV parser is not capable of using complex schema types using lists.

