Using broadcasting on Spark joins
Remember that table joins in Spark are split between the cluster workers. If the data is not local, various shuffle operations are required and can have a negative impact on performance. Instead, we're going to use Spark's broadcast operations to give each node a copy of the specified data.

A couple tips:
Broadcast the smaller DataFrame. The larger the DataFrame, the more time required to transfer to the worker nodes.
On small DataFrames, it may be better skip broadcasting and let Spark figure out any optimization on its own.
If you look at the query execution plan, a broadcastHashJoin indicates you've successfully configured broadcasting.

The DataFrames flights_df and airports_df are available to you.



Import the broadcast() method from pyspark.sql.functions.
Create a new DataFrame broadcast_df by joining flights_df with airports_df, using the broadcasting.
Show the query plan and consider differences from the original.

# Import the broadcast method from pyspark.sql.functions
from  pyspark.sql.functions import broadcast

# Join the flights_df and airports_df DataFrames using broadcasting
broadcast_df = flights_df.join(broadcast(airports_df), \
    flights_df["Destination Airport"] == airports_df["IATA"] )

# Show the query plan and compare against the original
broadcast_df.explain()


<script.py> output:
    == Physical Plan ==
    *(2) BroadcastHashJoin [Destination Airport#292], [IATA#309], Inner, BuildRight
    :- *(2) Project [Date (MM/DD/YYYY)#290, Flight Number#291, Destination Airport#292, Actual elapsed time (Minutes)#293]
    :  +- *(2) Filter isnotnull(Destination Airport#292)
    :     +- *(2) FileScan csv [Date (MM/DD/YYYY)#290,Flight Number#291,Destination Airport#292,Actual elapsed time (Minutes)#293] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/tmp/tmpj8y1ychh/AA_DFW_2018_Departures_Short.csv.gz], PartitionFilters: [], PushedFilters: [IsNotNull(Destination Airport)], ReadSchema: struct<Date (MM/DD/YYYY):string,Flight Number:string,Destination Airport:string,Actual elapsed ti...
    +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]))
       +- *(1) Project [AIRPORTNAME#308, IATA#309]
          +- *(1) Filter isnotnull(IATA#309)
             +- *(1) FileScan csv [AIRPORTNAME#308,IATA#309] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/tmp/tmpj8y1ychh/airportnames.txt.gz], PartitionFilters: [], PushedFilters: [IsNotNull(IATA)], ReadSchema: struct<AIRPORTNAME:string,IATA:string>
             
             

Great! You've used Spark broadcasting to improve the performance of your data operations. You should see that the query plan uses the Broadcast operations instead of the default Spark versions. You'll likely use broadcasting often with production datasets - checking the query plan will help validate your configuration without actually running the tasks.


