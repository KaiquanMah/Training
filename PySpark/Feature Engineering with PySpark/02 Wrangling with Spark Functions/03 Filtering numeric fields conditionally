Filtering numeric fields conditionally
Again, understanding the context of your data is extremely important. We want to understand what a normal range of houses sell for. Let's make sure we exclude any outlier homes that have sold for significantly more or less than the average. Here we will calculate the mean and standard deviation and use them to filer the near normal field log_SalesClosePrice.


Import mean() and stddev() from pyspark.sql.functions.
Use agg() to calculate the mean and standard deviation for 'log_SalesClosePrice' with the imported functions.
Create the upper and lower bounds by taking mean_val +/- 3 times stddev_val.
Create a where() filter for 'log_SalesClosePrice' using both low_bound and hi_bound.




from pyspark.sql.functions import mean, stddev

# Calculate values used for outlier filtering
mean_val = df.agg({'log_SalesClosePrice': 'mean'}).collect()[0][0]
stddev_val = df.agg({'log_SalesClosePrice': 'stddev'}).collect()[0][0]

In [2]: df.agg({'log_SalesClosePrice': 'mean'}).collect()
Out[2]: [Row(avg(log_SalesClosePrice)=12.364393326742066)]

In [3]: df.agg({'log_SalesClosePrice': 'mean'}).collect()[0]
Out[3]: Row(avg(log_SalesClosePrice)=12.364393326742066)

In [4]: df.agg({'log_SalesClosePrice': 'mean'}).collect()[0][0]
Out[4]: 12.364393326742066



In [5]: df.agg({'log_SalesClosePrice': 'stddev'}).collect()[0][0]
Out[5]: 0.43767049827713883






# Create three standard deviation (μ ± 3σ) lower and upper bounds for data
low_bound = mean_val - (3 * stddev_val)
hi_bound = mean_val + (3 * stddev_val)

In [7]: low_bound
Out[7]: 11.05138183191065

In [8]: hi_bound
Out[8]: 13.677404821573482




# Filter the data to fit between the lower and upper bounds
df = df.where((df['log_SalesClosePrice'] < hi_bound) & (df['log_SalesClosePrice'] > low_bound))


Awesome, now we've set proper constaints on our data. If we were to get new data, or the value for Jumbo Loans changes, we can dynamically refilter it!

