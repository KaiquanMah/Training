Loading data in PySpark shell
In PySpark, we express our computation through operations on distributed collections that are automatically parallelized across the cluster. In the previous exercise, you have seen an example of loading a list as parallelized collections and in this exercise, you'll load the data from a local file in PySpark shell.
Remember you already have a SparkContext sc and file_path variable (which is the path to the README.md file) already available in your workspace.


Load a local text file README.md in PySpark shell.
In [1]: file_path
Out[1]: '/usr/local/share/datasets/README.md'

# Load a local file into PySpark shell
lines = sc.textFile(file_path)

In [3]: lines
Out[3]: /usr/local/share/datasets/README.md MapPartitionsRDD[7] at textFile at NativeMethodAccessorImpl.java:0


Wonderful! SparkContext's textFile() method is quite powerful for creating distributed collections of unstructured data which you'll see in the next chapter.

