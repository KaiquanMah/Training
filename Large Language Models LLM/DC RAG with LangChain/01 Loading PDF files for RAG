Loading PDF files for RAG
To begin implementing Retrieval Augmented Generation (RAG), you'll first need to load the documents that the model will access. These documents can come from a variety of sources, and LangChain supports document loaders for many of them.
In this exercise, you'll use a document loader to load a PDF document containing the paper, Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks by Lewis et al. (2021). This file is available for you as 'rag_paper.pdf'.
Note: pypdf, a dependency for loading PDF documents in LangChain, has already been installed for you.


Import the appropriate class for loading PDF documents in LangChain.
Create a document loader for the 'rag_paper.pdf' document.
Load the document into memory to view the contents of the first document, or page.


# Import library
from langchain_community.document_loaders import PyPDFLoader

# Create a document loader for rag_paper.pdf
loader = PyPDFLoader('rag_paper.pdf')

# Load the document
data = loader.load()
print(data[0])


<script.py> output:
    page_content='Retrieval-Augmented Generation for
    Knowledge-Intensive NLP Tasks
    Patrick Lewis†‡, Ethan Perez⋆,
    Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,
    Mike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†
    †Facebook AI Research; ‡University College London; ⋆New York University;
    plewis@fb.com
    Abstract
    Large pre-trained language models have been shown to store factual knowledge
    in their parameters, and achieve state-of-the-art results when ﬁne-tuned on down-
    stream NLP tasks. However, their ability to access and precisely manipulate knowl-
    edge is still limited, and hence on knowledge-intensive tasks, their performance
    lags behind task-speciﬁc architectures. Additionally, providing provenance for their
    decisions and updating their world knowledge remain open research problems. Pre-
    trained models with a differentiable access mechanism to explicit non-parametric
    memory have so far been only investigated for extractive downstream tasks. We
    explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation
    (RAG) — models which combine pre-trained parametric and non-parametric mem-
    ory for language generation. We introduce RAG models where the parametric
    memory is a pre-trained seq2seq model and the non-parametric memory is a dense
    vector index of Wikipedia, accessed with a pre-trained neural retriever. We com-
    pare two RAG formulations, one which conditions on the same retrieved passages
    across the whole generated sequence, and another which can use different passages
    per token. We ﬁne-tune and evaluate our models on a wide range of knowledge-
    intensive NLP tasks and set the state of the art on three open domain QA tasks,
    outperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract
    architectures. For language generation tasks, we ﬁnd that RAG models generate
    more speciﬁc, diverse and factual language than a state-of-the-art parametric-only
    seq2seq baseline.
    1 Introduction
    Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowl-
    edge from data [47]. They can do so without any access to an external memory, as a parameterized
    implicit knowledge base [51, 52]. While this development is exciting, such models do have down-
    sides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into
    their predictions, and may produce “hallucinations” [38]. Hybrid models that combine parametric
    memory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these
    issues because knowledge can be directly revised and expanded, and accessed knowledge can be
    inspected and interpreted. REALM [ 20] and ORQA [ 31], two recently introduced models that
    combine masked language models [8] with a differentiable retriever, have shown promising results,
    arXiv:2005.11401v4  [cs.CL]  12 Apr 2021' metadata={'source': 'rag_paper.pdf', 'page': 0}




