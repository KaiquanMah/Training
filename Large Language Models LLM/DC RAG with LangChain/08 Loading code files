Loading code files
Chatbots can not only access text files, but also code files like Python (.py) and Markdown files (.md). In this exercise, you'll load a Python file containing the RAG architecture you created in Chapter 1. Let's load the file to get a reminder!
All of the classes needed to complete this exercise are already loaded.


Create a document loader from 'README.md' and load the document into memory.
# Create a document loader for README.md and load it
loader = UnstructuredMarkdownLoader("README.md")

markdown_data = loader.load()
print(markdown_data[0])

<script.py> output:
    page_content='ü¶úÔ∏èüîó LangChain
    
    ‚ö° Build context-aware reasoning applications ‚ö°
    
    Looking for the JS/TS library? Check out LangChain.js.
    
    To help you ship LangChain apps to production faster, check out LangSmith. 
    LangSmith is a unified developer platform for building, testing, and monitoring LLM applications. 
    Fill out this form to speak with our sales team.
    
    Quick Install
    
    With pip:
    bash
    pip install langchain
    
    With conda:
    bash
    conda install langchain -c conda-forge
    
    ü§î What is LangChain?
    
    LangChain is a framework for developing applications powered by large language models (LLMs).
    
    For these applications, LangChain simplifies the entire application lifecycle:
    
    Open-source libraries:  Build your applications using LangChain's open-source building blocks, components, and third-party integrations.
    Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.
    
    Productionization: Inspect, monitor, and evaluate your apps with LangSmith so that you can constantly optimize and deploy with confidence.
    
    Deployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Cloud.
    
    Open-source libraries
    
    langchain-core: Base abstractions and LangChain Expression Language.
    
    langchain-community: Third party integrations.
    
    Some integrations have been further split into partner packages that only rely on langchain-core. Examples include langchain_openai and langchain_anthropic.
    
    langchain: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.
    
    LangGraph: A library for building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph. Integrates smoothly with LangChain, but can be used without it.
    
    Productionization:
    
    LangSmith: A developer platform that lets you debug, test, evaluate, and monitor chains built on any LLM framework and seamlessly integrates with LangChain.
    
    Deployment:
    
    LangGraph Cloud: Turn your LangGraph applications into production-ready APIs and Assistants.
    
    üß± What can you build with LangChain?
    
    ‚ùì Question answering with RAG
    
    Documentation
    
    End-to-end Example: Chat LangChain and repo
    
    üß± Extracting structured output
    
    Documentation
    
    End-to-end Example: SQL Llama2 Template
    
    ü§ñ Chatbots
    
    Documentation
    
    End-to-end Example: Web LangChain (web researcher chatbot) and repo
    
    And much more! Head to the Tutorials section of the docs for more.
    
    üöÄ How does LangChain help?
    
    The main value props of the LangChain libraries are:
    1. Components: composable building blocks, tools and integrations for working with language models. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or not
    2. Off-the-shelf chains: built-in assemblages of components for accomplishing higher-level tasks
    
    Off-the-shelf chains make it easy to get started. Components make it easy to customize existing chains and build new ones.
    
    LangChain Expression Language (LCEL)
    
    LCEL is the foundation of many of LangChain's components, and is a declarative way to compose chains. LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest ‚Äúprompt + LLM‚Äù chain to the most complex chains.
    
    Overview: LCEL and its benefits
    
    Interface: The standard Runnable interface for LCEL objects
    
    Primitives: More on the primitives LCEL includes
    
    Cheatsheet: Quick overview of the most common usage patterns
    
    Components
    
    Components fall into the following modules:
    
    üìÉ Model I/O
    
    This includes prompt management, prompt optimization, a generic interface for chat models and LLMs, and common utilities for working with model outputs.
    
    üìö Retrieval
    
    Retrieval Augmented Generation involves loading data from a variety of sources, preparing it, then searching over (a.k.a. retrieving from) it for use in the generation step.
    
    ü§ñ Agents
    
    Agents allow an LLM autonomy over how a task is accomplished. Agents make decisions about which Actions to take, then take that Action, observe the result, and repeat until the task is complete. LangChain provides a standard interface for agents, along with LangGraph for building custom agents.
    
    üìñ Documentation
    
    Please see here for full documentation, which includes:
    
    Introduction: Overview of the framework and the structure of the docs.
    
    Tutorials: If you're looking to build something specific or are more of a hands-on learner, check out our tutorials. This is the best place to get started.
    
    How-to guides: Answers to ‚ÄúHow do I‚Ä¶.?‚Äù type questions. These guides are goal-oriented and concrete; they're meant to help you complete a specific task.
    
    Conceptual guide: Conceptual explanations of the key parts of the framework.
    
    API Reference: Thorough documentation of every class and method.
    
    üåê Ecosystem
    
    ü¶úüõ†Ô∏è LangSmith: Trace and evaluate your language model applications and intelligent agents to help you move from prototype to production.
    
    ü¶úüï∏Ô∏è LangGraph: Create stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it.
    
    ü¶úüèì LangServe: Deploy LangChain runnables and chains as REST APIs.
    
    üíÅ Contributing
    
    As an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.
    
    For detailed information on how to contribute, see here.
    
    üåü Contributors' metadata={'source': 'README.md'}












Create a document loader from 'rag.py' and load the document into memory.
# Create a document loader for rag.py and load it
loader = PythonLoader('rag.py')

python_data = loader.load()
print(python_data[0])




<script.py> output:
    page_content='__import__('pysqlite3')
    import sys
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_chroma import Chroma
    from langchain_openai import ChatOpenAI
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_community.document_loaders import PyPDFLoader
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.runnables import RunnablePassthrough
    from langchain_core.output_parsers import StrOutputParser
    import shutil
    import getpass
    import os
    
    if not os.environ.get("OPENAI_API_KEY"):
        os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter your OpenAI API key: ")
    
    openai_api_key = os.environ["OPENAI_API_KEY"]
    
    loader = PyPDFLoader("rag_paper.pdf")
    documents = loader.load()
    # Split the documents into manageable chunks
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_documents = text_splitter.split_documents(documents)
    
    # Initialize the all-MiniLM-L6-v2 embedding model
    embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    
    # Create a Chroma vector store and embed the chunks
    vector_store = Chroma.from_documents(
        documents=chunks, 
        embedding=embedding_model
    )
    
    prompt = ChatPromptTemplate.from_template("""
    Use the only the context provided to answer the following question. If you don't know the answer, reply that you are unsure.
    Context: {context}
    Question: {question}
    """)
    
    llm = ChatOpenAI(api_key=openai_api_key, model="gpt-4o-mini")
    
    # Convert the vector store into a retriever
    retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 2})
    
    # Create the LCEL retrieval chain
    chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    
    # Invoke the chain
    print(chain.invoke({"question": "Who are the authors?"}))' metadata={'source': 'rag.py'}













