{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a6f7481-4064-4e8c-9b94-21905d6db6d7",
   "metadata": {},
   "source": [
    "# Lesson 4: Technical Debt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e19aa2e-8c1b-43ff-b540-cea603a79b5c",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "Set the ~~MakerSuite~~ Gemini API key with the provided helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13f21190-1c61-4fb8-957d-ef5d070bf333",
   "metadata": {
    "height": 319
   },
   "outputs": [],
   "source": [
    "from utils import get_api_key\n",
    "\n",
    "# PaLM legacy\n",
    "## import google.generativeai as palm\n",
    "## palm.configure(api_key=get_api_key())\n",
    "\n",
    "# Gemini API\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from google.api_core import client_options as client_options_lib\n",
    "\n",
    "genai.configure(\n",
    "    api_key=get_api_key(),\n",
    "    transport=\"rest\",\n",
    "    client_options=client_options_lib.ClientOptions(\n",
    "        api_endpoint=os.getenv(\"GOOGLE_API_BASE\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7550f9-c19b-4bdf-83d7-61ee51d94083",
   "metadata": {},
   "source": [
    "#### Pick the model that generates text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc154143",
   "metadata": {},
   "source": [
    "```Python\n",
    "# Legacy models shown in the video\n",
    "models = [m for m in genai.list_models() if 'generateText' in m.supported_generation_methods]\n",
    "model_bison = models[0]\n",
    "# Model Bison set as legacy model in 2024\n",
    "model_bison\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "963ec2a9",
   "metadata": {
    "height": 75
   },
   "outputs": [],
   "source": [
    "# Set the model to connect to the Gemini API\n",
    "model_flash = genai.GenerativeModel(model_name='gemini-2.0-flash')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089429a3-357d-4df2-a11a-9ba4bc9b7e85",
   "metadata": {},
   "source": [
    "#### Helper function to call the PaLM API\n",
    "\n",
    "```Python\n",
    "from google.api_core import retry\n",
    "@retry.Retry()\n",
    "def generate_text(prompt, \n",
    "                  model=model_bison, \n",
    "                  temperature=0.0):\n",
    "    return palm.generate_text(prompt=prompt,\n",
    "                              model=model,\n",
    "                              temperature=temperature)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e213e43",
   "metadata": {},
   "source": [
    "### Helper function to call the Gemini API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87831e1c-493f-473c-a13a-f7856678aef8",
   "metadata": {
    "height": 113
   },
   "outputs": [],
   "source": [
    "def generate_text(prompt,\n",
    "                  model=model_flash,\n",
    "                  temperature=0.0):\n",
    "    return model_flash.generate_content(prompt,\n",
    "                                  generation_config={'temperature':temperature})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eef99a-7840-4296-beaf-ea0b758bb9a1",
   "metadata": {},
   "source": [
    "### Ask an LLM to explain a complex code base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71c2070a-56f6-4c88-b5eb-f6cb0ca5e6a7",
   "metadata": {
    "height": 4397
   },
   "outputs": [],
   "source": [
    "#@title Complex Code Block\n",
    "# Note: Taken from https://github.com/lmoroney/odmlbook/blob/63c0825094b2f44efc5c4d3226425a51990e73d6/BookSource/Chapter08/ios/cats_vs_dogs/CatVsDogClassifierSample/ModelDataHandler/ModelDataHandler.swift\n",
    "CODE_BLOCK = \"\"\"\n",
    "// Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
    "//\n",
    "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "// you may not use this file except in compliance with the License.\n",
    "// You may obtain a copy of the License at\n",
    "//\n",
    "//    http://www.apache.org/licenses/LICENSE-2.0\n",
    "//\n",
    "// Unless required by applicable law or agreed to in writing, software\n",
    "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "// See the License for the specific language governing permissions and\n",
    "// limitations under the License.\n",
    "\n",
    "import CoreImage\n",
    "import TensorFlowLite\n",
    "import UIKit\n",
    "\n",
    "\n",
    "/// An inference from invoking the `Interpreter`.\n",
    "struct Inference {\n",
    "  let confidence: Float\n",
    "  let label: String\n",
    "}\n",
    "\n",
    "/// Information about a model file or labels file.\n",
    "typealias FileInfo = (name: String, extension: String)\n",
    "\n",
    "/// Information about the MobileNet model.\n",
    "enum MobileNet {\n",
    "  static let modelInfo: FileInfo = (name: \"converted_model\", extension: \"tflite\")\n",
    "}\n",
    "\n",
    "/// This class handles all data preprocessing and makes calls to run inference on a given frame\n",
    "/// by invoking the `Interpreter`. It then formats the inferences obtained and returns the top N\n",
    "/// results for a successful inference.\n",
    "class ModelDataHandler {\n",
    "\n",
    "  // MARK: - Public Properties\n",
    "\n",
    "  /// The current thread count used by the TensorFlow Lite Interpreter.\n",
    "  let threadCount: Int\n",
    "\n",
    "  let resultCount = 1\n",
    "\n",
    "  // MARK: - Model Parameters\n",
    "\n",
    "  let batchSize = 1\n",
    "  let inputChannels = 3\n",
    "  let inputWidth = 224\n",
    "  let inputHeight = 224\n",
    "\n",
    "  // MARK: - Private Properties\n",
    "\n",
    "  /// List of labels from the given labels file.\n",
    "  private var labels: [String] = [\"Cat\", \"Dog\"]\n",
    "\n",
    "  /// TensorFlow Lite `Interpreter` object for performing inference on a given model.\n",
    "  private var interpreter: Interpreter\n",
    "\n",
    "  /// Information about the alpha component in RGBA data.\n",
    "  private let alphaComponent = (baseOffset: 4, moduloRemainder: 3)\n",
    "\n",
    "  // MARK: - Initialization\n",
    "\n",
    "  /// A failable initializer for `ModelDataHandler`. A new instance is created if the model and\n",
    "  /// labels files are successfully loaded from the app's main bundle. Default `threadCount` is 1.\n",
    "  init?(modelFileInfo: FileInfo, threadCount: Int = 1) {\n",
    "    let modelFilename = modelFileInfo.name\n",
    "\n",
    "    // Construct the path to the model file.\n",
    "    guard let modelPath = Bundle.main.path(\n",
    "      forResource: modelFilename,\n",
    "      ofType: modelFileInfo.extension\n",
    "      ) else {\n",
    "        print(\"Failed to load the model file with name: \\(modelFilename).\")\n",
    "        return nil\n",
    "    }\n",
    "\n",
    "    // Specify the options for the `Interpreter`.\n",
    "    self.threadCount = threadCount\n",
    "    var options = InterpreterOptions()\n",
    "    options.threadCount = threadCount\n",
    "    do {\n",
    "      // Create the `Interpreter`.\n",
    "      interpreter = try Interpreter(modelPath: modelPath, options: options)\n",
    "    } catch let error {\n",
    "      print(\"Failed to create the interpreter with error: \\(error.localizedDescription)\")\n",
    "      return nil\n",
    "    }\n",
    "\n",
    "  }\n",
    "\n",
    "  // MARK: - Public Methods\n",
    "\n",
    "  /// Performs image preprocessing, invokes the `Interpreter`, and process the inference results.\n",
    "  func runModel(onFrame pixelBuffer: CVPixelBuffer) -> [Inference]? {\n",
    "    let sourcePixelFormat = CVPixelBufferGetPixelFormatType(pixelBuffer)\n",
    "    assert(sourcePixelFormat == kCVPixelFormatType_32ARGB ||\n",
    "      sourcePixelFormat == kCVPixelFormatType_32BGRA ||\n",
    "      sourcePixelFormat == kCVPixelFormatType_32RGBA)\n",
    "\n",
    "\n",
    "    let imageChannels = 4\n",
    "    assert(imageChannels >= inputChannels)\n",
    "\n",
    "    // Crops the image to the biggest square in the center and scales it down to model dimensions.\n",
    "    let scaledSize = CGSize(width: inputWidth, height: inputHeight)\n",
    "    guard let thumbnailPixelBuffer = pixelBuffer.centerThumbnail(ofSize: scaledSize) else {\n",
    "      return nil\n",
    "    }\n",
    "\n",
    "    let outputTensor: Tensor\n",
    "    do {\n",
    "      // Allocate memory for the model's input `Tensor`s.\n",
    "      try interpreter.allocateTensors()\n",
    "\n",
    "      // Remove the alpha component from the image buffer to get the RGB data.\n",
    "      guard let rgbData = rgbDataFromBuffer(\n",
    "        thumbnailPixelBuffer,\n",
    "        byteCount: batchSize * inputWidth * inputHeight * inputChannels\n",
    "        ) else {\n",
    "          print(\"Failed to convert the image buffer to RGB data.\")\n",
    "          return nil\n",
    "      }\n",
    "\n",
    "      // Copy the RGB data to the input `Tensor`.\n",
    "      try interpreter.copy(rgbData, toInputAt: 0)\n",
    "\n",
    "      // Run inference by invoking the `Interpreter`.\n",
    "      try interpreter.invoke()\n",
    "\n",
    "      // Get the output `Tensor` to process the inference results.\n",
    "      outputTensor = try interpreter.output(at: 0)\n",
    "    } catch let error {\n",
    "      print(\"Failed to invoke the interpreter with error: \\(error.localizedDescription)\")\n",
    "      return nil\n",
    "    }\n",
    "\n",
    "    let results = [Float32](unsafeData: outputTensor.data) ?? []\n",
    "\n",
    "    // Process the results.\n",
    "    let topNInferences = getTopN(results: results)\n",
    "\n",
    "    // Return the inference time and inference results.\n",
    "    return topNInferences\n",
    "  }\n",
    "\n",
    "  // MARK: - Private Methods\n",
    "\n",
    "  /// Returns the top N inference results sorted in descending order.\n",
    "  private func getTopN(results: [Float]) -> [Inference] {\n",
    "    // Create a zipped array of tuples [(labelIndex: Int, confidence: Float)].\n",
    "    let zippedResults = zip(labels.indices, results)\n",
    "\n",
    "    // Sort the zipped results by confidence value in descending order.\n",
    "    let sortedResults = zippedResults.sorted { $0.1 > $1.1 }.prefix(resultCount)\n",
    "\n",
    "    // Return the `Inference` results.\n",
    "    return sortedResults.map { result in Inference(confidence: result.1, label: labels[result.0]) }\n",
    "  }\n",
    "\n",
    "  /// Loads the labels from the labels file and stores them in the `labels` property.\n",
    "  private func loadLabels(fileInfo: FileInfo) {\n",
    "    let filename = fileInfo.name\n",
    "    let fileExtension = fileInfo.extension\n",
    "    guard let fileURL = Bundle.main.url(forResource: filename, withExtension: fileExtension) else {\n",
    "      fatalError(\"Labels file not found in bundle. Please add a labels file with name \" +\n",
    "        \"\\(filename).\\(fileExtension) and try again.\")\n",
    "    }\n",
    "    do {\n",
    "      let contents = try String(contentsOf: fileURL, encoding: .utf8)\n",
    "      labels = contents.components(separatedBy: .newlines)\n",
    "    } catch {\n",
    "      fatalError(\"Labels file named \\(filename).\\(fileExtension) cannot be read. Please add a \" +\n",
    "        \"valid labels file and try again.\")\n",
    "    }\n",
    "  }\n",
    "\n",
    "  /// Returns the RGB data representation of the given image buffer with the specified `byteCount`.\n",
    "  ///\n",
    "  /// - Parameters\n",
    "  ///   - buffer: The pixel buffer to convert to RGB data.\n",
    "  ///   - byteCount: The expected byte count for the RGB data calculated using the values that the\n",
    "  ///       model was trained on: `batchSize * imageWidth * imageHeight * componentsCount`.\n",
    "  ///   - isModelQuantized: Whether the model is quantized (i.e. fixed point values rather than\n",
    "  ///       floating point values).\n",
    "  /// - Returns: The RGB data representation of the image buffer or `nil` if the buffer could not be\n",
    "  ///     converted.\n",
    "  private func rgbDataFromBuffer(\n",
    "    _ buffer: CVPixelBuffer,\n",
    "    byteCount: Int\n",
    "    ) -> Data? {\n",
    "    CVPixelBufferLockBaseAddress(buffer, .readOnly)\n",
    "    defer { CVPixelBufferUnlockBaseAddress(buffer, .readOnly) }\n",
    "    guard let mutableRawPointer = CVPixelBufferGetBaseAddress(buffer) else {\n",
    "      return nil\n",
    "    }\n",
    "    let count = CVPixelBufferGetDataSize(buffer)\n",
    "    let bufferData = Data(bytesNoCopy: mutableRawPointer, count: count, deallocator: .none)\n",
    "    var rgbBytes = [Float](repeating: 0, count: byteCount)\n",
    "    var index = 0\n",
    "    for component in bufferData.enumerated() {\n",
    "      let offset = component.offset\n",
    "      let isAlphaComponent = (offset % alphaComponent.baseOffset) == alphaComponent.moduloRemainder\n",
    "      guard !isAlphaComponent else { continue }\n",
    "      rgbBytes[index] = Float(component.element) / 255.0\n",
    "      index += 1\n",
    "    }\n",
    "\n",
    "    return rgbBytes.withUnsafeBufferPointer(Data.init)\n",
    "\n",
    "  }\n",
    "}\n",
    "\n",
    "// MARK: - Extensions\n",
    "\n",
    "extension Data {\n",
    "  /// Creates a new buffer by copying the buffer pointer of the given array.\n",
    "  ///\n",
    "  /// - Warning: The given array's element type `T` must be trivial in that it can be copied bit\n",
    "  ///     for bit with no indirection or reference-counting operations; otherwise, reinterpreting\n",
    "  ///     data from the resulting buffer has undefined behavior.\n",
    "  /// - Parameter array: An array with elements of type `T`.\n",
    "  init<T>(copyingBufferOf array: [T]) {\n",
    "    self = array.withUnsafeBufferPointer(Data.init)\n",
    "  }\n",
    "}\n",
    "\n",
    "extension Array {\n",
    "  /// Creates a new array from the bytes of the given unsafe data.\n",
    "  ///\n",
    "  /// - Warning: The array's `Element` type must be trivial in that it can be copied bit for bit\n",
    "  ///     with no indirection or reference-counting operations; otherwise, copying the raw bytes in\n",
    "  ///     the `unsafeData`'s buffer to a new array returns an unsafe copy.\n",
    "  /// - Note: Returns `nil` if `unsafeData.count` is not a multiple of\n",
    "  ///     `MemoryLayout<Element>.stride`.\n",
    "  /// - Parameter unsafeData: The data containing the bytes to turn into an array.\n",
    "  init?(unsafeData: Data) {\n",
    "\n",
    "    guard unsafeData.count % MemoryLayout<Element>.stride == 0 else { return nil }\n",
    "    #if swift(>=5.0)\n",
    "    self = unsafeData.withUnsafeBytes { .init($0.bindMemory(to: Element.self)) }\n",
    "    #else\n",
    "    self = unsafeData.withUnsafeBytes {\n",
    "      .init(UnsafeBufferPointer<Element>(\n",
    "        start: $0,\n",
    "        count: unsafeData.count / MemoryLayout<Element>.stride\n",
    "      ))\n",
    "    }\n",
    "    #endif  // swift(>=5.0)\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff3c49c7-a4d3-4eca-8bf0-4e0cdbd337d0",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Can you please explain how this code works?\n",
    "\n",
    "{question}\n",
    "\n",
    "Use a lot of detail and make it as clear as possible.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90c20db9-f6e9-4d27-96b5-d43f0b24a484",
   "metadata": {
    "height": 166
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's break down this Swift code, which is designed to perform image classification using a TensorFlow Lite model.  It's a common pattern for mobile applications that need to analyze images quickly and efficiently.\n",
      "\n",
      "**Overall Purpose**\n",
      "\n",
      "The code defines a `ModelDataHandler` class that encapsulates the logic for:\n",
      "\n",
      "1.  **Loading a TensorFlow Lite model:**  It reads the model file from the app's bundle.\n",
      "2.  **Preprocessing images:**  It takes a `CVPixelBuffer` (a common image format in iOS) as input, resizes it, and converts it into a format suitable for the TensorFlow Lite model.\n",
      "3.  **Running inference:**  It uses the TensorFlow Lite `Interpreter` to execute the model on the preprocessed image data.\n",
      "4.  **Postprocessing results:**  It extracts the model's output, finds the most likely classification labels, and returns them as `Inference` objects.\n",
      "\n",
      "**Code Breakdown**\n",
      "\n",
      "Let's go through the code section by section:\n",
      "\n",
      "**1. Copyright and Imports**\n",
      "\n",
      "```swift\n",
      "// Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
      "// ... (License information)\n",
      "\n",
      "import CoreImage\n",
      "import TensorFlowLite\n",
      "import UIKit\n",
      "```\n",
      "\n",
      "*   **Copyright:** Standard copyright notice.\n",
      "*   **Imports:**\n",
      "    *   `CoreImage`:  Provides image processing capabilities (e.g., resizing).\n",
      "    *   `TensorFlowLite`:  The TensorFlow Lite framework for running models on mobile devices.\n",
      "    *   `UIKit`:  The main framework for building iOS user interfaces (used here for `UIImage` and other UI-related types).\n",
      "\n",
      "**2. `Inference` Struct**\n",
      "\n",
      "```swift\n",
      "/// An inference from invoking the `Interpreter`.\n",
      "struct Inference {\n",
      "  let confidence: Float\n",
      "  let label: String\n",
      "}\n",
      "```\n",
      "\n",
      "*   This struct represents a single prediction made by the model.\n",
      "    *   `confidence`:  A floating-point number representing the model's confidence in the prediction (usually between 0 and 1).\n",
      "    *   `label`:  A string describing the predicted object or category (e.g., \"Cat\", \"Dog\", \"Car\").\n",
      "\n",
      "**3. `FileInfo` Type Alias and `MobileNet` Enum**\n",
      "\n",
      "```swift\n",
      "/// Information about a model file or labels file.\n",
      "typealias FileInfo = (name: String, extension: String)\n",
      "\n",
      "/// Information about the MobileNet model.\n",
      "enum MobileNet {\n",
      "  static let modelInfo: FileInfo = (name: \"converted_model\", extension: \"tflite\")\n",
      "}\n",
      "```\n",
      "\n",
      "*   `FileInfo`:  A type alias that defines a tuple to store the name and extension of a file.  This is used to represent the model file.\n",
      "*   `MobileNet`:  An enum (although it only has one case) that holds the `FileInfo` for the TensorFlow Lite model.  In this case, it assumes the model file is named \"converted\\_model.tflite\".  This is likely a placeholder, and you'd replace it with the actual name of your model file.\n",
      "\n",
      "**4. `ModelDataHandler` Class**\n",
      "\n",
      "This is the core of the code.\n",
      "\n",
      "```swift\n",
      "/// This class handles all data preprocessing and makes calls to run inference on a given frame\n",
      "/// by invoking the `Interpreter`. It then formats the inferences obtained and returns the top N\n",
      "/// results for a successful inference.\n",
      "class ModelDataHandler {\n",
      "\n",
      "  // MARK: - Public Properties\n",
      "\n",
      "  /// The current thread count used by the TensorFlow Lite Interpreter.\n",
      "  let threadCount: Int\n",
      "\n",
      "  let resultCount = 1\n",
      "\n",
      "  // MARK: - Model Parameters\n",
      "\n",
      "  let batchSize = 1\n",
      "  let inputChannels = 3\n",
      "  let inputWidth = 224\n",
      "  let inputHeight = 224\n",
      "\n",
      "  // MARK: - Private Properties\n",
      "\n",
      "  /// List of labels from the given labels file.\n",
      "  private var labels: [String] = [\"Cat\", \"Dog\"]\n",
      "\n",
      "  /// TensorFlow Lite `Interpreter` object for performing inference on a given model.\n",
      "  private var interpreter: Interpreter\n",
      "\n",
      "  /// Information about the alpha component in RGBA data.\n",
      "  private let alphaComponent = (baseOffset: 4, moduloRemainder: 3)\n",
      "\n",
      "  // MARK: - Initialization\n",
      "\n",
      "  /// A failable initializer for `ModelDataHandler`. A new instance is created if the model and\n",
      "  /// labels files are successfully loaded from the app's main bundle. Default `threadCount` is 1.\n",
      "  init?(modelFileInfo: FileInfo, threadCount: Int = 1) {\n",
      "    let modelFilename = modelFileInfo.name\n",
      "\n",
      "    // Construct the path to the model file.\n",
      "    guard let modelPath = Bundle.main.path(\n",
      "      forResource: modelFilename,\n",
      "      ofType: modelFileInfo.extension\n",
      "      ) else {\n",
      "        print(\"Failed to load the model file with name: \\(modelFilename).\")\n",
      "        return nil\n",
      "    }\n",
      "\n",
      "    // Specify the options for the `Interpreter`.\n",
      "    self.threadCount = threadCount\n",
      "    var options = InterpreterOptions()\n",
      "    options.threadCount = threadCount\n",
      "    do {\n",
      "      // Create the `Interpreter`.\n",
      "      interpreter = try Interpreter(modelPath: modelPath, options: options)\n",
      "    } catch let error {\n",
      "      print(\"Failed to create the interpreter with error: \\(error.localizedDescription)\")\n",
      "      return nil\n",
      "    }\n",
      "\n",
      "  }\n",
      "\n",
      "  // MARK: - Public Methods\n",
      "\n",
      "  /// Performs image preprocessing, invokes the `Interpreter`, and process the inference results.\n",
      "  func runModel(onFrame pixelBuffer: CVPixelBuffer) -> [Inference]? {\n",
      "    let sourcePixelFormat = CVPixelBufferGetPixelFormatType(pixelBuffer)\n",
      "    assert(sourcePixelFormat == kCVPixelFormatType_32ARGB ||\n",
      "      sourcePixelFormat == kCVPixelFormatType_32BGRA ||\n",
      "      sourcePixelFormat == kCVPixelFormatType_32RGBA)\n",
      "\n",
      "\n",
      "    let imageChannels = 4\n",
      "    assert(imageChannels >= inputChannels)\n",
      "\n",
      "    // Crops the image to the biggest square in the center and scales it down to model dimensions.\n",
      "    let scaledSize = CGSize(width: inputWidth, height: inputHeight)\n",
      "    guard let thumbnailPixelBuffer = pixelBuffer.centerThumbnail(ofSize: scaledSize) else {\n",
      "      return nil\n",
      "    }\n",
      "\n",
      "    let outputTensor: Tensor\n",
      "    do {\n",
      "      // Allocate memory for the model's input `Tensor`s.\n",
      "      try interpreter.allocateTensors()\n",
      "\n",
      "      // Remove the alpha component from the image buffer to get the RGB data.\n",
      "      guard let rgbData = rgbDataFromBuffer(\n",
      "        thumbnailPixelBuffer,\n",
      "        byteCount: batchSize * inputWidth * inputHeight * inputChannels\n",
      "        ) else {\n",
      "          print(\"Failed to convert the image buffer to RGB data.\")\n",
      "          return nil\n",
      "      }\n",
      "\n",
      "      // Copy the RGB data to the input `Tensor`.\n",
      "      try interpreter.copy(rgbData, toInputAt: 0)\n",
      "\n",
      "      // Run inference by invoking the `Interpreter`.\n",
      "      try interpreter.invoke()\n",
      "\n",
      "      // Get the output `Tensor` to process the inference results.\n",
      "      outputTensor = try interpreter.output(at: 0)\n",
      "    } catch let error {\n",
      "      print(\"Failed to invoke the interpreter with error: \\(error.localizedDescription)\")\n",
      "      return nil\n",
      "    }\n",
      "\n",
      "    let results = [Float32](unsafeData: outputTensor.data) ?? []\n",
      "\n",
      "    // Process the results.\n",
      "    let topNInferences = getTopN(results: results)\n",
      "\n",
      "    // Return the inference time and inference results.\n",
      "    return topNInferences\n",
      "  }\n",
      "\n",
      "  // MARK: - Private Methods\n",
      "\n",
      "  /// Returns the top N inference results sorted in descending order.\n",
      "  private func getTopN(results: [Float]) -> [Inference] {\n",
      "    // Create a zipped array of tuples [(labelIndex: Int, confidence: Float)].\n",
      "    let zippedResults = zip(labels.indices, results)\n",
      "\n",
      "    // Sort the zipped results by confidence value in descending order.\n",
      "    let sortedResults = zippedResults.sorted { $0.1 > $1.1 }.prefix(resultCount)\n",
      "\n",
      "    // Return the `Inference` results.\n",
      "    return sortedResults.map { result in Inference(confidence: result.1, label: labels[result.0]) }\n",
      "  }\n",
      "\n",
      "  /// Loads the labels from the labels file and stores them in the `labels` property.\n",
      "  private func loadLabels(fileInfo: FileInfo) {\n",
      "    let filename = fileInfo.name\n",
      "    let fileExtension = fileInfo.extension\n",
      "    guard let fileURL = Bundle.main.url(forResource: filename, withExtension: fileExtension) else {\n",
      "      fatalError(\"Labels file not found in bundle. Please add a labels file with name \" +\n",
      "        \"\\(filename).\\(fileExtension) and try again.\")\n",
      "    }\n",
      "    do {\n",
      "      let contents = try String(contentsOf: fileURL, encoding: .utf8)\n",
      "      labels = contents.components(separatedBy: .newlines)\n",
      "    } catch {\n",
      "      fatalError(\"Labels file named \\(filename).\\(fileExtension) cannot be read. Please add a \" +\n",
      "        \"valid labels file and try again.\")\n",
      "    }\n",
      "  }\n",
      "\n",
      "  /// Returns the RGB data representation of the given image buffer with the specified `byteCount`.\n",
      "  ///\n",
      "  /// - Parameters\n",
      "  ///   - buffer: The pixel buffer to convert to RGB data.\n",
      "  ///   - byteCount: The expected byte count for the RGB data calculated using the values that the\n",
      "  ///       model was trained on: `batchSize * imageWidth * imageHeight * componentsCount`.\n",
      "  ///   - isModelQuantized: Whether the model is quantized (i.e. fixed point values rather than\n",
      "  ///       floating point values).\n",
      "  /// - Returns: The RGB data representation of the image buffer or `nil` if the buffer could not be\n",
      "  ///     converted.\n",
      "  private func rgbDataFromBuffer(\n",
      "    _ buffer: CVPixelBuffer,\n",
      "    byteCount: Int\n",
      "    ) -> Data? {\n",
      "    CVPixelBufferLockBaseAddress(buffer, .readOnly)\n",
      "    defer { CVPixelBufferUnlockBaseAddress(buffer, .readOnly) }\n",
      "    guard let mutableRawPointer = CVPixelBufferGetBaseAddress(buffer) else {\n",
      "      return nil\n",
      "    }\n",
      "    let count = CVPixelBufferGetDataSize(buffer)\n",
      "    let bufferData = Data(bytesNoCopy: mutableRawPointer, count: count, deallocator: .none)\n",
      "    var rgbBytes = [Float](repeating: 0, count: byteCount)\n",
      "    var index = 0\n",
      "    for component in bufferData.enumerated() {\n",
      "      let offset = component.offset\n",
      "      let isAlphaComponent = (offset % alphaComponent.baseOffset) == alphaComponent.moduloRemainder\n",
      "      guard !isAlphaComponent else { continue }\n",
      "      rgbBytes[index] = Float(component.element) / 255.0\n",
      "      index += 1\n",
      "    }\n",
      "\n",
      "    return rgbBytes.withUnsafeBufferPointer(Data.init)\n",
      "\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "*   **Properties:**\n",
      "    *   `threadCount`:  The number of threads to use for the TensorFlow Lite interpreter.  More threads can potentially improve performance, but also increase resource usage.\n",
      "    *   `resultCount`: The number of top results to return.  Currently set to 1.\n",
      "    *   `batchSize`, `inputChannels`, `inputWidth`, `inputHeight`:  These parameters define the expected input shape of the TensorFlow Lite model.  They *must* match the model's input requirements.  In this case, it expects a single image (batch size 1), with 3 color channels (RGB), and a size of 224x224 pixels.\n",
      "    *   `labels`:  An array of strings representing the possible classification labels.  **Important:** This array *must* correspond to the output of your TensorFlow Lite model.  The order of the labels must match the order of the model's output probabilities.  The default is `[\"Cat\", \"Dog\"]`, which is likely just a placeholder.  You'll need to replace this with the actual labels for your model.\n",
      "    *   `interpreter`:  The TensorFlow Lite `Interpreter` object.  This is the object that actually runs the model.\n",
      "    *   `alphaComponent`: Used to remove the alpha component from the image.\n",
      "*   **`init?(modelFileInfo: FileInfo, threadCount: Int = 1)` (Initializer):**\n",
      "    *   This is a *failable initializer*.  It returns `nil` if it fails to load the model.\n",
      "    *   It takes the `FileInfo` for the model file and an optional `threadCount`.\n",
      "    *   It constructs the full path to the model file using `Bundle.main.path(forResource:ofType:)`.\n",
      "    *   It creates an `InterpreterOptions` object and sets the `threadCount`.\n",
      "    *   It creates the `Interpreter` object using `try Interpreter(modelPath:options:)`.  This can throw an error if the model file is invalid or if there are other issues.\n",
      "*   **`runModel(onFrame pixelBuffer: CVPixelBuffer) -> [Inference]?`:**\n",
      "    *   This is the main method that performs the image classification.\n",
      "    *   It takes a `CVPixelBuffer` as input, which represents an image frame.\n",
      "    *   It performs the following steps:\n",
      "        1.  **Input Validation:** Checks the pixel format of the input `CVPixelBuffer`.\n",
      "        2.  **Image Resizing:**  It resizes the input image to the dimensions expected by the model (224x224 in this case).  It uses the `centerThumbnail(ofSize:)` extension method (which is not shown in this code snippet, but it's assumed to crop the image to a square and then scale it).\n",
      "        3.  **Tensor Allocation:**  It calls `try interpreter.allocateTensors()` to allocate memory for the model's input and output tensors.\n",
      "        4.  **Data Conversion:**  It converts the `CVPixelBuffer` to RGB data in a format suitable for the model.  It calls the `rgbDataFromBuffer(_:byteCount:)` method to do this.\n",
      "        5.  **Data Copying:**  It copies the RGB data to the input tensor using `try interpreter.copy(_:toInputAt:)`.\n",
      "        6.  **Inference:**  It runs the model by calling `try interpreter.invoke()`.\n",
      "        7.  **Output Extraction:**  It gets the output tensor using `try interpreter.output(at:)`.\n",
      "        8.  **Result Processing:**  It converts the output tensor data to an array of `Float32` values.\n",
      "        9.  **Top-N Selection:**  It calls the `getTopN(results:)` method to find the top N most likely classification labels.\n",
      "        10. **Return Results:**  It returns an array of `Inference` objects representing the top N predictions.\n",
      "*   **`getTopN(results: [Float]) -> [Inference]`:**\n",
      "    *   This method takes the raw output of the model (an array of `Float` values representing probabilities or scores for each class) and finds the top N predictions.\n",
      "    *   It uses `zip` to create an array of tuples, pairing each label index with its corresponding confidence score.\n",
      "    *   It sorts the tuples by confidence score in descending order using `sorted { $0.1 > $1.1 }`.\n",
      "    *   It takes the top `resultCount` (which is 1 in this case) results using `prefix(resultCount)`.\n",
      "    *   It maps the sorted results to an array of `Inference` objects.\n",
      "*   **`loadLabels(fileInfo: FileInfo)`:**\n",
      "    *   This method is intended to load the labels from a separate text file.  Each line in the file would represent a label.\n",
      "    *   **Important:** This method is *not* called in the provided code.  You would need to call it in the `init?` method if you want to load labels from a file.  If you don't call it, the `labels` array will remain the default `[\"Cat\", \"Dog\"]`.\n",
      "*   **`rgbDataFromBuffer(_ buffer: CVPixelBuffer, byteCount: Int) -> Data?`:**\n",
      "    *   This method converts a `CVPixelBuffer` to RGB data.\n",
      "    *   It locks the base address of the pixel buffer to access the raw pixel data.\n",
      "    *   It iterates through the pixel data, extracting the R, G, and B components.  It skips the alpha component (A) in RGBA or ARGB formats.\n",
      "    *   It normalizes the RGB values to the range \\[0, 1] by dividing by 255.0.\n",
      "    *   It returns the RGB data as a `Data` object.\n",
      "\n",
      "**5. Extensions**\n",
      "\n",
      "```swift\n",
      "// MARK: - Extensions\n",
      "\n",
      "extension Data {\n",
      "  /// Creates a new buffer by copying the buffer pointer of the given array.\n",
      "  ///\n",
      "  /// - Warning: The given array's element type `T` must be trivial in that it can be copied bit\n",
      "  ///     for bit with no indirection or reference-counting operations; otherwise, reinterpreting\n",
      "  ///     data from the resulting buffer has undefined behavior.\n",
      "  /// - Parameter array: An array with elements of type `T`.\n",
      "  init<T>(copyingBufferOf array: [T]) {\n",
      "    self = array.withUnsafeBufferPointer(Data.init)\n",
      "  }\n",
      "}\n",
      "\n",
      "extension Array {\n",
      "  /// Creates a new array from the bytes of the given unsafe data.\n",
      "  ///\n",
      "  /// - Warning: The array's `Element` type must be trivial in that it can be copied bit for bit\n",
      "  ///     with no indirection or reference-counting operations; otherwise, copying the raw bytes in\n",
      "  ///     the `unsafeData`'s buffer to a new array returns an unsafe copy.\n",
      "  /// - Note: Returns `nil` if `unsafeData.count` is not a multiple of\n",
      "  ///     `MemoryLayout<Element>.stride`.\n",
      "  /// - Parameter unsafeData: The data containing the bytes to turn into an array.\n",
      "  init?(unsafeData: Data) {\n",
      "\n",
      "    guard unsafeData.count % MemoryLayout<Element>.stride == 0 else { return nil }\n",
      "    #if swift(>=5.0)\n",
      "    self = unsafeData.withUnsafeBytes { .init($0.bindMemory(to: Element.self)) }\n",
      "    #else\n",
      "    self = unsafeData.withUnsafeBytes {\n",
      "      .init(UnsafeBufferPointer<Element>(\n",
      "        start: $0,\n",
      "        count: unsafeData.count / MemoryLayout<Element>.stride\n",
      "      ))\n",
      "    }\n",
      "    #endif  // swift(>=5.0)\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "*   These extensions provide convenience initializers for `Data` and `Array` that allow you to create them from raw memory buffers.  They are used to efficiently copy data between the `CVPixelBuffer`, the TensorFlow Lite `Interpreter`, and the output results.  The warnings are important: these methods are only safe to use with \"trivial\" types (like `Float`, `Int`, etc.) that can be copied bitwise.\n",
      "\n",
      "**How to Use This Code**\n",
      "\n",
      "1.  **Get a TensorFlow Lite Model:** You'll need a TensorFlow Lite model (`.tflite` file) that has been trained for image classification.  You can convert a TensorFlow model to TensorFlow Lite using the TensorFlow Lite Converter.\n",
      "2.  **Add the Model to Your Project:**  Drag the `.tflite` file into your Xcode project.  Make sure it's added to the \"Copy Bundle Resources\" build phase of your target.\n",
      "3.  **Create a Labels File (Optional):**  If you have a separate labels file (a text file with one label per line), add it to your project as well.\n",
      "4.  **Update `MobileNet.modelInfo`:**  Change the `name` and `extension` in the `MobileNet.modelInfo` enum to match the name of your model file.\n",
      "5.  **Update the `labels` Array:**\n",
      "    *   If you're using a labels file, call the `loadLabels(fileInfo:)` method in the `init?` of `ModelDataHandler`, passing in the `FileInfo` for your labels file.\n",
      "    *   Otherwise, replace the default `[\"Cat\", \"Dog\"]` array with the actual labels for your model.  Make sure the order of the labels matches the order of the model's output.\n",
      "6.  **Integrate with Your Camera or Image Source:**  You'll need to get a `CVPixelBuffer` from your camera or image source.  This is typically done using `AVFoundation` for camera input or `CoreImage` for image processing.\n",
      "7.  **Create a `ModelDataHandler` Instance:**\n",
      "\n",
      "    ```swift\n",
      "    let modelDataHandler = ModelDataHandler(modelFileInfo: MobileNet.modelInfo)\n",
      "    ```\n",
      "\n",
      "8.  **Run Inference:**\n",
      "\n",
      "    ```swift\n",
      "    if let pixelBuffer = yourPixelBuffer { // Replace with your actual pixel buffer\n",
      "        if let inferences = modelDataHandler?.runModel(onFrame: pixelBuffer) {\n",
      "            // Process the inferences\n",
      "            for inference in inferences {\n",
      "                print(\"Label: \\(inference.label), Confidence: \\(inference.confidence)\")\n",
      "            }\n",
      "        } else {\n",
      "            print(\"Inference failed.\")\n",
      "        }\n",
      "    }\n",
      "    ```\n",
      "\n",
      "**Important Considerations**\n",
      "\n",
      "*   **Model Input Requirements:**  The `batchSize`, `inputChannels`, `inputWidth`, and `inputHeight` properties *must* match the input requirements of your TensorFlow Lite model.  If they don't, the model will likely produce incorrect results or crash.\n",
      "*   **Labels:**  The `labels` array *must* be correct and in the correct order.  This is crucial for interpreting the model's output.\n",
      "*   **Error Handling:**  The code includes some basic error handling (e.g., checking if the model file can be loaded), but you may want to add more robust error handling for production use.\n",
      "*   **Performance:**  Running TensorFlow Lite models on mobile devices can be resource-intensive.  Consider using techniques like quantization to reduce the model size and improve performance.  Also, experiment with different `threadCount` values to find the optimal balance between performance and resource usage.\n",
      "*   **Image Preprocessing:** The `centerThumbnail` function is not provided in the code. You will need to implement this function to crop and resize the image to the correct dimensions for the model.\n",
      "\n",
      "Let me know if you have any specific questions about any part of the code or how to use it with your own TensorFlow Lite model!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "completion = generate_text(\n",
    "    prompt = prompt_template.format(question=CODE_BLOCK)\n",
    ")\n",
    "\n",
    "# Gemini API\n",
    "print(completion.text)\n",
    "\n",
    "# PaLM legacy\n",
    "## print(completion.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcb1824-56dd-4e34-8079-9767cd9539e0",
   "metadata": {},
   "source": [
    "#### Try it out on your own code!\n",
    "- Try inputting some code into the `CODE_BLOCK` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42ddae3b-5936-4b8e-b5ac-d6671c7a03e6",
   "metadata": {
    "height": 336
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's break down this Python code snippet step-by-step.\n",
      "\n",
      "**Code:**\n",
      "\n",
      "```python\n",
      "def foo(a):\n",
      "  b = a + 1\n",
      "  return 2*b\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "1.  **`def foo(a):`**:\n",
      "    *   `def` is a keyword in Python that signifies the beginning of a function definition.  A function is a reusable block of code that performs a specific task.\n",
      "    *   `foo` is the name we've given to this function.  You can choose almost any valid name for your functions (following Python's naming rules).  It's good practice to choose a name that describes what the function does.\n",
      "    *   `(a)`:  This part defines the *parameters* of the function.  In this case, the function `foo` takes one parameter, which we've named `a`.  A parameter is a variable that the function expects to receive as input when it's called.  Think of it as a placeholder for a value that will be provided later.\n",
      "\n",
      "2.  **`b = a + 1`**:\n",
      "    *   This line is inside the function's *body*.  The body contains the instructions that the function will execute.\n",
      "    *   `b = ...`: This is an assignment statement.  It means we're creating a variable named `b` and assigning a value to it.\n",
      "    *   `a + 1`: This is an arithmetic expression.  It takes the value of the parameter `a` (which was passed into the function) and adds 1 to it.\n",
      "    *   So, the entire line `b = a + 1` calculates the value of `a + 1` and stores the result in the variable `b`.  `b` is a *local variable* because it's only accessible within the `foo` function.\n",
      "\n",
      "3.  **`return 2*b`**:\n",
      "    *   `return` is a keyword that specifies the value that the function will \"return\" to the caller.  When a `return` statement is executed, the function stops executing, and the specified value is sent back to the part of the code that called the function.\n",
      "    *   `2*b`: This is another arithmetic expression.  It takes the value of the variable `b` (which we calculated in the previous line) and multiplies it by 2.\n",
      "    *   So, the entire line `return 2*b` calculates `2 * b` and returns that result.\n",
      "\n",
      "**In Summary:**\n",
      "\n",
      "The function `foo` does the following:\n",
      "\n",
      "1.  Takes a number `a` as input.\n",
      "2.  Adds 1 to `a` and stores the result in a variable `b`.\n",
      "3.  Multiplies `b` by 2.\n",
      "4.  Returns the result of that multiplication.\n",
      "\n",
      "**Example:**\n",
      "\n",
      "Let's say you call the function like this:\n",
      "\n",
      "```python\n",
      "result = foo(5)\n",
      "print(result)  # Output: 12\n",
      "```\n",
      "\n",
      "Here's what happens:\n",
      "\n",
      "1.  `foo(5)`:  You're calling the `foo` function and passing the value `5` as the argument for the parameter `a`.\n",
      "2.  Inside `foo`:\n",
      "    *   `a` is now equal to 5.\n",
      "    *   `b = a + 1` becomes `b = 5 + 1`, so `b` is assigned the value 6.\n",
      "    *   `return 2*b` becomes `return 2*6`, so the function returns the value 12.\n",
      "3.  `result = foo(5)`: The value returned by `foo` (which is 12) is assigned to the variable `result`.\n",
      "4.  `print(result)`: The value of `result` (which is 12) is printed to the console.\n",
      "\n",
      "**Another Example:**\n",
      "\n",
      "```python\n",
      "x = 10\n",
      "y = foo(x)\n",
      "print(y) # Output: 22\n",
      "```\n",
      "\n",
      "In this case:\n",
      "\n",
      "1. `x` is assigned the value 10.\n",
      "2. `foo(x)` is called, so `a` inside the function becomes 10.\n",
      "3. `b = a + 1` becomes `b = 10 + 1`, so `b` is assigned the value 11.\n",
      "4. `return 2*b` becomes `return 2*11`, so the function returns the value 22.\n",
      "5. `y = foo(x)` assigns the returned value 22 to the variable `y`.\n",
      "6. `print(y)` prints the value of `y`, which is 22.\n",
      "\n",
      "**Key Concepts:**\n",
      "\n",
      "*   **Functions:** Reusable blocks of code.\n",
      "*   **Parameters:** Inputs to a function.\n",
      "*   **Return Value:** The output of a function.\n",
      "*   **Local Variables:** Variables defined inside a function, only accessible within that function.\n",
      "*   **Assignment:**  Using the `=` operator to give a variable a value.\n",
      "*   **Arithmetic Operators:**  Symbols like `+` (addition) and `*` (multiplication) used to perform calculations.\n",
      "\n",
      "I hope this detailed explanation helps you understand how the code works! Let me know if you have any more questions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CODE_BLOCK2 = \"\"\" \n",
    "# replace this with your own code\n",
    "def foo(a):\n",
    "  b = a + 1\n",
    "  return 2*b\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Can you please explain how this code works?\n",
    "\n",
    "{question}\n",
    "\n",
    "Use a lot of detail and make it as clear as possible.\n",
    "\"\"\"\n",
    "\n",
    "completion = generate_text(\n",
    "    prompt = prompt_template.format(question=CODE_BLOCK2)\n",
    ")\n",
    "print(completion.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93864a7-fd47-427b-b6f1-b492e159560d",
   "metadata": {},
   "source": [
    "### Ask an LLM to document a complex code base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57ac2cb3-0865-4d9b-89ba-bfc237ebad3b",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Please write technical documentation for this code and \\n\n",
    "make it easy for a non swift developer to understand:\n",
    "\n",
    "{question}\n",
    "\n",
    "Output the results in markdown\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dcb82c6-a870-476b-b587-688d0344f33d",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```markdown\n",
      "## Technical Documentation for Image Classification Model Handler (Swift)\n",
      "\n",
      "This document describes the `ModelDataHandler` class, a Swift class designed to perform image classification using a TensorFlow Lite model. It's intended for developers who may not be familiar with Swift but need to understand how this code works.\n",
      "\n",
      "**Purpose:**\n",
      "\n",
      "The `ModelDataHandler` class is responsible for:\n",
      "\n",
      "1.  **Loading a TensorFlow Lite model:**  It loads a pre-trained model from your application's resources.\n",
      "2.  **Preprocessing images:** It takes an image (represented as a `CVPixelBuffer`) and prepares it for the model by resizing and converting it to the correct format.\n",
      "3.  **Running inference:** It feeds the preprocessed image to the TensorFlow Lite interpreter and gets the model's predictions.\n",
      "4.  **Post-processing results:** It interprets the model's output and returns the top prediction(s) with their associated confidence scores.\n",
      "\n",
      "**Key Concepts:**\n",
      "\n",
      "*   **TensorFlow Lite (TFLite):** A lightweight version of TensorFlow designed for running machine learning models on mobile and embedded devices.\n",
      "*   **Interpreter:** The TFLite `Interpreter` is the core component that executes the model.  It takes input data, runs the calculations defined in the model, and produces output data.\n",
      "*   **Model:** A pre-trained machine learning model that has been converted to the TensorFlow Lite format (`.tflite` file).  This model contains the knowledge learned from training data and is used to make predictions.\n",
      "*   **Inference:** The process of using a trained model to make a prediction on new data.\n",
      "*   **CVPixelBuffer:** A Core Video data type that holds pixel data for images and video frames.  It's a common way to represent images in iOS.\n",
      "*   **RGB Data:** A representation of color images where each pixel's color is defined by the intensity of red, green, and blue light.\n",
      "*   **Confidence:** A numerical value (typically between 0 and 1) that indicates how certain the model is about its prediction.  Higher values indicate greater confidence.\n",
      "*   **Labels:** A list of human-readable names that correspond to the possible categories the model can predict (e.g., \"cat\", \"dog\", \"car\").\n",
      "\n",
      "**Class Structure: `ModelDataHandler`**\n",
      "\n",
      "```swift\n",
      "class ModelDataHandler {\n",
      "  // ... properties and methods ...\n",
      "}\n",
      "```\n",
      "\n",
      "**Properties (Data Members):**\n",
      "\n",
      "*   `threadCount: Int`:  The number of threads the TensorFlow Lite interpreter will use.  More threads can potentially speed up inference, but also consume more resources.  The default is 1.\n",
      "*   `resultCount: Int`: The number of top predictions to return.  In this case, it's set to 1, meaning the class will return only the most likely prediction.\n",
      "*   `batchSize: Int`: The number of images processed in one go.  Here, it's set to 1, meaning the model processes one image at a time.\n",
      "*   `inputChannels: Int`: The number of color channels in the input image (3 for RGB).\n",
      "*   `inputWidth: Int`: The expected width of the input image for the model (224 pixels).\n",
      "*   `inputHeight: Int`: The expected height of the input image for the model (224 pixels).\n",
      "*   `labels: [String]`: An array of strings representing the possible labels (categories) the model can predict.  In this example, it's initialized with `[\"Cat\", \"Dog\"]`.  **Important:**  In a real application, this would be loaded from a file containing the actual labels for your model.\n",
      "*   `interpreter: Interpreter`:  The TensorFlow Lite interpreter object.  This is the object that actually runs the model.\n",
      "*   `alphaComponent: (baseOffset: Int, moduloRemainder: Int)`: Information used to remove the alpha (transparency) component from the image data.\n",
      "\n",
      "**Initialization: `init?(modelFileInfo: FileInfo, threadCount: Int = 1)`**\n",
      "\n",
      "This is the constructor for the `ModelDataHandler` class.  It attempts to load the TensorFlow Lite model from a file.\n",
      "\n",
      "*   `modelFileInfo: FileInfo`: A tuple containing the name and extension of the model file (e.g., `(name: \"converted_model\", extension: \"tflite\")`).\n",
      "*   `threadCount: Int`:  The number of threads to use for the interpreter (defaults to 1).\n",
      "\n",
      "The initializer performs the following steps:\n",
      "\n",
      "1.  **Finds the model file:**  It uses `Bundle.main.path` to locate the model file within the application's resources.\n",
      "2.  **Creates an `InterpreterOptions` object:**  This object allows you to configure the interpreter (e.g., set the number of threads).\n",
      "3.  **Creates an `Interpreter` object:**  It initializes the `interpreter` property with the path to the model file and the options.  If this fails (e.g., the model file is invalid), the initializer returns `nil`, indicating that the `ModelDataHandler` could not be created.\n",
      "\n",
      "**Public Method: `runModel(onFrame pixelBuffer: CVPixelBuffer) -> [Inference]?`**\n",
      "\n",
      "This is the main method that performs the image classification.\n",
      "\n",
      "*   `onFrame pixelBuffer: CVPixelBuffer`:  The input image, represented as a `CVPixelBuffer`.\n",
      "*   **Returns:** An optional array of `Inference` objects.  If the inference is successful, it returns an array containing the top prediction(s).  If an error occurs, it returns `nil`.\n",
      "\n",
      "The `runModel` method performs the following steps:\n",
      "\n",
      "1.  **Checks Pixel Format:** Verifies that the pixel format of the input image is compatible (32ARGB, 32BGRA, or 32RGBA).\n",
      "2.  **Resizes and Crops the Image:** It calls the `centerThumbnail` function (assumed to be an extension on `CVPixelBuffer`) to crop the image to a square and resize it to the dimensions expected by the model (224x224 pixels).\n",
      "3.  **Allocates Tensors:**  It calls `interpreter.allocateTensors()` to allocate memory for the model's input and output tensors.  Tensors are multi-dimensional arrays that hold the data used by the model.\n",
      "4.  **Converts Image to RGB Data:** It calls the `rgbDataFromBuffer` method (described below) to convert the `CVPixelBuffer` to a `Data` object containing the RGB pixel values.  This involves removing the alpha (transparency) component and normalizing the pixel values to the range 0.0 to 1.0.\n",
      "5.  **Copies Data to Input Tensor:** It calls `interpreter.copy(rgbData, toInputAt: 0)` to copy the RGB data to the input tensor of the model.\n",
      "6.  **Runs Inference:** It calls `interpreter.invoke()` to run the TensorFlow Lite model.  This performs the calculations defined in the model and produces the output.\n",
      "7.  **Gets Output Tensor:** It calls `interpreter.output(at: 0)` to get the output tensor from the model.  This tensor contains the model's predictions.\n",
      "8.  **Processes Results:** It converts the output tensor data to an array of `Float` values and calls the `getTopN` method (described below) to find the top prediction(s).\n",
      "9.  **Returns Results:** It returns the top prediction(s) as an array of `Inference` objects.\n",
      "\n",
      "**Private Methods:**\n",
      "\n",
      "*   `getTopN(results: [Float]) -> [Inference]`:  This method takes an array of prediction scores and returns the top `resultCount` (which is 1 in this case) predictions, sorted by confidence.  It creates `Inference` objects containing the label and confidence for each top prediction.\n",
      "*   `loadLabels(fileInfo: FileInfo)`: This method is intended to load the labels from a file.  However, in the provided code, it's not used.  It shows how to read a text file and split it into an array of strings, which would then be assigned to the `labels` property.\n",
      "*   `rgbDataFromBuffer(_ buffer: CVPixelBuffer, byteCount: Int) -> Data?`: This method converts a `CVPixelBuffer` to a `Data` object containing the RGB pixel values.  It iterates through the pixel data, removes the alpha component, and normalizes the RGB values to the range 0.0 to 1.0.\n",
      "\n",
      "**Data Structure: `Inference`**\n",
      "\n",
      "```swift\n",
      "struct Inference {\n",
      "  let confidence: Float\n",
      "  let label: String\n",
      "}\n",
      "```\n",
      "\n",
      "This struct represents a single prediction from the model.\n",
      "\n",
      "*   `confidence: Float`: The confidence score for the prediction.\n",
      "*   `label: String`: The human-readable label for the prediction (e.g., \"cat\", \"dog\").\n",
      "\n",
      "**Data Structure: `FileInfo`**\n",
      "\n",
      "```swift\n",
      "typealias FileInfo = (name: String, extension: String)\n",
      "```\n",
      "\n",
      "This type alias defines a tuple that holds the name and extension of a file.  It's used to represent the model file and (potentially) the labels file.\n",
      "\n",
      "**Enum: `MobileNet`**\n",
      "\n",
      "```swift\n",
      "enum MobileNet {\n",
      "  static let modelInfo: FileInfo = (name: \"converted_model\", extension: \"tflite\")\n",
      "}\n",
      "```\n",
      "\n",
      "This enum provides information about the MobileNet model file.  It defines a `modelInfo` property that contains the name and extension of the model file.  This is a convenient way to store this information in a single place.\n",
      "\n",
      "**Extensions:**\n",
      "\n",
      "The code includes extensions to the `Data` and `Array` classes to facilitate data conversion. These extensions are used to efficiently create `Data` objects from arrays and vice versa.\n",
      "\n",
      "**How to Use:**\n",
      "\n",
      "1.  **Add the TensorFlow Lite framework to your project.**\n",
      "2.  **Add your TensorFlow Lite model file (`.tflite`) to your project's bundle.**\n",
      "3.  **Create a `ModelDataHandler` instance:**\n",
      "\n",
      "    ```swift\n",
      "    if let modelDataHandler = ModelDataHandler(modelFileInfo: MobileNet.modelInfo) {\n",
      "      // Use the modelDataHandler\n",
      "    } else {\n",
      "      print(\"Failed to create ModelDataHandler.\")\n",
      "    }\n",
      "    ```\n",
      "\n",
      "4.  **Get a `CVPixelBuffer` from your image source (e.g., camera, photo library).**\n",
      "5.  **Call the `runModel` method:**\n",
      "\n",
      "    ```swift\n",
      "    if let inferences = modelDataHandler.runModel(onFrame: pixelBuffer) {\n",
      "      // Process the inferences\n",
      "      for inference in inferences {\n",
      "        print(\"Label: \\(inference.label), Confidence: \\(inference.confidence)\")\n",
      "      }\n",
      "    } else {\n",
      "      print(\"Failed to run inference.\")\n",
      "    }\n",
      "    ```\n",
      "\n",
      "**Important Considerations:**\n",
      "\n",
      "*   **Model Compatibility:**  Ensure that the input image dimensions (`inputWidth`, `inputHeight`, `inputChannels`) and data types used in the code match the requirements of your TensorFlow Lite model.\n",
      "*   **Labels File:**  Replace the hardcoded `labels` array with code that loads the labels from a file.  The labels file should contain one label per line.\n",
      "*   **Error Handling:**  The code includes basic error handling, but you may need to add more robust error handling for production use.\n",
      "*   **Performance:**  Experiment with the `threadCount` property to find the optimal value for your device.  Also, consider using a more efficient image resizing algorithm if performance is critical.\n",
      "*   **Memory Management:**  Be mindful of memory usage, especially when working with large images.  Release resources when they are no longer needed.\n",
      "*   **Preprocessing:** The image preprocessing steps (resizing, cropping, normalization) are crucial for achieving good accuracy.  Make sure that the preprocessing steps used in the code match the preprocessing steps used during model training.\n",
      "\n",
      "This documentation provides a comprehensive overview of the `ModelDataHandler` class. By understanding the purpose, structure, and key concepts of this code, you can effectively use it to perform image classification in your iOS applications. Remember to adapt the code to your specific model and requirements.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "completion = generate_text(\n",
    "    prompt = prompt_template.format(question=CODE_BLOCK)\n",
    ")\n",
    "print(completion.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9676f08-dd6a-4e83-9022-eb14ce4775f9",
   "metadata": {},
   "source": [
    "### Try it out on your own code!\n",
    "- Notice that we've modified the prompt slightly to refer to Python instead of Swift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17699e40-d85e-4b9e-a569-e55dcdc2a23b",
   "metadata": {
    "height": 353
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```markdown\n",
      "## Technical Documentation for the `foo` Function\n",
      "\n",
      "This document describes the functionality of a simple function named `foo`.  It's written to be understandable even if you don't know Python.\n",
      "\n",
      "**1. Purpose**\n",
      "\n",
      "The `foo` function takes a single numerical input, performs a calculation, and returns a numerical result.  Think of it like a mini-calculator that does a specific set of operations.\n",
      "\n",
      "**2. Input (Parameters)**\n",
      "\n",
      "*   **Name:** `a`\n",
      "*   **Type:**  A number (integer or decimal).  The function is designed to work with numerical values.\n",
      "*   **Description:**  This is the initial number that the function will use in its calculations.  It's the starting point.\n",
      "\n",
      "**3. Process (How it Works)**\n",
      "\n",
      "The function performs the following steps:\n",
      "\n",
      "1.  **Add 1:** It takes the input number `a` and adds 1 to it.  The result of this addition is stored in a temporary variable called `b`.  So, `b = a + 1`.\n",
      "\n",
      "2.  **Multiply by 2:** It then takes the value of `b` (which is `a + 1`) and multiplies it by 2.\n",
      "\n",
      "**4. Output (Return Value)**\n",
      "\n",
      "*   **Type:** A number (integer or decimal).\n",
      "*   **Description:** The function returns the final result of the calculation, which is `2 * b` (or, equivalently, `2 * (a + 1)`).\n",
      "\n",
      "**5. Example**\n",
      "\n",
      "Let's say you call the function with the input `a = 5`:\n",
      "\n",
      "1.  `a` is 5.\n",
      "2.  `b` becomes `5 + 1 = 6`.\n",
      "3.  The function returns `2 * 6 = 12`.\n",
      "\n",
      "Therefore, `foo(5)` would return `12`.\n",
      "\n",
      "**6. Code (Python)**\n",
      "\n",
      "```python\n",
      "def foo(a):\n",
      "  b = a + 1\n",
      "  return 2*b\n",
      "```\n",
      "\n",
      "**7.  Explanation of the Code (for non-Python developers)**\n",
      "\n",
      "*   `def foo(a):`  This line *defines* the function.  It's like giving the calculator a name (`foo`) and specifying that it needs one input (`a`).\n",
      "*   `b = a + 1` This line calculates `a + 1` and stores the result in a variable named `b`.  The `=` sign means \"assign the value on the right to the variable on the left.\"\n",
      "*   `return 2*b` This line calculates `2 * b` and *returns* that value.  The `return` statement is how the function sends the result back to whoever called it.\n",
      "\n",
      "**8.  Mathematical Representation**\n",
      "\n",
      "The function can be represented mathematically as:\n",
      "\n",
      "`foo(a) = 2 * (a + 1)`\n",
      "\n",
      "**9.  Use Cases**\n",
      "\n",
      "This function, in itself, is a simple example.  However, it demonstrates the basic structure of a function: taking input, performing calculations, and returning output.  Such functions are the building blocks of more complex programs.  You might use similar logic in a real-world scenario to calculate costs, adjust values, or perform other numerical operations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CODE_BLOCK2 = \"\"\" \n",
    "# replace this with your own code\n",
    "def foo(a):\n",
    "  b = a + 1\n",
    "  return 2*b\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Please write technical documentation for this code and \\n\n",
    "make it easy for a non Python developer to understand:\n",
    "\n",
    "{question}\n",
    "\n",
    "Output the results in markdown\n",
    "\"\"\"\n",
    "\n",
    "completion = generate_text(\n",
    "    prompt = prompt_template.format(question=CODE_BLOCK2)\n",
    ")\n",
    "print(completion.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd7fd23-141c-477a-97c9-e7723df9d424",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
