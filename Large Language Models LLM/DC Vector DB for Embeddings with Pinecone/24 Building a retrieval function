Building a retrieval function
A key process in the Retrieval Augmented Generation (RAG) workflow is retrieving data from the database. In this exercise, you'll design a custom function called retrieve() that will perform this crucial process in the final exercise of the course.


Initialize the Pinecone client with your API key (the OpenAI client is available as client).
Define the function retrieve that takes four parameters: query, top_k, namespace, and emb_model.
Embed the input query using the emb_model argument.
Retrieve the top_k similar vectors to query_emb with metadata, specifying the namespace provided to the function as an argument.



# Initialize the Pinecone client
pc = Pinecone(api_key="apikey")
index = pc.Index('pinecone-datacamp')

# Define a retrieve function that takes four arguments: query, top_k, namespace, and emb_model
def retrieve(query, top_k, namespace, emb_model):
    # Encode the input query using OpenAI
    query_response = client.embeddings.create(
        input=query,
        model=emb_model
    )
    
    query_emb = query_response.data[0].embedding
    
    # Query the index using the query_emb
    docs = index.query(vector=query_emb, top_k=top_k, namespace=namespace, include_metadata=True)
    
    retrieved_docs = []
    sources = []
    for doc in docs['matches']:
        retrieved_docs.append(doc['metadata']['text'])
        sources.append((doc['metadata']['title'], doc['metadata']['url']))
    
    return retrieved_docs, sources

documents, sources = retrieve(
  query="How to build next-level Q&A with OpenAI",
  top_k=3,
  namespace='youtube_rag_dataset',
  emb_model="text-embedding-3-small"
)
print(documents)
print(sources)





<script.py> output:
    ["and our model into a pipeline, into a Q&A pipeline. So again, we get this pipeline from the Transformers library. So we come down here, do from Transformers, import pipeline. And now what we want to do is just initialize a pipeline object. So to do that, we just write pipeline. And then in here, what we need to add is a model type. So obviously, you can see up here, we have all of these different tasks. So summarization, text generation and so on. The Transformers library needs to understand, or this pipeline object needs to understand which one of those pipelines or functions we are intending to use. So to tell it that we want to do question answering, we just write question answering. And that basically sets the wrapper of the pipeline to handle question answering formats. So we'll see our input and for our input, we will be passing a context and a question. So we'll see that it will convert into the right structure that we need for question answering, which is the CLS context separator, question separator and padding. It will convert into that, feed it into our tokenizer.", "And on the Hugging Face website, we just want to go over to the Models page. So it's here. Okay and on this Models page, the thing that we want to be looking at is this question and answering task. So here we have all these tasks because when you're working with transformers, they can work with a lot of different things. Text summarization, text classification, generation, loads of different things. But what we want to do is question answering. So we click on here and this filters all of the models that are available to us just purely for question and answering. So this is the sort of power of using the Hugging Face Transformers library. It already has all these pre-trained models that we can just download and start using. Now, when you want to go and apply these to specific use cases, you probably want to fine tune it, which means you want to train it a little bit more than what it is already trained. But for actually getting used to how all of this works, all you need to do is download this model and start asking questions and understanding how everything is actually functioning. So obviously there's a lot of models here. We've got 262 models for question answering,", "pulled from a different Wikipedia page. In this case, the title is a titled page. So this one is from the University of Notre Dame Wikipedia page. We have answers. So further down, we're going to ask a question and these answers here. So we have the text, which is the text answer. And then we have the position, so the character position where the answer starts within the context, which is what you can see here. Now we have a question here, which we're asking. And then the model, the Q&A model is going to extract the answer from our context there. Okay. So we're not going to be training model in this video or anything like that. We're just experimenting with the data sets library. We don't need to worry so much about that. So the first thing I want to do is have a look at how we can modify some of the features in our data. So with SQUAD, when we are training a model, one of the first things we would do is we take our answer start and the text and we will use that to get the answer end position as well."]
    [('How to Build Q&A Models in Python (Transformers)', 'https://youtu.be/scJsty_DR3o'), ('How to Build Q&A Models in Python (Transformers)', 'https://youtu.be/scJsty_DR3o'), ('Build NLP Pipelines with HuggingFace Datasets', 'https://youtu.be/r-zQQ16wTCA')]



