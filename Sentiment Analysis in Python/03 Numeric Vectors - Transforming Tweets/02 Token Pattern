Specify the token pattern
In this exercise, you will work with the text column of the tweets dataset. Your task is to vectorize the object column using CountVectorizer. You will apply different patterns of tokens in the vectorizer. Remember that by specifying the token pattern, you can filter out characters.
The CountVectorizer has been imported for you.

Build a vectorizer from the text column, specifying the pattern of tokens to be equal to r'\b[^\d\W][^\d\W]'.
# Build and fit the vectorizer
vect = CountVectorizer(token_pattern=r'\b[^\d\W][^\d\W]').fit(tweets.text)
vect.transform(tweets.text)
print('Length of vectorizer: ', len(vect.get_feature_names()))

<script.py> output:
    Length of vectorizer:  332



Build a vectorizer from the text column using the default values of the function's arguments.
Build a second vectorizer, specifying the pattern of tokens to be equal to r'\b[^\d\W][^\d\W]'.

In [1]: tweets.text
Out[1]: 
0                     @VirginAmerica What @dhepburn said.
1       @VirginAmerica plus you've added commercials t...
2       @VirginAmerica I didn't today... Must mean I n...
3       @VirginAmerica it's really aggressive to blast...
...
996     @united I'm trying to get to my final destinat...
997     @united that guy really has no customer servic...
998                @united he has no priority and Iove it
999              @united Pleased to be a Premier Platinum
1000    @united how can you not put my bag on plane to...
Name: text, Length: 1001, dtype: object



# Build the first vectorizer
vect1 = CountVectorizer().fit(tweets.text)
vect1.transform(tweets.text)

# Build the second vectorizer
vect2 = CountVectorizer(token_pattern=r'\b[^\d\W][^\d\W]').fit(tweets.text)
vect2.transform(tweets.text)

# Print out the length of each vectorizer
print('Length of vectorizer 1: ', len(vect1.get_feature_names()))
print('Length of vectorizer 2: ', len(vect2.get_feature_names()))

<script.py> output:
    Length of vectorizer 1:  3081
    Length of vectorizer 2:  332
    
    
    
Did you notice how fewer features were created when we specified the token pattern? It is a nice way to limit the size of our vocabulary and make sure we only include certain tokens when we create it.

    
    
    





