Specify token sequence length with BOW
We saw in the video that by specifying different length of tokens - what we called n-grams - we can better capture the context, which can be very important.
In this exercise, you will work with a sample of the Amazon product reviews. Your task is to build a BOW vocabulary, using the review column and specify the sequence length of tokens.

Build the vectorizer, specifying the token sequence length to be uni- and bigrams.
Fit the vectorizer.
Transform the fitted vectorizer.
In the DataFrame, make sure to correctly specify the column names.

from sklearn.feature_extraction.text import CountVectorizer 

# Build the vectorizer, specify token sequence and fit
vect = CountVectorizer(ngram_range=(1,2))
vect.fit(reviews.review)

# Transform the review column
X_review = vect.transform(reviews.review)

# Create the bow representation
X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())
print(X_df.head())

<script.py> output:
       10  10 95  10 cups  100  100 years  ...  zelbessdisk  zelbessdisk three  zen  zen baseball  zen motorcycle
    0   0      0        0    0          0  ...            0                  0    0             0               0
    1   0      0        0    0          0  ...            0                  0    0             0               0
    2   0      0        0    0          0  ...            0                  0    0             0               0
    3   0      0        0    0          0  ...            1                  1    0             0               0
    4   0      0        0    0          0  ...            0                  0    0             0               0
    
    [5 rows x 8436 columns]

You have built a numeric representation of the review column using uni- and bigrams!







Size of vocabulary of movies reviews
In this exercise, you will practice different ways to limit the size of the vocabulary using a sample of the movies reviews dataset. The first column is the review, which is of type object and the second column is the label, which is 0 for a negative review and 1 for a positive one.
The three methods that you will use will transform the text column to new numeric columns, capturing the count of a word or a phrase in each review. Each method will ultimately result in building a different number of new features.

Using the movies dataset, limit the size of the vocabulary to 100.
from sklearn.feature_extraction.text import CountVectorizer 

# Build the vectorizer, specify size of vocabulary and fit
vect = CountVectorizer(max_features=100)
vect.fit(movies.review)

# Transform the review column
X_review = vect.transform(movies.review)
# Create the bow representation
X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())
print(X_df.head())

    [5 rows x 200 columns]



Using the movies dataset, limit the size of the vocabulary to include terms which occur in no more than 200 documents.
from sklearn.feature_extraction.text import CountVectorizer 

# Build and fit the vectorizer
vect = CountVectorizer(max_df=200)
vect.fit(movies.review)

# Transform the review column
X_review = vect.transform(movies.review)
# Create the bow representation
X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())
print(X_df.head())

    [5 rows x 17669 columns]







Using the movies dataset, limit the size of the vocabulary to ignore terms which occur in less than 50 documents.
from sklearn.feature_extraction.text import CountVectorizer 

# Build and fit the vectorizer
vect = CountVectorizer(min_df=50)
vect.fit(movies.review)

# Transform the review column
X_review = vect.transform(movies.review)
# Create the bow representation
X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())
print(X_df.head())

    [5 rows x 434 columns]
Any of the three methods you applied here can be used to limit the size of the vocabulary. Which of the three methods you used resulted in the lowest number of constructed features?













BOW with n-grams and vocabulary size
In this exercise, you will practice building a bag-of-words once more, using the reviews dataset of Amazon product reviews. Your main task will be to limit the size of the vocabulary and specify the length of the token sequence.

Import the vectorizer from sklearn.
Build the vectorizer and make sure to specify the following parameters: the size of the vocabulary should be limited to 1000, include only bigrams, and ignore terms that appear in more than 500 documents.
Fit the vectorizer to the review column.
Create a DataFrame from the BOW representation.

#Import the vectorizer
from sklearn.feature_extraction.text import CountVectorizer 

# Build the vectorizer, specify max features and fit
vect = CountVectorizer(max_features=1000, ngram_range=(2, 2), max_df=500)
vect.fit(reviews.review)

# Transform the review
X_review = vect.transform(reviews.review)

# Create a DataFrame from the bow representation
X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())
print(X_df.head())

<script.py> output:
       1980 style  aa batteries  aaa batteries  able to  about the  ...  you want  you will  your imagination  your money  yr old
    0           0             0              0        0          0  ...         0         0                 0           0       0
    1           0             0              0        0          0  ...         0         0                 0           0       0
    2           0             0              0        0          0  ...         0         0                 0           0       0
    3           0             0              0        0          0  ...         0         0                 0           0       0
    4           0             0              0        0          0  ...         0         0                 0           0       0
    
    [5 rows x 1000 columns]
    
    

You have successfully created a bag-of-words representation of the product reviews dataset, including more sophisticated sequence of tokens, while limiting the size of the vocabulary

