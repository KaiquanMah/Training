Hierarchical agglomerative clustering
In the last exercise, you saw how the number of clusters while performing K-means clustering could impact your results allowing you to discuss K-means in a machine learning interview. However, another clustering model you can use is hierarchical agglomerative clustering. In Python, you could derive the optimal number of clusters for this technique both visually and mathematically. You will the scipy and sklearn modules to do both.
Recall that selecting the optimal number of clusters from a dendrogram depends on both the linkage criteria and distance threshold. Here, you'll create a dendrogram with the X matrix from diabetes, then extend an imaginary line at length 1.50, counting the number of vertical lines crossed to represent the optimal number of clusters for your hierarchical clustering algorithm going forward.


Import the relevant packages to create a dendrogram and perform agglomerative hierarchical clustering.
# Import hierarchical clustering libraries
import scipy.cluster.hierarchy as sch
from sklearn.cluster import AgglomerativeClustering






Create a dendrogram on X using the ward linkage method.
# Import hierarchical clustering libraries
import scipy.cluster.hierarchy as sch
from sklearn.cluster import AgglomerativeClustering

# Create dendrogram
dendrogram = sch.dendrogram(sch.linkage(X, method='ward'))
plt.show()







Instantiate an agglomerative clustering cluster cluster object, then fit it to the data matrix X.
# Import hierarchical clustering libraries
import scipy.cluster.hierarchy as sch
from sklearn.cluster import AgglomerativeClustering

# Create dendrogram
dendrogram = sch.dendrogram(sch.linkage(X, method='ward'))
plt.show()

# Create clusters and fit
hc = AgglomerativeClustering(affinity = 'euclidean', linkage = 'ward')
hc.fit(X)








Print the number of clusters automatically chosen through agglomerative clustering.
# Import hierarchical clustering libraries
import scipy.cluster.hierarchy as sch
from sklearn.cluster import AgglomerativeClustering

# Create dendrogram
dendrogram = sch.dendrogram(sch.linkage(X, method='ward'))
plt.show()

# Create clusters and fit
hc = AgglomerativeClustering(affinity = 'euclidean', linkage = 'ward')
hc.fit(X)

# Print number of clusters
print(hc.n_clusters_)



<script.py> output:
    2
    
    
    
Excellent! You might be surprised by the fact that both the dendrogram and the AgglomerativeClustering() algorithm selected 2 clusters with diabetes, even though the .inertia_ using KMeans was the highest with n_clusters=2. We'll investigate this more in the next lesson on how to select the best value for k!

    
