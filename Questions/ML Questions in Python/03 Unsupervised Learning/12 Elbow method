Elbow method
The silhouette score and elbow methods provide validation to the results you see from each. In a machine learning interview setting, you may get asked about one or the other but discussing the specifics of one in the context of the other would contribute greatly to your answer.
In this exercise, for each k value, you will initialize a K-means model and use the inertia_ attribute to identify the sum of squared distances of samples to the nearest cluster center that, when plotted, appears to be an 'elbow' in the plot.
Recall in the last lesson that as the value of k increased, the value for inertia_ decreased.
Already imported for you are matplotlib.pyplot as plt and KMeans from sklearn.cluster. The feature matrix X has also been created for you.





Create an empty list named sum_of_squared_distances.
# Create empty list
sum_of_squared_distances = []




Instantiate and fit a K-Means object for each number of clusters ranging from 1 to 14.
Append the inertia score for each iteration of K-Means to sum_of_squared_distances.
# Create empty list
sum_of_squared_distances = []

# Create for loop
for k in range(1,15):
    kmeans = KMeans(n_clusters=k)
    kmeans = kmeans.fit(X)
    sum_of_squared_distances.append(kmeans.inertia_)






Create a plot with the number of clusters on the x-axis, and the sum of squared distances on the y-axis.
# Create empty list
sum_of_squared_distances = []

# Create for loop
for k in range(1,15):
    kmeans = KMeans(n_clusters=k)
    kmeans = kmeans.fit(X)
    sum_of_squared_distances.append(kmeans.inertia_)

# Plot
plt.plot(range(1,15), sum_of_squared_distances, 'bx-')
plt.xlabel('k')
plt.ylabel('Sum of squared distances')
plt.title('Elbow Method For Optimal k')
plt.show()


In [2]: sum_of_squared_distances
Out[2]: 
[10.000000000000547,
 7.187000018583656,
 6.514486940354866,
 5.976940562956073,
 5.550099167667501,
 5.2238673544758765,
 4.9779440294928605,
 4.77038339870596,
 4.558544637122533,
 4.409102016663596,
 4.267369239659365,
 4.1659648739153425,
 4.012294079931476,
 3.9324770052966196]



Fantastic! The AgglomerativeClustering(), dendrogram, silhouette score, and elbow method all agree! The best value for k with the diabetes dataset is 2! And really that makes sense if you think about it. The target variable progression, even though it is continuous, can be thought of as belonging to one of 2 groups. When compared to a previous baseline measurement, a given person's diabetes has either progressed or it hasn't!

