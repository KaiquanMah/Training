Principal component analysis
In the last 2 chapters, you saw various instances about how to reduce the dimensionality of your dataset including regularization and feature selection. It is important to be able to explain different aspects of reducing dimensionality in a machine learning interview. Large datasets take a long time to compute, and noise in your data can bias your results.
One way of reducing dimensionality is principal component analysis. It's an effective way of reducing the size of the data by creating new features that preserve the most useful information on a dataset while at the same time removing multicollinearity. In this exercise, you will be using the sklearn.decomposition module to perform PCA on the features of the diabetes dataset while isolating the target variable progression.


Import the relevant module to perform PCA.
Create a feature matrix X and target array y with progression from the diabetes dataset.
# Import module
from sklearn.decomposition import PCA

# Feature matrix and target array
X = diabetes.drop('progression', axis=1)
y = diabetes['progression']




Instantiate a principal component analysis object to perform linear dimensionality reduction that returns 3 components.
# PCA
pca = PCA(n_components=3)





Fit and transform the feature matrix.
# Fit and transform
principalComponents = pca.fit_transform(X)





Print the ratio of variance explained.
# Print ratio of variance explained
print(pca.explained_variance_ratio_)


<script.py> output:
    [0.40242142 0.14923182 0.12059623]
    
    
    Nice job! The first PC explains 40% of the variance with all 3 around 67%. Let's see if singular value decomposition gives different results!

