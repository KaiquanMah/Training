Singular value decomposition
In the last exercise, you saw how useful PCA could be in reducing the dimensions of a dataset when you're given a question about high dimensionality in a machine learning interview.
In this exercise, you'll practice SVD on the diabetes. This particular transformer can work with sparse matrices efficiently, as opposed to PCA, and performs linear dimensionality reductions by way of truncated singular value decomposition.
Recall that singular value decomposition takes the original data matrix, decomposes it into three matrices and uses them to calculate and return singular values.



Import the relevant module to perform SVD.
Create a feature matrix X and target array y with progression from the diabetes dataset.
# Import module
from sklearn.decomposition import TruncatedSVD

# Feature matrix and target array
X = diabetes.drop('progression', axis=1)
y = diabetes['progression']





Instantiate a singular value decomposition object to perform dimensionality reduction that returns 3 components.
# SVD
svd = TruncatedSVD(n_components = 3)





Fit and transform the feature matrix.
# Fit and transform
principalComponents = svd.fit_transform(X)





Print the ratio of variance explained.

# Print ratio of variance explained
print(svd.explained_variance_ratio_)


<script.py> output:
    [0.40242142 0.14923182 0.12059623]


Excellent! You've learned feature extraction techniques using dimensionality reduction methods PCA and SVD. It looks like they both perform exactly the same with this dataset! But you never know so trying both is a good idea. Whenever you're given high dimensional data, you know what to do!

