K-means clustering
In a machine learning interview setting, you might be asked how the output from K-means clustering might be used to assess its performance as the best algorithm.
In this exercise you'll practice K-means clustering. Using the .inertia_ attribute to compare models with different numbers of clusters, k, you'll then also use this information to assess cluster number in the next exercise.
Recall that the target variable in the diabetes dataset is progression.


Create a feature matrix X by dropping the target variable progression and fit the data to the instantiated k-means object.
# Import module
from sklearn.cluster import KMeans

# Create feature matrix
X = diabetes.drop("progression", axis=1)

# Instantiate
kmeans = KMeans(n_clusters=2, random_state=123)

# Fit
fit = kmeans.fit(X)

# Print inertia
print("Sum of squared distances for 2 clusters is", kmeans.inertia_)


<script.py> output:
    Sum of squared distances for 2 clusters is 7.187000018583656
    
    
    
    
    
    
    
    
    

Instantiate a 5 cluster k-means and print its inertia.
# Import module
from sklearn.cluster import KMeans

# Create feature matrix
X = diabetes.drop("progression", axis=1)

# Instantiate
kmeans = KMeans(n_clusters=5, random_state=123)

# Fit
fit = kmeans.fit(X)

# Print inertia
print("Sum of squared distances for 5 clusters is", kmeans.inertia_)

<script.py> output:
    Sum of squared distances for 5 clusters is 5.554112564296045
    
    
    
    
    
    

Fit the feature matrix to a 10-cluster k-means and print its inertia.
# Import module
from sklearn.cluster import KMeans

# Create feature matrix
X = diabetes.drop("progression", axis=1)

# Instantiate
kmeans = KMeans(n_clusters=10, random_state=123)

# Fit
fit = kmeans.fit(X)

# Print inertia
print("Sum of squared distances for 10 clusters is", kmeans.inertia_)

<script.py> output:
    Sum of squared distances for 10 clusters is 4.36463117911814







Fit the feature matrix to a 20-cluster k-means and print its inertia.
# Import module
from sklearn.cluster import KMeans

# Create feature matrix
X = diabetes.drop("progression", axis=1)

# Instantiate
kmeans = KMeans(n_clusters=20, random_state=123)

# Fit
fit = kmeans.fit(X)

# Print inertia
print("Sum of squared distances for 20 clusters is", kmeans.inertia_)

<script.py> output:
    Sum of squared distances for 20 clusters is 3.516404544955704

Nice job! Which value of k returned the lowest inertia? You'll compare it to the number selected by hierarchical clustering in this next exercise.

