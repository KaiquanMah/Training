Gradient boosting ensemble
Boosting is a technique where the error of one predictor is passed as input to the next in a sequential manner. Gradient Boosting uses a gradient descent procedure to minimize the log loss for each subsequent classification tree added one at a time that, on their own, are weak decision models. Gradient Boosting for regression is similar, but uses a loss function such as mean squared error applied to gradient descent.
In this exercise, you will create a Gradient Boosting Classifier model and compare its performance to the Random Forest from the previous exercise, which had an accuracy score of 72.5%.
The loan_data DataFrame has already been split is available in your workspace as X_train, X_test, y_train, and y_test.



Import the modules to create a Gradient Boosting model and print out the confusion matrix, accuracy, precision, recall, and F1-scores.
Instantiate a GB classifier and set the appropriate argument to generate 50 estimators and with a learning rate of 0.01.
# Import
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

# Instantiate
gb_model = GradientBoostingClassifier(n_estimators=50, learning_rate=0.01,random_state=123)





Fit the data and create predictions.
gb_model.fit(X_train, y_train)
gb_pred = gb_model.predict(X_test)




Evaluate the model fit by printing trained model evaluation metrics.
# Print evaluation metrics
print("Gradient Boosting Accuracy: {}".format(accuracy_score(y_test, gb_pred)))
print("Confusion matrix:\n {}".format(confusion_matrix(y_test, gb_pred)))
print("Precision: {}".format(precision_score(y_test, gb_pred)))
print("Recall: {}".format(recall_score(y_test, gb_pred)))
print("F1: {}".format(f1_score(y_test, gb_pred)))



<script.py> output:
    Gradient Boosting Accuracy: 0.7162
    Confusion matrix:
     [[    0  4257]
     [    0 10743]]
    Precision: 0.7162
    Recall: 1.0
    F1: 0.834634657965272










Pick the ensemble model that had the best accuracy.
Gradient Boosting

#yes - Random Forest


Outstanding! You've learned how to effectively deal with missing data, avoid under or overfitting, apply transformations, among many other best practices in the step-by-step process of creating the best machine learning models. With continued practice, you will be able to ace your next Machine Learning Interview! Congratulations!

