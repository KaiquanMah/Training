Random forest vs gradient boosting
What are the main similarities and differences of Random Forest (RF) and Gradient Boosting (GB)algorithms?


Select the answer that is false:
Random Forest and Gradient Boosting machine learning techniques create multiple random samples that are used to produce a final prediction model.
Random Forest uses the bootstrapping method while Gradient Boosting uses weights given to incorrectly predicted observations from a previous sample applied to subsequent samples.
The final prediction of Random Forest uses a decision tree and is an average of all generated bootstrap samples, while the final prediction of Gradient Boosting is a weighted average of the generated weak learners and can use any algorithm.

#yes - Random Forest and Gradient Boosting can use any algorithm, not just decision trees.
That's correct! While GB can use any algorithm, RF uses decision trees!

