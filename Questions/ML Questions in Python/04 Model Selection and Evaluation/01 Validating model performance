Validating model performance
How do you ensure your model will perform well against test (unseen) data?


Select the answer that is false:
#not - Perform K-fold cross-validation to tune hyperparameters during machine learning model training.
Yes, almost any machine learning algorithm will benefit from cross-validation methods used to best tune model hyperparameters.

#not - Use a bootstrapping method, where a subset of the data is selected with replacement, so that the output of averaged predictions produces a more accurate model from the reduction in variance.
Absolutely! Boostrapping methods, also known as ensemble methods,include RandomForestClassifier and BaggingClassifier and their complementary Regressors used for continuous target variable predictions, better ensure model generalizability during model training.

#not - Create a Random Forest using GridSearchCV to find the best hyperparameters and create a final model with them before evaluating on the test data.
Indeed! You can also use RandomizedSearchCV to find the best hyperparameters, just be aware it takes more computing power to search every combination of parameters.

#yes - Create a cross-validated Decision Tree model, then evaluate its performance on the test set.
Not necessarily! A Decision Tree model is unfortunately a greedy algorithm, not guaranteed to return the optimal tree whether or not it's cross-validated. You're better off to combine cross-validation with a bootstrap method.



