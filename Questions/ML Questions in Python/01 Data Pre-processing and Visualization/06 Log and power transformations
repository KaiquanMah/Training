Log and power transformations
In the last exercise, you compared the distributions of a training set and test set of loan_data. This is especially poignant in a machine learning interview because the distribution observed dictates whether or not you need to use techniques which nudge your feature distributions toward a normal distribution so that normality assumptions are not violated.
In this exercise, you will be using the log and power transformation from the scipy.stats module on the Years of Credit History feature of loan_data along with the distplot() function from seaborn, which plots both its distribution and kernel density estimation.
All relevant packages have been imported for you.

Here is where you are in the pipeline:
pre-process data



Subset loan_data for 'Years of Credit History' and plot its distribution and kernel density estimation (kde) using distplot().
# Subset loan_data
cr_yrs = loan_data['Years of Credit History']

# Histogram and kernel density estimate
plt.figure()
sns.distplot(cr_yrs)
plt.show()







Apply a log transformation using the Box-Cox transformation to cr_yrs and plot its distribution and kde.
# Subset loan_data
cr_yrs = loan_data['Years of Credit History']

# Box-Cox transformation
cr_yrs_log = boxcox(cr_yrs, lmbda=0.0)

# Histogram and kernel density estimate
plt.figure()
sns.distplot(cr_yrs_log)
plt.show()









Transform 'Years of Credit History' using the Box-Cox square-root argument and plot its distribution and kde.
# Subset loan_data
cr_yrs = loan_data['Years of Credit History']

# Square root transform
cr_yrs_sqrt = boxcox(cr_yrs, lmbda=0.5)

# Histogram and kernel density estimate
plt.figure()
sns.distplot(cr_yrs_sqrt)
plt.show()




Excellent! Now you know exactly how to apply data transformations and quickly compare before and after plots to improve data going into your machine learning models!



