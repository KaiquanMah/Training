Filter and wrapper methods
Questions about reducing the dimensionality of a dataset are highly common in machine learning interviews. One way to reduce the dimensionality of a dataset is by only selecting relevant features in your dataset.
Here you'll practice a filter method on the diabetes DataFrame followed by 2 different styles of wrapper methods that include cross-validation. You will be using pandas, matplotlib.pyplot and seaborn to visualize correlations, process your data and apply feature selection techniques to your dataset.
The feature matrix with the dropped target variable column (progression) is loaded as X, while the target variable is loaded as y.
Note that pandas, matplotlib.pyplot, and seaborn have already been imported to your workspace and aliased as pd, plt, and sns respectively.
Notice you've added a Cross-validate step to your pipeline.




Create correlation matrix with diabetes and a heatmap, then subset the features which have greater than 50% correlation.
# Create correlation matrix and print it
cor = diabetes.corr()
print(cor)

                  age       sex       bmi        bp        s1  ...        s3        s4        s5        s6  progression
age          1.000000  0.173737  0.185085  0.335427  0.260061  ... -0.075181  0.203841  0.270777  0.301731     0.187889
sex          0.173737  1.000000  0.088161  0.241013  0.035277  ... -0.379090  0.332115  0.149918  0.208133     0.043062
bmi          0.185085  0.088161  1.000000  0.395415  0.249777  ... -0.366811  0.413807  0.446159  0.388680     0.586450
bp           0.335427  0.241013  0.395415  1.000000  0.242470  ... -0.178761  0.257653  0.393478  0.390429     0.441484
s1           0.260061  0.035277  0.249777  0.242470  1.000000  ...  0.051519  0.542207  0.515501  0.325717     0.212022
s2           0.219243  0.142637  0.261170  0.185558  0.896663  ... -0.196455  0.659817  0.318353  0.290600     0.174054
s3          -0.075181 -0.379090 -0.366811 -0.178761  0.051519  ...  1.000000 -0.738493 -0.398577 -0.273697    -0.394789
s4           0.203841  0.332115  0.413807  0.257653  0.542207  ... -0.738493  1.000000  0.617857  0.417212     0.430453
s5           0.270777  0.149918  0.446159  0.393478  0.515501  ... -0.398577  0.617857  1.000000  0.464670     0.565883
s6           0.301731  0.208133  0.388680  0.390429  0.325717  ... -0.273697  0.417212  0.464670  1.000000     0.382483
progression  0.187889  0.043062  0.586450  0.441484  0.212022  ... -0.394789  0.430453  0.565883  0.382483     1.000000

[11 rows x 11 columns]




# Correlation matrix heatmap
plt.figure()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()

# Correlation with output variable
cor_target = abs(cor["progression"])

In [4]: cor_target
Out[4]: 
age            0.187889
sex            0.043062
bmi            0.586450
bp             0.441484
s1             0.212022
s2             0.174054
s3             0.394789
s4             0.430453
s5             0.565883
s6             0.382483
progression    1.000000
Name: progression, dtype: float64




# Selecting highly correlated features
best_features = cor_target[cor_target > 0.5]
print(best_features)

<script.py> output:
    bmi            0.586450
    s5             0.565883
    progression    1.000000
    Name: progression, dtype: float64
    
    
    
    
    
    
    
    
    
    
    

Instantiate a linear kernel SVR estimator and a feature selector with 5 cross-validations, fit to features and target.
# Import modules
from sklearn.svm import SVR
from sklearn.feature_selection import RFECV

# Instantiate estimator and feature selector
svr_mod = SVR(kernel="linear")
#https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html
#sklearn.feature_selection.RFECV(estimator, step=1, min_features_to_select=1, cv=None, scoring=None, verbose=0, n_jobs=None)
feat_selector = RFECV(svr_mod, cv=5)

# Fit
feat_selector = feat_selector.fit(X, y)

# Print support and ranking
print(feat_selector.support_)
print(feat_selector.ranking_)
print(X.columns)

<script.py> output:
    [ True False  True  True  True  True  True  True  True  True]
    [1 2 1 1 1 1 1 1 1 1]
    Index(['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6'], dtype='object')












Drop the unimportant column found in step 2 from X and instantiate a LarsCV object and fit it to your data.
# Import modules
from sklearn.linear_model import LarsCV

# Drop feature suggested not important in step 2
X = X.drop('sex', axis=1)

# Instantiate
lars_mod = LarsCV(cv=5, normalize=False)

# Fit
feat_selector = lars_mod.fit(X, y)

# Print r-squared score and estimated alpha
print(lars_mod.score(X, y))
print(lars_mod.alpha_)

<script.py> output:
    0.4982900996498095
    0.05226862285159132

Nice job! While filter and wrapper methods work pretty well for feature selection, tree-based selection methods can be even better. You'll try them out in the next exercise!

