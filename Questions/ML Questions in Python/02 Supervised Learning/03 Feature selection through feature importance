Feature selection through feature importance
In the last exercise, you practiced how filter and wrapper methods could be of use when selecting features in machine learning, and in machine learning interviews. In this exercise, you'll practice feature selection methods using the built-in feature importance in tree-based machine learning algorithms on the diabetes DataFrame.
Although there is only time and space to practice with a few of them on DataCamp, there is some excellent documentation available from the scikit-learn website that goes over several other ways to select features.
https://scikit-learn.org/stable/modules/feature_selection.html
The feature matrix and target array are saved to your workspace as X and y, respectively.
Recall that feature selection is considered a pre-processing step.




Import the correct function to instantiate a Random Forest regression model.
Fit the model and print feature importance.
# Import
from sklearn.ensemble import RandomForestRegressor

# Instantiate
rf_mod = RandomForestRegressor(max_depth=2, random_state=123, 
              n_estimators=100, oob_score=True)

# Fit
rf_mod.fit(X, y)

# Print
print(diabetes.columns)
print(rf_mod.feature_importances_)

<script.py> output:
    Index(['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6', 'progression'], dtype='object')
    [0.         0.         0.39961579 0.03085607 0.         0.00128948
     0.00700339 0.00417891 0.53899467 0.01806168]
     
     
     
     
     
     
     
     


Import the correct function to instantiate an Extra Tree regression model.
Fit the model and print feature importance.
# Import
from sklearn.ensemble import ExtraTreesRegressor

# Instantiate
xt_mod = ExtraTreesRegressor()

# Fit
xt_mod.fit(X, y)

# Print
print(diabetes.columns)
print(xt_mod.feature_importances_)

<script.py> output:
    Index(['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6', 'progression'], dtype='object')
    [0.06184751 0.03494109 0.28094491 0.13227859 0.04057116 0.0562529
     0.05587508 0.07300182 0.1922598  0.07202714]


Amazing! Though both the Random Forest and Extra Trees ultimately methods have similar results, the output from the Random Forest are a bit more clear. Either way, one of the simplest ways to have an algorithm select features for you before training your machine learning models are tree-based methods such as these. Now let's learn the ins-and-outs of regularization methods in the next lesson...

