XG Boost
In this exercise, you'll practice yet another boosting technique. Dubbed the new queen of Machine Learning, XGBoost is an optimized distributed gradient boosting package that is "taking over the world!". That said, getting asked about it in a Machine Learning interview is likely or, at the very least, would be to your benefit to discuss in one of your answers to display your knowledge of cutting-edge and highly accurate algorithms.
The argument learning_rate=0.1 specifies the size of the step to take in each iteration while searching for the global minimum and max_depth controls the size (depth) of the decision trees, here 3.
All relevant packages have been imported for you: pandas as pd, train_test_split from sklearn.model_selection, accuracy_score from sklearn.linear_model, LogisticRegression from sklearn.linear_model, BaggingClassifier and AdaBoostClassifier from sklearn.ensemble, and XGBClassifier from xgboost.
The loan_data DataFrame is already split into X_train, X_test, y_train and y_test.





Instantiate an XGBoost boosting classifier and set the appropriate argument to generate 10 estimators.
# Instantiate
xgb = XGBClassifier(n_estimators=10, random_state=123, learning_rate=0.1, max_depth=3)




Fit the data.
# Fit
xgb = xgb.fit(X_train, y_train)



Create predictions using the test data.
# Predict
xgb_pred = xgb.predict(X_test)



Evaluate the model fit.
# Instantiate
xgb = XGBClassifier(random_state=123, learning_rate=0.1, n_estimators=10, max_depth=3)

# Fit
xgb = xgb.fit(X_train, y_train)

# Predict
xgb_pred = xgb.predict(X_test)

# Print accuracy score
print('Final prediction score: [%.8f]' % accuracy_score(y_test, xgb_pred))


<script.py> output:
    Final prediction score: [0.71960000]
    
    
    Outstanding job! The AdaBoost Classifier did slightly better than XGBoost with this dataset. But that's not always the case, so make sure to try both! That was the final exercise for this chapter, let's move on to Chapter 3 on Unsupervised Learning!

