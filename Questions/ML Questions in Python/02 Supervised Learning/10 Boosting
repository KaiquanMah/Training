Boosting
In the last exercise, you started off with ensemble techniques by using bagging. In a machine learning interview, you might be prompted to try out or discuss more than one ensemble technique.
Here, you'll practice Boosting which uses all data to train each learner, but instances that were misclassified by the previous learners are given more weight so that subsequent learners give more focus to them during training. This results in a model with decreased bias.
All relevant packages have been imported for you: pandas as pd, train_test_split from sklearn.model_selection, accuracy_score from sklearn.linear_model, LogisticRegression from sklearn.linear_model, and BaggingClassifier and AdaBoostClassifier from sklearn.ensemble.
The loan_data DataFrame is already split into X_train, X_test, y_train and y_test.





Instantiate an AdaBoost boosting classifier and set the appropriate argument to generate 50 estimators.
# Boosting model
boosted_model = AdaBoostClassifier(n_estimators=50, random_state=123)




Fit the data.
# Fit
boosted_model_fit = boosted_model.fit(X_train, y_train)




Create predictions using the test set.
# Predict
boosted_pred = boosted_model_fit.predict(X_test)




Evaluate the model fit.
# Print model accuracy
print(accuracy_score(y_test, boosted_pred))

<script.py> output:
    0.72408
    
    Nice job! The boosted model did slightly better than the bagged model, but let's see what happens when we stack them. Onward!

