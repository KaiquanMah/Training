Bootstrap aggregation (bagging)
In the last lesson, you got a small taste of classification models by applying logistic regression on data with engineered features. In machine learning interviews, it's sometimes worthwhile to know about ensemble models since they combine weak learners to create a strong learner that improves model accuracy.
In this exercise, you will start off by applying a bagging classifier which uses a sampling technique, with replacement, to maintain randomness and reduce overfitting. You will be using functions from the sklearn.ensemble module which you saw in the video exercise.
All relevant packages have been imported for you: pandas as pd, train_test_split from sklearn.model_selection, accuracy_score from sklearn.linear_model, LogisticRegression from sklearn.linear_model, and BaggingClassifier and AdaBoostClassifier from sklearn.ensemble.
The loan_data DataFrame is already split into X_train, X_test, y_train and y_test.





Instantiate a bagging classifier by calling the appropriate function as introduced in the video exercise and set the appropriate argument for 50 estimators.
# Instantiate bootstrap aggregation model
bagged_model = BaggingClassifier(n_estimators=50, random_state=123)





Fit the model object to the data.
# Fit
bagged_model.fit(X_train, y_train)




Create predictions using the test set.
# Predict
bagged_pred = bagged_model.predict(X_test)




Evaluate the model fit.
# Print accuracy score
print(accuracy_score(y_test, bagged_pred))


<script.py> output:
    0.70448
    
    Well done! Bagged models decrease model variance, so this model should generalize nicely!

