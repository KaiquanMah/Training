Classification evaluation
Moving forward with evaluation metrics, this time you'll evaluate our logistic regression model from before with the goal of predicting the binary RainTomorrow feature using humidity.
We have gone ahead and imported the model as clf and the same test sets assigned to the X_test and y_test variables. Generate and analyze the confusion matrix and then compute both precision and recall before making a conclusion.


Generate and print out the confusion matrix for your model; identify the Type I and Type II errors.
# Generate and output the confusion matrix
from sklearn.metrics import confusion_matrix
preds = clf.predict(X_test)
matrix = confusion_matrix(y_test, preds)
print(matrix)

<script.py> output:
    [[185   0]
     [ 51   4]]
     
     
     
     

Compute and print the precision of your model; can you explain why precision is helpful in this context?
# Compute and print the precision
from sklearn.metrics import precision_score
preds = clf.predict(X_test)
precision = precision_score(y_test, preds)
print(precision)

<script.py> output:
    1.0
    
    
    
    
    
    
Adapt your code to compute and print the recall of your model; what do you conclude?
# Compute and print the recall
from sklearn.metrics import recall_score
preds = clf.predict(X_test)
recall = recall_score(y_test, preds)
print(recall)

<script.py> output:
    0.07272727272727272

Good work! You can see here that the precision of our rain prediction model was quite high, meaning that we didn't make too many Type I errors. However, there were plenty of Type II errors shown in the bottom-left quadrant of the confusion matrix. This is indicated further by the low recall score, meaning that there were plenty of rainy days that we missed out on. Think a little about the context and what method you would choose to optimize for!


