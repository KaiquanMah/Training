Visualizing the tradeoff
We know that the bias-variance tradeoff serves as the basis for dealing with issues like overfitting and underfitting in machine learning.
In this final exercise, you'll revisit our weather dataset one last time by visualizing the difference between high bias and high variance models using the already imported preds and preds2 variables.
As a reminder, we are using the Temp9am feature to predict our dependent variable, the Temp3pm feature. The usual packages have been imported.



Visualize the relationship between the imported X and y variables using the scatter() function.
# Use X and y to create a scatterplot
plt.scatter(X, y)
plt.show()




Add the simple linear regression predictions saved as preds to your scatter plot; this is the model you created earlier in the chapter.
# Use X and y to create a scatterplot
plt.scatter(X, y)

# Add your model predictions to the scatter plot 
plt.plot(np.sort(X), preds)
plt.show()





Similarly, add the higher-complexity predictions saved as preds2 to your scatter plot; observe the difference between the two fits.
# Use X and y to create a scatterplot
plt.scatter(X, y)

# Add your model predictions to the scatter plot 
plt.plot(np.sort(X), preds)

# Add the higher-complexity model predictions as well
plt.plot(np.sort(X), preds2)
plt.show()



Great job! This plot does an excellent job of showing both high-bias and high-variance models. Your original simple linear regression fit (the straight blue line) has higher bias, while the higher-order polynomial model is much more flexible to new observations and therefore has higher variance. In most cases, you'll find that the best result comes somewhere in between these examples - hence the tradeoff!

