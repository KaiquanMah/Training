Log-odds scale
Previously, we considered two formulations of logistic regression models:
on the probability scale, the units are easy to interpret, but the function is non-linear, which makes it hard to understand
on the odds scale, the units are harder (but not impossible) to interpret, and the function in exponential, which makes it harder (but not impossible) to interpret

We'll now add a third formulation:
on the log-odds scale, the units are nearly impossible to interpret, but the function is linear, which makes it easy to understand
As you can see, none of these three is uniformly superior. Most people tend to interpret the fitted values on the probability scale and the function on the log-odds scale. The interpretation of the coefficients is most commonly done on the odds scale. Recall that we interpreted our slope coefficient β1 in linear regression as the expected change in y given a one unit change in x. On the probability scale, the function is non-linear and so this approach won't work. On the log-odds, the function is linear, but the units are not interpretable (what does the log of the odds mean??). However, on the odds scale, a one unit change in x leads to the odds being multiplied by a factor of β1. To see why, we form the odds ratio:
https://en.wikipedia.org/wiki/Odds_ratio
OR = odds(y^|x+1) / odds(y^|x) = exp^β1

Thus, the exponentiated coefficent β1 tells us how the expected odds change for a one unit increase in the explanatory variable. It is tempting to interpret this as a change in the expected probability, but this is wrong and can lead to nonsensical predictions (e.g. expected probabilities greater than 1).



Add a variable called log_odds to MedGPA_binned that records the odds of being accepted for each bin. Recall that odds(p) = p/(1−p).
Create a scatterplot called data_space for log_odds as a function of mean_GPA using the binned data in MedGPA_binned. Use geom_line to connect the points.
Add a variable called log_odds_hat to MedGPA_plus that records the predicted odds of being accepted for each observation.
Use geom_line() to illustrate the model through the fitted values. Note that you should be plotting the logoddsˆ's.

# compute log odds for bins
MedGPA_binned <- MedGPA_binned %>%
  mutate(log_odds = log(acceptance_rate/(1-acceptance_rate)))

# plot binned log odds
data_space <- MedGPA_binned %>%
  ggplot(aes(x=mean_GPA, y=log_odds)) +
  geom_point() +
  geom_line()

# compute log odds for observations
MedGPA_plus <- MedGPA_plus %>%
  mutate(log_odds_hat = log(.fitted/(1-.fitted)))

# logistic model on log odds scale
data_space +
  geom_line(data=MedGPA_plus,
            aes(x=GPA, y=log_odds_hat), 
            color = "red")



> MedGPA_binned
# A tibble: 6 x 4
  bin         mean_GPA acceptance_rate log_odds
  <fct>          <dbl>           <dbl>    <dbl>
1 [2.72,3.3]      3.11           0.2     -1.39 
2 (3.3,3.44]      3.39           0.2     -1.39 
3 (3.44,3.58]     3.54           0.75     1.10 
4 (3.58,3.7]      3.65           0.333   -0.693
5 (3.7,3.87]      3.79           0.889    2.08 
6 (3.87,3.97]     3.91           1      Inf


> MedGPA_plus
   Acceptance  GPA    .fitted    .se.fit     .resid       .hat   .sigma
1           0 3.62 0.63124876 0.07900903 -1.4125389 0.02681777 1.026465
2           1 3.84 0.85036851 0.07009771  0.5693601 0.03862222 1.042388
3           1 3.23 0.16944765 0.08614814  1.8842565 0.05274316 1.010435
4           1 3.69 0.71491359 0.07865471  0.8192601 0.03035531 1.039108
5           1 3.38 0.31617156 0.09302470  1.5175443 0.04002682 1.023193
6           1 3.72 0.74706022 0.07810909  0.7636877 0.03228887 1.039936
7           1 3.89 0.88186412 0.06421649  0.5014325 0.03959016 1.043084
8           0 3.34 0.27099332 0.09364731 -0.7950753 0.04439531 1.039392
9           1 3.71 0.73661581 0.07834295  0.7819064 0.03163655 1.039671
10          1 3.89 0.88186412 0.06421649  0.5014325 0.03959016 1.043084
11          1 3.97 0.92030775 0.05330346  0.4075467 0.03874979 1.043904
12          1 3.49 0.45723876 0.08503889  1.2510392 0.02913975 1.030561
13          1 3.77 0.79506038 0.07597224  0.6772551 0.03542586 1.041112
14          0 3.61 0.61846455 0.07910537 -1.3882014 0.02651944 1.027126
15          0 3.30 0.23009846 0.09243699 -0.7231772 0.04823841 1.040428
16          1 3.54 0.52528952 0.08141251  1.1347297 0.02657995 1.033257
17          0 3.65 0.66845438 0.07887704 -1.4859272 0.02807332 1.024388
18          1 3.54 0.52528952 0.08141251  1.1347297 0.02657995 1.033257
19          0 3.25 0.18535740 0.08842742 -0.6403215 0.05179238 1.041510
20          1 3.89 0.88186412 0.06421649  0.5014325 0.03959016 1.043084
21          1 3.71 0.73661581 0.07834295  0.7819064 0.03163655 1.039671
22          0 3.77 0.79506038 0.07597224 -1.7804718 0.03542586 1.014818
23          1 3.91 0.89276358 0.06160154  0.4763055 0.03964509 1.043319
24          1 3.88 0.87606259 0.06547654  0.5144273 0.03949188 1.042957
25          0 3.68 0.70366831 0.07874700 -1.5596640 0.02973960 1.022176
26          1 3.56 0.55238896 0.08040711  1.0894979 0.02614823 1.034223
27          0 3.44 0.39074729 0.08930107 -0.9955121 0.03349887 1.036021
28          1 3.58 0.57918073 0.07969300  1.0451227 0.02605737 1.035128
29          1 3.40 0.34021444 0.09209307  1.4684544 0.03778484 1.024676
30          1 3.82 0.83595184 0.07209778  0.5986389 0.03790914 1.042063
31          1 3.62 0.63124876 0.07900903  0.9592239 0.02681777 1.036762
32          0 3.09 0.08681729 0.06405967 -0.4261908 0.05177734 1.043731
33          1 3.89 0.88186412 0.06421649  0.5014325 0.03959016 1.043084
34          0 3.70 0.72589836 0.07852260 -1.6088855 0.03098985 1.020631
35          0 3.24 0.17726253 0.08733066 -0.6246889 0.05230319 1.041700
36          1 3.86 0.86372477 0.06787966  0.5412968 0.03915198 1.042686
37          0 3.54 0.52528952 0.08141251 -1.2206967 0.02657995 1.031319
38          0 3.40 0.34021444 0.09209307 -0.9119654 0.03778484 1.037514
39          1 3.87 0.87001815 0.06669907  0.5277143 0.03934584 1.042825
40          1 3.14 0.11101432 0.07267756  2.0967098 0.05353532 1.001865
41          0 3.37 0.30449919 0.09334378 -0.8522008 0.04114474 1.038505
42          0 3.38 0.31617156 0.09302470 -0.8718351 0.04002682 1.038186
43          1 3.62 0.63124876 0.07900903  0.9592239 0.02681777 1.036762
44          1 3.94 0.90745180 0.05750699  0.4407149 0.03938638 1.043633
45          0 3.37 0.30449919 0.09334378 -0.8522008 0.04114474 1.038505
46          0 3.36 0.29307306 0.09355675 -0.8328601 0.04225033 1.038812
47          1 3.97 0.92030775 0.05330346  0.4075467 0.03874979 1.043904
48          0 3.04 0.06749390 0.05544078 -0.3738438 0.04885368 1.044142
49          0 3.29 0.22057873 0.09184716 -0.7059796 0.04907377 1.040663
50          0 3.67 0.69217046 0.07880825 -1.5350629 0.02914950 1.022928
51          0 2.72 0.01247874 0.01680041 -0.1584756 0.02291923 1.045258
52          0 3.56 0.55238896 0.08040711 -1.2679358 0.02614823 1.030200
53          1 3.48 0.44373791 0.08588931  1.2747715 0.02988657 1.029973
54          0 2.80 0.01917403 0.02333899 -0.1967752 0.02898058 1.045127
55          0 3.44 0.39074729 0.08930107 -0.9955121 0.03349887 1.036021
        
        .cooksd .std.resid log_odds_hat
1  0.0242365846 -1.4318691    0.5375777
2  0.0036765026  0.5806842    1.7374942
3  0.1440562490  1.9360035   -1.5895470
4  0.0064372800  0.8319850    0.9193694
5  0.0469707264  1.5488588   -0.7714221
6  0.0058370438  0.7763239    1.0829943
7  0.0028749094  0.5116632    2.0102025
8  0.0090360294 -0.8133344   -0.9895887
9  0.0060315780  0.7945762    1.0284527
10 0.0028749094  0.5116632    2.0102025
11 0.0018157223  0.4156800    2.4465358
12 0.0183488245  1.2696750   -0.1714638
13 0.0049073273  0.6895797    1.3557026
14 0.0226808723 -1.4069829    0.4830361
15 0.0079576557 -0.7412772   -1.2077553
16 0.0126751640  1.1501176    0.1012445
17 0.0299588531 -1.5072344    0.7012027
18 0.0126751640  1.1501176    0.1012445
19 0.0065534768 -0.6575766   -1.4804636
20 0.0028749094  0.5116632    2.0102025
21 0.0060315780  0.7945762    1.0284527
22 0.0738572908 -1.8128726    1.3557026
23 0.0025816758  0.4860375    2.1192859
24 0.0030279106  0.5248963    1.9556609
25 0.0375075257 -1.5833863    0.8648277
26 0.0111707534  1.1040277    0.2103278
27 0.0114998950 -1.0126173   -0.4441721
28 0.0099796507  1.0590113    0.3194111
29 0.0395724757  1.4970088   -0.6623388
30 0.0040185732  0.6103190    1.6284109
31 0.0082706058  0.9723507    0.5375777
32 0.0027373959 -0.4376721   -2.3531302
33 0.0028749094  0.5116632    2.0102025
34 0.0437015712 -1.6344099    0.9739110
35 0.0062735742 -0.6416956   -1.5350053
36 0.0033454601  0.5522149    1.8465776
37 0.0155200609 -1.2372504    0.1012445
38 0.0105218738 -0.9296987   -0.6623388
39 0.0031848510  0.5384128    1.9011192
40 0.2392860474  2.1551928   -2.0804219
41 0.0097964059 -0.8702928   -0.8259637
42 0.0100410373 -0.8898254   -0.7714221
43 0.0082706058  0.9723507    0.5375777
44 0.0021765216  0.4496591    2.2829108
45 0.0097964059 -0.8702928   -0.8259637
46 0.0095476717 -0.8510323   -0.8805054
47 0.0018157223  0.4156800    2.4465358
48 0.0019542740 -0.3833244   -2.6258385
49 0.0076792212 -0.7239669   -1.2622970
50 0.0347695600 -1.5579374    0.8102860
51 0.0001516818 -0.1603235   -4.3711716
52 0.0170126068 -1.2848453    0.2103278
53 0.0199046219  1.2942587   -0.2260055
54 0.0003004296 -0.1996900   -3.9348383
55 0.0114998950 -1.0126173   -0.4441721



