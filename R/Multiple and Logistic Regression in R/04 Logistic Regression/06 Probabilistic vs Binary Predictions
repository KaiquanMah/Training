Making probabilistic predictions
Just as we did with linear regression, we can use our logistic regression model to make predictions about new observations. In this exercise, we will use the newdata argument to the augment() function from the broom package to make predictions about students who were not in our original data set. These predictions are sometimes called out-of-sample.
Following our previous discussion about scales, with logistic regression it is important that we specify on which scale we want the predicted values. Although the default is terms -- which uses the log-odds scale -- we want our predictions on the probability scale, which is the scale of the response variable. The type.predict argument to augment() controls this behavior.
A logistic regression model object, mod, has been defined for you.

Create a new data frame which has one variable called GPA and one row, with the value 3.51.
Use augment() to find the expected probability of admission to medical school for a student with a GPA of 3.51.

# create new data frame
new_data <- data.frame(GPA=3.51)
> new_data
   GPA
1 3.51


# make predictions
augment(mod, newdata=new_data, type.predict="response")
   GPA   .fitted    .se.fit
1 3.51 0.4844099 0.08343193

By framing your prediction as a probability you can show how likely it is that this student will get admitted to medical school.









Making binary predictions
Naturally, we want to know how well our model works. Did it predict acceptance for the students who were actually accepted to medical school? Did it predict rejections for the student who were not admitted? These types of predictions are called in-sample. One common way to evaluate models with a binary response is with a confusion matrix. [Yes, that is actually what it is called!]
However, note that while our response variable is binary, our fitted values are probabilities. Thus, we have to round them somehow into binary predictions. While the probabilities convey more information, we might ultimately have to make a decision, and so this rounding is common in practice. There are many different ways to round, but for simplicity we will predict admission if the fitted probability is greater than 0.5, and rejection otherwise.
First, we'll use augment() to make the predictions, and then mutate() and round() to convert these probabilities into binary decisions. Then we will form the confusion matrix using the table() function. table() will compute a 2-way table when given a data frame with two categorical variables, so we will first use select() to grab only those variables.
You will find that this model made only 15 mistakes on these 55 observations, so it is nearly 73% accurate.
https://en.wikipedia.org/wiki/Confusion_matrix

The model object mod is already in your worskpace.
Create a data frame with the actual observations, and their fitted probabilities, and add a new column, Acceptance_hat, with the binary decision by rounding the fitted probabilities.
Compute the confusion matrix between the actual and predicted acceptance.


# data frame with binary predictions
tidy_mod <- augment(mod, type.predict = "response") %>%
  mutate(Acceptance_hat = round(.fitted))
  
# confusion matrix
tidy_mod %>%
  select(Acceptance, Acceptance_hat) %>% 
  table()




> augment(mod, type.predict = "response")
   Acceptance  GPA    .fitted    .se.fit     .resid       .hat   .sigma
1           0 3.62 0.63124876 0.07900903 -1.4125389 0.02681777 1.026465
2           1 3.84 0.85036851 0.07009771  0.5693601 0.03862222 1.042388
3           1 3.23 0.16944765 0.08614814  1.8842565 0.05274316 1.010435
4           1 3.69 0.71491359 0.07865471  0.8192601 0.03035531 1.039108
5           1 3.38 0.31617156 0.09302470  1.5175443 0.04002682 1.023193
6           1 3.72 0.74706022 0.07810909  0.7636877 0.03228887 1.039936
7           1 3.89 0.88186412 0.06421649  0.5014325 0.03959016 1.043084
8           0 3.34 0.27099332 0.09364731 -0.7950753 0.04439531 1.039392
9           1 3.71 0.73661581 0.07834295  0.7819064 0.03163655 1.039671
10          1 3.89 0.88186412 0.06421649  0.5014325 0.03959016 1.043084
11          1 3.97 0.92030775 0.05330346  0.4075467 0.03874979 1.043904
12          1 3.49 0.45723876 0.08503889  1.2510392 0.02913975 1.030561
13          1 3.77 0.79506038 0.07597224  0.6772551 0.03542586 1.041112
14          0 3.61 0.61846455 0.07910537 -1.3882014 0.02651944 1.027126
15          0 3.30 0.23009846 0.09243699 -0.7231772 0.04823841 1.040428
16          1 3.54 0.52528952 0.08141251  1.1347297 0.02657995 1.033257
17          0 3.65 0.66845438 0.07887704 -1.4859272 0.02807332 1.024388
18          1 3.54 0.52528952 0.08141251  1.1347297 0.02657995 1.033257
19          0 3.25 0.18535740 0.08842742 -0.6403215 0.05179238 1.041510
20          1 3.89 0.88186412 0.06421649  0.5014325 0.03959016 1.043084
21          1 3.71 0.73661581 0.07834295  0.7819064 0.03163655 1.039671
22          0 3.77 0.79506038 0.07597224 -1.7804718 0.03542586 1.014818
23          1 3.91 0.89276358 0.06160154  0.4763055 0.03964509 1.043319
24          1 3.88 0.87606259 0.06547654  0.5144273 0.03949188 1.042957
25          0 3.68 0.70366831 0.07874700 -1.5596640 0.02973960 1.022176
26          1 3.56 0.55238896 0.08040711  1.0894979 0.02614823 1.034223
27          0 3.44 0.39074729 0.08930107 -0.9955121 0.03349887 1.036021
28          1 3.58 0.57918073 0.07969300  1.0451227 0.02605737 1.035128
29          1 3.40 0.34021444 0.09209307  1.4684544 0.03778484 1.024676
30          1 3.82 0.83595184 0.07209778  0.5986389 0.03790914 1.042063
31          1 3.62 0.63124876 0.07900903  0.9592239 0.02681777 1.036762
32          0 3.09 0.08681729 0.06405967 -0.4261908 0.05177734 1.043731
33          1 3.89 0.88186412 0.06421649  0.5014325 0.03959016 1.043084
34          0 3.70 0.72589836 0.07852260 -1.6088855 0.03098985 1.020631
35          0 3.24 0.17726253 0.08733066 -0.6246889 0.05230319 1.041700
36          1 3.86 0.86372477 0.06787966  0.5412968 0.03915198 1.042686
37          0 3.54 0.52528952 0.08141251 -1.2206967 0.02657995 1.031319
38          0 3.40 0.34021444 0.09209307 -0.9119654 0.03778484 1.037514
39          1 3.87 0.87001815 0.06669907  0.5277143 0.03934584 1.042825
40          1 3.14 0.11101432 0.07267756  2.0967098 0.05353532 1.001865
41          0 3.37 0.30449919 0.09334378 -0.8522008 0.04114474 1.038505
42          0 3.38 0.31617156 0.09302470 -0.8718351 0.04002682 1.038186
43          1 3.62 0.63124876 0.07900903  0.9592239 0.02681777 1.036762
44          1 3.94 0.90745180 0.05750699  0.4407149 0.03938638 1.043633
45          0 3.37 0.30449919 0.09334378 -0.8522008 0.04114474 1.038505
46          0 3.36 0.29307306 0.09355675 -0.8328601 0.04225033 1.038812
47          1 3.97 0.92030775 0.05330346  0.4075467 0.03874979 1.043904
48          0 3.04 0.06749390 0.05544078 -0.3738438 0.04885368 1.044142
49          0 3.29 0.22057873 0.09184716 -0.7059796 0.04907377 1.040663
50          0 3.67 0.69217046 0.07880825 -1.5350629 0.02914950 1.022928
51          0 2.72 0.01247874 0.01680041 -0.1584756 0.02291923 1.045258
52          0 3.56 0.55238896 0.08040711 -1.2679358 0.02614823 1.030200
53          1 3.48 0.44373791 0.08588931  1.2747715 0.02988657 1.029973
54          0 2.80 0.01917403 0.02333899 -0.1967752 0.02898058 1.045127
55          0 3.44 0.39074729 0.08930107 -0.9955121 0.03349887 1.036021
        .cooksd .std.resid
1  0.0242365846 -1.4318691
2  0.0036765026  0.5806842
3  0.1440562490  1.9360035
4  0.0064372800  0.8319850
5  0.0469707264  1.5488588
6  0.0058370438  0.7763239
7  0.0028749094  0.5116632
8  0.0090360294 -0.8133344
9  0.0060315780  0.7945762
10 0.0028749094  0.5116632
11 0.0018157223  0.4156800
12 0.0183488245  1.2696750
13 0.0049073273  0.6895797
14 0.0226808723 -1.4069829
15 0.0079576557 -0.7412772
16 0.0126751640  1.1501176
17 0.0299588531 -1.5072344
18 0.0126751640  1.1501176
19 0.0065534768 -0.6575766
20 0.0028749094  0.5116632
21 0.0060315780  0.7945762
22 0.0738572908 -1.8128726
23 0.0025816758  0.4860375
24 0.0030279106  0.5248963
25 0.0375075257 -1.5833863
26 0.0111707534  1.1040277
27 0.0114998950 -1.0126173
28 0.0099796507  1.0590113
29 0.0395724757  1.4970088
30 0.0040185732  0.6103190
31 0.0082706058  0.9723507
32 0.0027373959 -0.4376721
33 0.0028749094  0.5116632
34 0.0437015712 -1.6344099
35 0.0062735742 -0.6416956
36 0.0033454601  0.5522149
37 0.0155200609 -1.2372504
38 0.0105218738 -0.9296987
39 0.0031848510  0.5384128
40 0.2392860474  2.1551928
41 0.0097964059 -0.8702928
42 0.0100410373 -0.8898254
43 0.0082706058  0.9723507
44 0.0021765216  0.4496591
45 0.0097964059 -0.8702928
46 0.0095476717 -0.8510323
47 0.0018157223  0.4156800
48 0.0019542740 -0.3833244
49 0.0076792212 -0.7239669
50 0.0347695600 -1.5579374
51 0.0001516818 -0.1603235
52 0.0170126068 -1.2848453
53 0.0199046219  1.2942587
54 0.0003004296 -0.1996900
55 0.0114998950 -1.0126173


          Acceptance_hat
Acceptance  0  1
         0 16  9
         1  6 24
         
         
         
