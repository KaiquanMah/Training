Spark, Hadoop and Hive
You've encountered quite a few open source projects in the previous video. There's Hadoop, Hive, and PySpark. It's easy to get confused between these projects.
They have a few things in common: they are all currently maintained by the Apache Software Foundation, and they've all been used for massive parallel processing. Can you spot the differences?

Classify the cards to the corresponding software project.
PySpark 
python interface for the spark framework
uses dataframe abstraction

Hadoop
HDFS is part of it
MapReduce is part of it
Collection of open source packages for big data

Hive
initially used Hadoop MapReduce
use structured queries for parallel processing

