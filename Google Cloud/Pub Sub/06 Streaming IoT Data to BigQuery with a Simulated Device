https://googlecoursera.qwiklabs.com/focuses/11857020?parent=lti_session
create a Pub/Sub topic to ingest data streaming from simulated data. 
create a BigQuery table and export the streaming data to it. 
create a query to query and modify the data.






Enable APIs
Cloud IoT API
Cloud Pub/Sub API
Container Registry API






+++++++additional step to add/check permissions
project id/name         qwiklabs-gcp-01-efe3758c49a4
project number          1017786332145


Check project permissions
Navigation menu click IAM & Admin > IAM.
Confirm that the default compute Service Account {project-number}-compute@developer.gserviceaccount.com is present and has the editor role assigned. 
90388388750-compute@developer.gserviceaccount.com
The account prefix is the project number, which you can find on Navigation menu > Home.

If the account is not present in IAM or does not have the editor role, follow the steps below to assign the required role.
In the Google Cloud console, on the Navigation menu, click Home.
Copy the project number (e.g. 729328892908).
On the Navigation menu, click IAM & Admin > IAM.
At the top of the IAM page, click Add.
For New members, type:
{project-number}-compute@developer.gserviceaccount.com                  x
Replace {project-number} with your project number.
For Role, select Project > Editor. Click Save.

	
Type    Member                                                    Name                                      Role    Analyzed permissions (excess/total)         Inheritance
        1017786332145-compute@developer.gserviceaccount.com	      Compute Engine default service account	  Editor  3072/3072                                   No inheritance, for role Editor
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++















Installing and using virtualenv with Python3
Execute the following command to download and update the packages list.
sudo apt-get update

********************************************************************************
You are running apt-get inside of Cloud Shell. Note that your Cloud Shell
machine is ephemeral and no system-wide change will persist beyond session end.
To suppress this warning, create an empty ~/.cloudshell/no-apt-get-warning file.
The command will automatically proceed in 5 seconds or on any key.
Visit https://cloud.google.com/shell/help for more information.
********************************************************************************
Err:1 https://packages.cloud.google.com/apt cloud-sdk-buster InRelease
  Temporary failure resolving 'packages.cloud.google.com'
Hit:2 http://deb.debian.org/debian buster InRelease                                                                                                                                                                  
Get:3 http://deb.debian.org/debian buster-updates InRelease [51.9 kB]                                                                                                                                                
Hit:4 https://packages.microsoft.com/repos/microsoft-debian-buster-prod buster InRelease                                                                                                                                                           
Hit:5 http://storage.googleapis.com/bazel-apt stable InRelease                                                                                                                                                                       
Get:7 https://packages.sury.org/php buster InRelease [6,759 B]                                                                                                                        
Get:8 https://packages.sury.org/php buster/main amd64 Packages [144 kB]                                                                                      
Hit:6 https://apt.llvm.org/buster llvm-toolchain-buster-9 InRelease                                                                                                                        
Hit:9 http://repo.mysql.com/apt/debian buster InRelease                                                        
Get:10 http://security.debian.org/debian-security buster/updates InRelease [65.4 kB]                                                                          
Get:11 https://download.docker.com/linux/debian buster InRelease [44.4 kB]                                                                            
Get:12 http://packages.cloud.google.com/apt gcsfuse-buster InRelease [3,724 B]                               
Get:13 http://security.debian.org/debian-security buster/updates/main Sources [140 kB]
Get:14 http://security.debian.org/debian-security buster/updates/main amd64 Packages [235 kB]
Ign:15 http://ftp.debian.org/debian jessie InRelease                            
Ign:16 http://ftp.debian.org/debian stretch InRelease
Get:17 http://ftp.debian.org/debian stretch-backports InRelease [91.8 kB]
Get:18 http://ftp.debian.org/debian stretch-updates InRelease [93.6 kB]                                                                                                                                                                            
Hit:19 http://ftp.debian.org/debian jessie Release                                                                                                                                                                                                 
Hit:21 http://ftp.debian.org/debian stretch Release                                                                                                                                                                                                
Get:22 http://ftp.debian.org/debian stretch-backports/main Sources.diff/Index [27.8 kB]                                                                                                                                                            
Get:23 http://ftp.debian.org/debian stretch-backports/main amd64 Packages.diff/Index [27.8 kB]                                                                                                                                                     
Get:25 http://ftp.debian.org/debian stretch-backports/main Sources 2020-10-16-1418.20.pdiff [60 B]                                                                                                                                                 
Get:25 http://ftp.debian.org/debian stretch-backports/main Sources 2020-10-16-1418.20.pdiff [60 B]                                                                                                                                                 
Get:26 http://ftp.debian.org/debian stretch-backports/main amd64 Packages 2020-10-16-1418.20.pdiff [78 B]    
Get:26 http://ftp.debian.org/debian stretch-backports/main amd64 Packages 2020-10-16-1418.20.pdiff [78 B]                                                                                                                                          
Fetched 933 kB in 9s (105 kB/s)                                                                                                                                                                                                                    
Reading package lists... Done
W: Failed to fetch https://packages.cloud.google.com/apt/dists/cloud-sdk-buster/InRelease  Temporary failure resolving 'packages.cloud.google.com'
W: Some index files failed to download. They have been ignored, or old ones used instead.




Python virtual environments are used to isolate package installation from the system.
sudo apt-get install virtualenv

Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following additional packages will be installed:
  python3-virtualenv
The following NEW packages will be installed:
  python3-virtualenv virtualenv
0 upgraded, 2 newly installed, 0 to remove and 18 not upgraded.
Need to get 78.2 kB of archives.
After this operation, 171 kB of additional disk space will be used.
Do you want to continue? [Y/n] y
Get:1 http://deb.debian.org/debian buster/main amd64 python3-virtualenv all 15.1.0+ds-2 [58.1 kB]
Get:2 http://deb.debian.org/debian buster/main amd64 virtualenv all 15.1.0+ds-2 [20.1 kB]
Fetched 78.2 kB in 0s (4,913 kB/s)    
debconf: delaying package configuration, since apt-utils is not installed
Selecting previously unselected package python3-virtualenv.
(Reading database ... 130271 files and directories currently installed.)
Preparing to unpack .../python3-virtualenv_15.1.0+ds-2_all.deb ...
Unpacking python3-virtualenv (15.1.0+ds-2) ...
Selecting previously unselected package virtualenv.
Preparing to unpack .../virtualenv_15.1.0+ds-2_all.deb ...
Unpacking virtualenv (15.1.0+ds-2) ...
Setting up python3-virtualenv (15.1.0+ds-2) ...
Setting up virtualenv (15.1.0+ds-2) ...
Processing triggers for man-db (2.8.5-2) ...




If prompted [Y/n], press Y and then Enter.
virtualenv -p python3 venv

Activate the virtual environment.
source venv/bin/activate

-bash: venv/bin/activate: No such file or directory















Task 1. Create a topic and subscription
In this section, you create a Pub/Sub topic.
On the Navigation menu, click Pub/Sub > Topics.
Click Create Topic.
For Topic ID, type device-events, and then click Create Topic.





Task 2. Create a storage bucket
In this section, you create a Google Cloud Storage bucket.
In the GCP Console, on the Navigation menu, click Storage > Browser.
Click Create bucket.
Use your project name as the bucket name.
Click Create




Task 3. Start publishing simulated sensor data
In this section, you start a simulator for the sensors. 
The simulator is a simple Python script that publishes sensor data every 30 seconds. 
The data is actual data from an Android Things NXP board and Rainbow HAT.

The Python script imports a pubsub library from google.cloud. 
For the script to work properly, the following four commands must be executed in Cloud Shell.
Copy and paste into Cloud Shell. Execute the following commands.

pip uninstall google-cloud-pubsub
pip uninstall google-cloud
pip install google-cloud
********************************************************************************
Python 2 is deprecated. Upgrade to pip3 as soon as possible.
See https://cloud.google.com/python/docs/python2-sunset

To suppress this warning, create an empty ~/.cloudshell/no-pip-warning file.
The command will automatically proceed in 5 seconds or on any key.
********************************************************************************
Collecting google-cloud
  Using cached https://files.pythonhosted.org/packages/ba/b1/7c54d1950e7808df06642274e677dbcedba57f75307adf2e5ad8d39e5e0e/google_cloud-0.34.0-py2.py3-none-any.whl
Installing collected packages: google-cloud
Successfully installed google-cloud-0.34.0



pip install google-cloud-pubsub
********************************************************************************
Python 2 is deprecated. Upgrade to pip3 as soon as possible.
See https://cloud.google.com/python/docs/python2-sunset

To suppress this warning, create an empty ~/.cloudshell/no-pip-warning file.
The command will automatically proceed in 5 seconds or on any key.
********************************************************************************
Collecting google-cloud-pubsub
  Using cached https://files.pythonhosted.org/packages/1f/b3/dd83eca4cd1019d592e82595ea45d53f11e39db4ee99daa66ceb8a1b2d89/google_cloud_pubsub-1.7.0-py2.py3-none-any.whl
Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /usr/local/lib/python2.7/dist-packages (from google-cloud-pubsub) (0.12.3)
Requirement already satisfied: enum34; python_version < "3.4" in /usr/local/lib/python2.7/dist-packages (from google-cloud-pubsub) (1.1.10)
Requirement already satisfied: google-api-core[grpc]!=1.17.*,!=1.18.*,!=1.19.*,>=1.14.0 in /usr/local/lib/python2.7/dist-packages (from google-cloud-pubsub) (1.22.4)
Requirement already satisfied: grpcio<2.0.0dev,>=1.0.0 in /usr/local/lib/python2.7/dist-packages (from grpc-google-iam-v1<0.13dev,>=0.12.3->google-cloud-pubsub) (1.32.0)
Requirement already satisfied: googleapis-common-protos[grpc]<2.0.0dev,>=1.5.2 in /usr/local/lib/python2.7/dist-packages (from grpc-google-iam-v1<0.13dev,>=0.12.3->google-cloud-pubsub) (1.52.0)
Requirement already satisfied: setuptools>=34.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-core[grpc]!=1.17.*,!=1.18.*,!=1.19.*,>=1.14.0->google-cloud-pubsub) (44.1.1)
Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python2.7/dist-packages (from google-api-core[grpc]!=1.17.*,!=1.18.*,!=1.19.*,>=1.14.0->google-cloud-pubsub) (3.13.0)
Requirement already satisfied: futures>=3.2.0; python_version < "3.2" in /usr/local/lib/python2.7/dist-packages (from google-api-core[grpc]!=1.17.*,!=1.18.*,!=1.19.*,>=1.14.0->google-cloud-pubsub) (3.3.0)
Requirement already satisfied: pytz in /usr/local/lib/python2.7/dist-packages (from google-api-core[grpc]!=1.17.*,!=1.18.*,!=1.19.*,>=1.14.0->google-cloud-pubsub) (2020.1)
Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python2.7/dist-packages (from google-api-core[grpc]!=1.17.*,!=1.18.*,!=1.19.*,>=1.14.0->google-cloud-pubsub) (1.15.0)
Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from google-api-core[grpc]!=1.17.*,!=1.18.*,!=1.19.*,>=1.14.0->google-cloud-pubsub) (1.22.1)
Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python2.7/dist-packages (from google-api-core[grpc]!=1.17.*,!=1.18.*,!=1.19.*,>=1.14.0->google-cloud-pubsub) (2.24.0)
Requirement already satisfied: rsa<4.6; python_version < "3.5" in /usr/local/lib/python2.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]!=1.17.*,!=1.18.*,!=1.19.*,>=1.14.0->google-cloud-pubsub) (4.5)
Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]!=1.17.*,!=1.18.*,!=1.19.*,>=1.14.0->google-cloud-pubsub) (0.2.8)
Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]!=1.17.*,!=1.18.*,!=1.19.*,>=1.14.0->google-cloud-pubsub) (3.1.1)
Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=1.17.*,!=1.18.*,!=1.19.*,>=1.14.0->google-cloud-pubsub) (2.10)
Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=1.17.*,!=1.18.*,!=1.19.*,>=1.14.0->google-cloud-pubsub) (1.25.10)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=1.17.*,!=1.18.*,!=1.19.*,>=1.14.0->google-cloud-pubsub) (2020.6.20)
Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=1.17.*,!=1.18.*,!=1.19.*,>=1.14.0->google-cloud-pubsub) (3.0.4)
Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa<4.6; python_version < "3.5"->google-auth<2.0dev,>=1.21.1->google-api-core[grpc]!=1.17.*,!=1.18.*,!=1.19.*,>=1.14.0->google-cloud-pubsub) (0.4.8)
Installing collected packages: google-cloud-pubsub
Successfully installed google-cloud-pubsub-1.7.0




Get the Python script from GitHub.
Copy and paste into Cloud Shell. 
Execute the following commands
git clone https://github.com/GoogleCloudPlatform/training-data-analyst.git
cd training-data-analyst/iot/sensor-sim

Cloning into 'training-data-analyst'...
remote: Enumerating objects: 50, done.
remote: Counting objects: 100% (50/50), done.
remote: Compressing objects: 100% (39/39), done.
remote: Total 42388 (delta 16), reused 34 (delta 11), pack-reused 42338
Receiving objects: 100% (42388/42388), 465.04 MiB | 16.36 MiB/s, done.
Resolving deltas: 100% (26525/26525), done.
Checking out files: 100% (9064/9064), done.
student_01_4212f8c936a5@cloudshell:~ (qwiklabs-gcp-01-7c4f4ddf7056)$ cd training-data-analyst/iot/sensor-sim
student_01_4212f8c936a5@cloudshell:~/training-data-analyst/iot/sensor-sim (qwiklabs-gcp-01-7c4f4ddf7056)$




To start the Python script, execute the following command, and replace the project name placeholder, including the brackets, with the name of your project:
python sendData.py -p <project_name> -t device-events           python sendData.py -p qwiklabs-gcp-01-7c4f4ddf7056 -t device-events

The script immediately starts publishing data to the topic and printing it in Cloud Shell. It should look similar to what is shown below.
Let the script run in this Cloud Shell.
{"data":[{"timestamp_ambient_pressure":1528752334018,"ambient_pressure":1012.0814819335938},{"timestamp_temperature":1528752334018,"temperature":35.23337936401367}]}
...
{"data":[{"timestamp_ambient_pressure":1528752313484,"ambient_pressure":1012.103515625},{"timestamp_temperature":1528752313484,"temperature":35.18467712402344}]}




















Task 4. Streaming data to BigQuery
In this section, you create a BigQuery table and stream data directly to it. 
Then you perform a query on the sensor data.

Create a BigQuery table
The first step in streaming data to BigQuery is to create a BigQuery table.
On the Navigation menu, click BigQuery then click Done.
Click on your project name, then click Create Dataset.
For Dataset ID type sensorHubData. 
Select the data location as Default, click Create dataset.

To add a table, click on your dataset then click Create table.
Create table from:	Empty table
Dataset name	      sensorHubData
Table type	        Native table
Table name	        sensorData
Under Schema, click Edit as Text.

Remove any text in the text box.
Copy the BigQuery table schema text shown below and paste it into the text box.

The schema of the data from the device is an array of records.
[
    {
        "name": "data",
        "type": "RECORD",
        "mode": "REPEATED",
        "fields": [
            {
                "name": "timestamp_ambient_pressure",
                "type": "STRING"
            },
            {
                "name": "ambient_pressure",
                "type": "FLOAT"
            },
            {
                "name": "timestamp_temperature",
                "type": "STRING"
            },
            {
                "name": "temperature",
                "type": "FLOAT"
            }
        ]
    }
]




Dataset info 
Dataset ID	              qwiklabs-gcp-01-7c4f4ddf7056:sensorHubData                    qwiklabs-gcp-01-efe3758c49a4:sensorHubData.sensorData
Created	                  Oct 18, 2020, 4:22:55 PM
Default table expiration	Never
Last modified	            Oct 18, 2020, 4:22:55 PM
Data location	            US

Schema
Field name	                      Type	      Mode	      Policy tags 	Description
data	                            RECORD	    REPEATED	
data. timestamp_ambient_pressure	STRING	    NULLABLE	
data. ambient_pressure	          FLOAT	      NULLABLE	
data. timestamp_temperature	      STRING	    NULLABLE	
data. temperature	                FLOAT	      NULLABLE	

Policy tags are used to control access to individual columns.




Click the Details tab, and select and copy the Table ID text. You need this information at a later step in the lab.
Table info 
Table ID	            qwiklabs-gcp-01-7c4f4ddf7056:sensorHubData.sensorData
Table size	          0 B
Number of rows	      0
Created	              Oct 18, 2020, 4:25:15 PM
Table expiration	    Never
Last modified	        Oct 18, 2020, 4:25:15 PM
Data location	        US










Export data to BigQuery
At this point you have:
Authorized Google Cloud Platform to read the data.
Created a BigQuery table to accept the published data from the device.

Now you are ready for the final step, exporting the data to BigQuery.
On the Navigation menu, click Pub/Sub.
Click on the name of the Topic device-events.
Go to bottom of the page, click Create subscription > Export to BigQuery.

On the Create job from template page, specify the following, and leave the remaining settings as their defaults:
Property	              Value (type or select)
Job name	              Enter the job name                                              device-events             ps-to-bq-device-events
Cloud Dataflow template	Cloud Pub/Sub Topic to BigQuery
Cloud Pub/Sub           input topic	projects/<project>/topics/<topic>                   projects/qwiklabs-gcp-01-efe3758c49a4/topics/device-events
BigQuery output table	  <project_name>:sensorHubData.sensorData (paste the BigQuery table name that you copied)         qwiklabs-gcp-01-efe3758c49a4:sensorHubData.sensorData
Temporary Location	    gs://<bucket_name>/tmp                                          gs://qwiklabs-gcp-01-efe3758c49a4/tmp

Click Run job.
The data from the device, which is published on Pub/Sub, will be stored in a BigQuery table.
A Cloud Dataflow graph and job status page will open. 
To check that the job is running correctly, click on the Job Logs menu item. 
The job should run without errors.



Errors on 2020.10.18:
Dataflow>device-events>logs
2020-10-18T08:34:36.711092860ZWorkflow failed. Causes: Permissions verification for controller service account failed. 
IAM role roles/dataflow.worker should be granted to controller service account 383741794385-compute@developer.gserviceaccount.com.

IAM and admin>permissions>add
You need permissions for this action.
Required permission(s): resourcemanager.projects.setIamPolicy







Job logs on 2020.10.19:
2020-10-19T04:36:24.700568267ZAutoscaling: Enabled for job 2020-10-18_21_36_20-3426351493754657670 between 1 and 3 worker processes.
2020-10-19T04:36:25.223436444ZWorker configuration: n1-standard-4 in us-central1-b.
2020-10-19T04:36:27.676171275ZGenerating 3 persistent disks which will allow autoscaling from 1 to 3 workers.
2020-10-19T04:36:28.147125529ZStarting 1 workers...
2020-10-19T04:36:31.115537361ZExecuting operation WriteFailedRecords/WriteFailedRecordsToBigQuery/StreamingInserts/StreamingWriteTables/Reshuffle/GroupByKey/ReadStream+WriteFailedRecords/WriteFailedRecordsToBigQuery/StreamingInserts/StreamingWriteTables/Reshuffle/GroupByKey/MergeBuckets+WriteFailedRecords/WriteFailedRecordsToBigQuery/StreamingInserts/StreamingWriteTables/Reshuffle/ExpandIterable+WriteFailedRecords/WriteFailedRecordsToBigQuery/StreamingInserts/StreamingWriteTables/GlobalWindow/Window.Assign+WriteFailedRecords/WriteFailedRecordsToBigQuery/StreamingInserts/StreamingWriteTables/StreamingWrite
2020-10-19T04:36:31.181936450ZExecuting operation WriteFailedRecords2/WriteFailedRecordsToBigQuery/StreamingInserts/StreamingWriteTables/Reshuffle/GroupByKey/ReadStream+WriteFailedRecords2/WriteFailedRecordsToBigQuery/StreamingInserts/StreamingWriteTables/Reshuffle/GroupByKey/MergeBuckets+WriteFailedRecords2/WriteFailedRecordsToBigQuery/StreamingInserts/StreamingWriteTables/Reshuffle/ExpandIterable+WriteFailedRecords2/WriteFailedRecordsToBigQuery/StreamingInserts/StreamingWriteTables/GlobalWindow/Window.Assign+WriteFailedRecords2/WriteFailedRecordsToBigQuery/StreamingInserts/StreamingWriteTables/StreamingWrite
2020-10-19T04:36:31.216227407ZExecuting operation ReadPubSubTopic/PubsubUnboundedSource+ReadPubSubTopic/MapElements/Map+ConvertMessageToTableRow/MapToRecord+ConvertMessageToTableRow/InvokeUDF/ProcessUdf+WriteFailedRecords/FailedRecordToTableRow+WriteFailedRecords/WriteFailedRecordsToBigQuery/PrepareWrite/ParDo(Anonymous)+WriteFailedRecords/WriteFailedRecordsToBigQuery/StreamingInserts/CreateTables/ParDo(CreateTables)+WriteFailedRecords/WriteFailedRecordsToBigQuery/StreamingInserts/StreamingWriteTables/ShardTableWrites+WriteFailedRecords/WriteFailedRecordsToBigQuery/StreamingInserts/StreamingWriteTables/TagWithUniqueIds+WriteFailedRecords/WriteFailedRecordsToBigQuery/StreamingInserts/StreamingWriteTables/Reshuffle/Window.Into()/Window.Assign+WriteFailedRecords/WriteFailedRecordsToBigQuery/StreamingInserts/StreamingWriteTables/Reshuffle/GroupByKey/WriteStream+ConvertMessageToTableRow/JsonToTableRow/JsonToTableRow+WriteFailedRecords/FailedRecordToTableRow+WriteFailedRecords/WriteFailedRecordsToBigQuery/PrepareWrite/ParDo(Anonymous)+WriteFailedRecords/WriteFailedRecordsToBigQuery/StreamingInserts/CreateTables/ParDo(CreateTables)+WriteFailedRecords/WriteFailedRecordsToBigQuery/StreamingInserts/StreamingWriteTables/ShardTableWrites+WriteFailedRecords/WriteFailedRecordsToBigQuery/StreamingInserts/StreamingWriteTables/TagWithUniqueIds+WriteFailedRecords/WriteFailedRecordsToBigQuery/StreamingInserts/StreamingWriteTables/Reshuffle/Window.Into()/Window.Assign+WriteFailedRecords/WriteFailedRecordsToBigQuery/StreamingInserts/StreamingWriteTables/Reshuffle/GroupByKey/WriteStream+WriteSuccessfulRecords/PrepareWrite/ParDo(Anonymous)+WriteSuccessfulRecords/StreamingInserts/CreateTables/ParDo(CreateTables)+WriteSuccessfulRecords/StreamingInserts/StreamingWriteTables/ShardTableWrites+WriteSuccessfulRecords/StreamingInserts/StreamingWriteTables/TagWithUniqueIds+WriteSuccessfulRecords/StreamingInserts/StreamingWriteTables/Reshuffle/Window.Into()/Window.Assign+WriteSuccessfulRecords/StreamingInserts/StreamingWriteTables/Reshuffle/GroupByKey/WriteStream
2020-10-19T04:36:31.248815110ZExecuting operation WriteSuccessfulRecords/StreamingInserts/StreamingWriteTables/Reshuffle/GroupByKey/ReadStream+WriteSuccessfulRecords/StreamingInserts/StreamingWriteTables/Reshuffle/GroupByKey/MergeBuckets+WriteSuccessfulRecords/StreamingInserts/StreamingWriteTables/Reshuffle/ExpandIterable+WriteSuccessfulRecords/StreamingInserts/StreamingWriteTables/GlobalWindow/Window.Assign+WriteSuccessfulRecords/StreamingInserts/StreamingWriteTables/StreamingWrite+WrapInsertionErrors/Map+WriteFailedRecords2/FailedRecordToTableRow+WriteFailedRecords2/WriteFailedRecordsToBigQuery/PrepareWrite/ParDo(Anonymous)+WriteFailedRecords2/WriteFailedRecordsToBigQuery/StreamingInserts/CreateTables/ParDo(CreateTables)+WriteFailedRecords2/WriteFailedRecordsToBigQuery/StreamingInserts/StreamingWriteTables/ShardTableWrites+WriteFailedRecords2/WriteFailedRecordsToBigQuery/StreamingInserts/StreamingWriteTables/TagWithUniqueIds+WriteFailedRecords2/WriteFailedRecordsToBigQuery/StreamingInserts/StreamingWriteTables/Reshuffle/Window.Into()/Window.Assign+WriteFailedRecords2/WriteFailedRecordsToBigQuery/StreamingInserts/StreamingWriteTables/Reshuffle/GroupByKey/WriteStream
2020-10-19T04:36:57.982107706ZWorker configuration: n1-standard-4 in us-central1-b.





























Task 5. Query the data in BigQuery
The data can be queried in BigQuery immediately. 
Cloud Dataflow will continue to write data to the table as long as the device is still publishing and the Cloud Dataflow job is still running.
On the Navigation menu, click BigQuery.
In the navigation pane, click <your-project-id> > sensorHubData.
Click sensorData. The schema for the table you created will appear.
Click Query Table. A query window and a row of buttons appear.

Click on More > Query settings, and ensure you are using Standard SQL dialect. 
You are using Standard SQL if Standard radio button is selected, then click Cancel. 
If not selected, select Standard SQL and click Save.

Copy and paste this query into the query editor:
SELECT * FROM `<project name>.sensorHubData.sensorData` LIMIT 10
SELECT * FROM `qwiklabs-gcp-01-efe3758c49a4.sensorHubData.sensorData` LIMIT 10
Important tip: do not use single quotes around the table name. Use the back-quote symbol, which is different than single quotes.

Click Run.
Your results should be similar to what is shown below.



Query complete (0.3 sec elapsed, 0 B processed)
Job info tab
Job ID	qwiklabs-gcp-01-efe3758c49a4:US.bquxjob_4c66adcd_1753f29f875
User	  student-01-c194e2346b5c@qwiklabs.net
Location  United States (US)
Creation time	Oct 19, 2020, 12:42:54 PM
Start time	  Oct 19, 2020, 12:42:55 PM
End time	    Oct 19, 2020, 12:42:55 PM
Duration	    0.3 sec
Bytes processed	0 B
Bytes billed	  0 B
Job priority	  INTERACTIVE
Destination table	    Temporary table
Use legacy SQL	      false



Results tab
Row	  data.timestamp_ambient_pressure	  data.ambient_pressure	  data.timestamp_temperature	data.temperature	
1	    1528752559871                     1012.0789794921875      null                        null
      null                              null                    1528752559871               35.554813385009766
2	    1528752642002                     1012.094482421875       null                        null
      null                              null                    1528752642002               35.579166412353516
...
























Task 6. Reformat the data in BigQuery
In this section, you run a query to reformat the data and convert the temperature into Farenheit.
Copy the query shown below and change <project-id> with your project.

#query:
SELECT
  #timestamp in miliseconds
  EXTRACT(DATE FROM TIMESTAMP_MILLIS(CAST(timestamp_ambient_pressure AS INT64))) AS Pressuredate,
  TIMESTAMP_MILLIS(CAST(timestamp_ambient_pressure AS INT64)) AS Pressuretime,

  EXTRACT(DATE FROM TIMESTAMP_MILLIS(CAST(timestamp_temperature AS INT64))) AS Tempdate,
  TIMESTAMP_MILLIS(CAST(timestamp_temperature AS INT64)) AS Temptime,

  ambient_pressure as pressure,
  temperature as temp_c,
  (temperature*1.8)+32 as temp_f

FROM
  `<project-id>.sensorHubData.sensorData`,
  UNNEST(data) AS d
  WHERE timestamp_temperature IS NOT NULL OR timestamp_ambient_pressure IS NOT NULL
Paste the query into the Query window.
Click Run.
Your results should be similar to what is shown below.




SELECT
  #timestamp in miliseconds
  EXTRACT(DATE FROM TIMESTAMP_MILLIS(CAST(timestamp_ambient_pressure AS INT64))) AS Pressuredate,
  TIMESTAMP_MILLIS(CAST(timestamp_ambient_pressure AS INT64)) AS Pressuretime,

  EXTRACT(DATE FROM TIMESTAMP_MILLIS(CAST(timestamp_temperature AS INT64))) AS Tempdate,
  TIMESTAMP_MILLIS(CAST(timestamp_temperature AS INT64)) AS Temptime,

  ambient_pressure as pressure,
  temperature as temp_c,
  (temperature*1.8)+32 as temp_f

FROM
  `qwiklabs-gcp-01-efe3758c49a4.sensorHubData.sensorData`,
  UNNEST(data) AS d
  WHERE timestamp_temperature IS NOT NULL OR timestamp_ambient_pressure IS NOT NULL
  


Row	  Pressuredate	Pressuretime	                  Tempdate	  Temptime	                    pressure	              temp_c	            temp_f	
1	    2018-06-11    2018-06-11 21:29:19.871 UTC     null        null                          1012.0789794921875      null                null
2	    null          null                            2018-06-11  2018-06-11 21:29:19.871 UTC   null                    35.554813385009766  95.99866409301758
...
  


