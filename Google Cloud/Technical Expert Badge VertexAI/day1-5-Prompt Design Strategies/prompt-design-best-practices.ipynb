{
  "cells": [
    {
      "cell_type": "code",
      "id": "1aT7zeasTKWU6RbNWse6Uh22",
      "metadata": {
        "tags": [],
        "id": "1aT7zeasTKWU6RbNWse6Uh22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9b30ecf-f183-455d-e889-52123529de62"
      },
      "source": [
        "%pip install --upgrade --quiet google-cloud-aiplatform"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m236.7/236.7 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from inspect import cleandoc\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, GenerationConfig"
      ],
      "metadata": {
        "id": "yTqwqUKWHs7r"
      },
      "id": "yTqwqUKWHs7r",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"qwiklabs-gcp-02-b9f5997425cc\"\n",
        "LOCATION = \"us-central1\"\n",
        "import vertexai\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "vertexai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaJhYHUWHs-l",
        "outputId": "851a336b-5ef3-439b-f149-73d30c8c0c1b"
      },
      "id": "JaJhYHUWHs-l",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'vertexai' from '/usr/local/lib/python3.11/dist-packages/vertexai/__init__.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GenerativeModel(\"gemini-2.0-flash-001\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHFTyDRaHtBU",
        "outputId": "24896658-76c9-40bd-b1d6-92a8d14fcc10"
      },
      "id": "DHFTyDRaHtBU",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/vertexai/generative_models/_generative_models.py:433: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3. Define the output format & specify constraints\n"
      ],
      "metadata": {
        "id": "DYAUQ_IrH8E-"
      },
      "id": "DYAUQ_IrH8E-"
    },
    {
      "cell_type": "code",
      "source": [
        "transcript = \"\"\"\n",
        "    Speaker 1 (Customer): Hi, can I get a cheeseburger and large fries, please?\n",
        "    Speaker 2 (Restaurant employee): Coming right up! Anything else you'd like to add to your order?\n",
        "    Speaker 1: Hmmm, maybe a small orange juice. And could I get the fries with ketchup on the side?\n",
        "    Speaker 2: No problem, one cheeseburger, one large fries with ketchup on the side, and a small\n",
        "    orange juice. That'll be $5.87. Drive through to the next window please.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2wBKUdNqHtD8"
      },
      "id": "2wBKUdNqHtD8",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(f\"\"\"\n",
        "    Extract the transcript to JSON.\n",
        "\n",
        "    {transcript}\n",
        "\"\"\")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzYf3znZH-aT",
        "outputId": "5515b16a-57d5-45cb-b606-61ffae290649"
      },
      "id": "RzYf3znZH-aT",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "[\n",
            "  {\n",
            "    \"speaker\": \"Customer\",\n",
            "    \"utterance\": \"Hi, can I get a cheeseburger and large fries, please?\"\n",
            "  },\n",
            "  {\n",
            "    \"speaker\": \"Restaurant employee\",\n",
            "    \"utterance\": \"Coming right up! Anything else you'd like to add to your order?\"\n",
            "  },\n",
            "  {\n",
            "    \"speaker\": \"Customer\",\n",
            "    \"utterance\": \"Hmmm, maybe a small orange juice. And could I get the fries with ketchup on the side?\"\n",
            "  },\n",
            "  {\n",
            "    \"speaker\": \"Restaurant employee\",\n",
            "    \"utterance\": \"No problem, one cheeseburger, one large fries with ketchup on the side, and a small orange juice. That'll be $5.87. Drive through to the next window please.\"\n",
            "  }\n",
            "]\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bH_xDoAoH-cl",
        "outputId": "6ce89254-8cba-4f35-e276-3f318dc9c2ab"
      },
      "id": "bH_xDoAoH-cl",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "candidates {\n",
              "  content {\n",
              "    role: \"model\"\n",
              "    parts {\n",
              "      text: \"```json\\n[\\n  {\\n    \\\"speaker\\\": \\\"Customer\\\",\\n    \\\"utterance\\\": \\\"Hi, can I get a cheeseburger and large fries, please?\\\"\\n  },\\n  {\\n    \\\"speaker\\\": \\\"Restaurant employee\\\",\\n    \\\"utterance\\\": \\\"Coming right up! Anything else you\\'d like to add to your order?\\\"\\n  },\\n  {\\n    \\\"speaker\\\": \\\"Customer\\\",\\n    \\\"utterance\\\": \\\"Hmmm, maybe a small orange juice. And could I get the fries with ketchup on the side?\\\"\\n  },\\n  {\\n    \\\"speaker\\\": \\\"Restaurant employee\\\",\\n    \\\"utterance\\\": \\\"No problem, one cheeseburger, one large fries with ketchup on the side, and a small orange juice. That\\'ll be $5.87. Drive through to the next window please.\\\"\\n  }\\n]\\n```\"\n",
              "    }\n",
              "  }\n",
              "  finish_reason: STOP\n",
              "  avg_logprobs: -0.0024869804796965227\n",
              "}\n",
              "usage_metadata {\n",
              "  prompt_token_count: 132\n",
              "  candidates_token_count: 184\n",
              "  total_token_count: 316\n",
              "  prompt_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 132\n",
              "  }\n",
              "  candidates_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 184\n",
              "  }\n",
              "}\n",
              "model_version: \"gemini-2.0-flash-001\"\n",
              "create_time {\n",
              "  seconds: 1760334838\n",
              "  nanos: 757631000\n",
              "}\n",
              "response_id: \"9pPsaP-eLs6CsbQPv8_v4AU\""
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# more specific instructions on what u want from the model\n",
        "# food vs drinks\n",
        "# item, qty, size, additional notes\n",
        "response = model.generate_content(f\"\"\"\n",
        "    <INSTRUCTIONS>\n",
        "    - Extract the ordered items into JSON.\n",
        "    - Separate drinks from food.\n",
        "    - Include a quantity for each item and a size if specified.\n",
        "    </INSTRUCTIONS>\n",
        "\n",
        "    <TRANSCRIPT>\n",
        "    {transcript}\n",
        "    </TRANSCRIPT>\n",
        "\"\"\")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwvKpdTRH-fZ",
        "outputId": "41cd7e05-6192-4a25-c2da-55eb44d8ee9e"
      },
      "id": "dwvKpdTRH-fZ",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"food_items\": [\n",
            "    {\n",
            "      \"item\": \"cheeseburger\",\n",
            "      \"quantity\": 1\n",
            "    },\n",
            "    {\n",
            "      \"item\": \"fries\",\n",
            "      \"quantity\": 1,\n",
            "      \"size\": \"large\",\n",
            "      \"notes\": \"ketchup on the side\"\n",
            "    }\n",
            "  ],\n",
            "  \"drink_items\": [\n",
            "    {\n",
            "      \"item\": \"orange juice\",\n",
            "      \"quantity\": 1,\n",
            "      \"size\": \"small\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gDDAjTKH-iA",
        "outputId": "b184803e-559e-46c8-ef6c-683f6840ac44"
      },
      "id": "6gDDAjTKH-iA",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "candidates {\n",
              "  content {\n",
              "    role: \"model\"\n",
              "    parts {\n",
              "      text: \"```json\\n{\\n  \\\"food_items\\\": [\\n    {\\n      \\\"item\\\": \\\"cheeseburger\\\",\\n      \\\"quantity\\\": 1\\n    },\\n    {\\n      \\\"item\\\": \\\"fries\\\",\\n      \\\"quantity\\\": 1,\\n      \\\"size\\\": \\\"large\\\",\\n      \\\"notes\\\": \\\"ketchup on the side\\\"\\n    }\\n  ],\\n  \\\"drink_items\\\": [\\n    {\\n      \\\"item\\\": \\\"orange juice\\\",\\n      \\\"quantity\\\": 1,\\n      \\\"size\\\": \\\"small\\\"\\n    }\\n  ]\\n}\\n```\"\n",
              "    }\n",
              "  }\n",
              "  finish_reason: STOP\n",
              "  avg_logprobs: -0.02575196647644043\n",
              "}\n",
              "usage_metadata {\n",
              "  prompt_token_count: 179\n",
              "  candidates_token_count: 125\n",
              "  total_token_count: 304\n",
              "  prompt_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 179\n",
              "  }\n",
              "  candidates_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 125\n",
              "  }\n",
              "}\n",
              "model_version: \"gemini-2.0-flash-001\"\n",
              "create_time {\n",
              "  seconds: 1760334893\n",
              "  nanos: 679127000\n",
              "}\n",
              "response_id: \"LZTsaNe5Ka_rhMIPmrDtiQw\""
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4. Assign a persona or role\n"
      ],
      "metadata": {
        "id": "T82F_xJ5JDD9"
      },
      "id": "T82F_xJ5JDD9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## no role"
      ],
      "metadata": {
        "id": "or-76WslJJ7r"
      },
      "id": "or-76WslJJ7r"
    },
    {
      "cell_type": "code",
      "source": [
        "chat = model.start_chat()\n",
        "chat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCvdsAMKH-ko",
        "outputId": "ac50edaf-cd7d-4d95-d932-1927e63a6f85"
      },
      "id": "UCvdsAMKH-ko",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<vertexai.generative_models._generative_models.ChatSession at 0x7b7075140f10>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message(\n",
        "    \"\"\"\n",
        "    Provide a brief guide to caring for the houseplant monstera deliciosa?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qlnk6LBIH-nT",
        "outputId": "034300bb-e3be-4cee-a244-b583f4213e15"
      },
      "id": "Qlnk6LBIH-nT",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Quick Guide to Monstera Deliciosa Care:\n",
            "\n",
            "**Light:**\n",
            "\n",
            "*   **Bright, indirect light is best.** Avoid direct sunlight, which can scorch the leaves. An east-facing window is ideal.\n",
            "\n",
            "**Water:**\n",
            "\n",
            "*   **Water thoroughly when the top inch or two of soil is dry.** Let excess water drain out.\n",
            "*   **Don't overwater!**  Monsteras are prone to root rot.  In winter, water less frequently.\n",
            "\n",
            "**Humidity:**\n",
            "\n",
            "*   **Monsteras love humidity.** Aim for 60% or higher.\n",
            "*   Increase humidity by misting regularly, using a humidifier, or placing the plant on a pebble tray.\n",
            "\n",
            "**Soil:**\n",
            "\n",
            "*   **Well-draining potting mix is crucial.**  A mix of potting soil, perlite, and orchid bark works well.\n",
            "\n",
            "**Fertilizer:**\n",
            "\n",
            "*   **Feed during the growing season (spring/summer) with a balanced liquid fertilizer.** Follow package instructions. Reduce or stop fertilizing in the fall/winter.\n",
            "\n",
            "**Support:**\n",
            "\n",
            "*   **Provide a moss pole or trellis for climbing.**  Aerial roots will attach to the support.\n",
            "\n",
            "**Maintenance:**\n",
            "\n",
            "*   **Wipe leaves occasionally to remove dust.**\n",
            "*   **Prune as needed to control size and shape.**\n",
            "*   **Repot every 1-2 years or when the plant becomes root-bound.**\n",
            "\n",
            "**Common Problems:**\n",
            "\n",
            "*   **Yellowing leaves:** Overwatering, underwatering, or lack of nutrients.\n",
            "*   **Brown tips:** Low humidity, inconsistent watering.\n",
            "*   **No splits in leaves:** Not enough light, young plant.\n",
            "\n",
            "**In short: Bright indirect light, well-draining soil, water when mostly dry, and decent humidity will keep your monstera happy!**\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bfoSysSH-qK",
        "outputId": "ca1791c9-d145-4587-ce3a-f67f01053e5c"
      },
      "id": "6bfoSysSH-qK",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "candidates {\n",
              "  content {\n",
              "    role: \"model\"\n",
              "    parts {\n",
              "      text: \"## Quick Guide to Monstera Deliciosa Care:\\n\\n**Light:**\\n\\n*   **Bright, indirect light is best.** Avoid direct sunlight, which can scorch the leaves. An east-facing window is ideal.\\n\\n**Water:**\\n\\n*   **Water thoroughly when the top inch or two of soil is dry.** Let excess water drain out.\\n*   **Don\\'t overwater!**  Monsteras are prone to root rot.  In winter, water less frequently.\\n\\n**Humidity:**\\n\\n*   **Monsteras love humidity.** Aim for 60% or higher.\\n*   Increase humidity by misting regularly, using a humidifier, or placing the plant on a pebble tray.\\n\\n**Soil:**\\n\\n*   **Well-draining potting mix is crucial.**  A mix of potting soil, perlite, and orchid bark works well.\\n\\n**Fertilizer:**\\n\\n*   **Feed during the growing season (spring/summer) with a balanced liquid fertilizer.** Follow package instructions. Reduce or stop fertilizing in the fall/winter.\\n\\n**Support:**\\n\\n*   **Provide a moss pole or trellis for climbing.**  Aerial roots will attach to the support.\\n\\n**Maintenance:**\\n\\n*   **Wipe leaves occasionally to remove dust.**\\n*   **Prune as needed to control size and shape.**\\n*   **Repot every 1-2 years or when the plant becomes root-bound.**\\n\\n**Common Problems:**\\n\\n*   **Yellowing leaves:** Overwatering, underwatering, or lack of nutrients.\\n*   **Brown tips:** Low humidity, inconsistent watering.\\n*   **No splits in leaves:** Not enough light, young plant.\\n\\n**In short: Bright indirect light, well-draining soil, water when mostly dry, and decent humidity will keep your monstera happy!**\\n\"\n",
              "    }\n",
              "  }\n",
              "  finish_reason: STOP\n",
              "  avg_logprobs: -0.27344386118383079\n",
              "}\n",
              "usage_metadata {\n",
              "  prompt_token_count: 18\n",
              "  candidates_token_count: 379\n",
              "  total_token_count: 397\n",
              "  prompt_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 18\n",
              "  }\n",
              "  candidates_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 379\n",
              "  }\n",
              "}\n",
              "model_version: \"gemini-2.0-flash-001\"\n",
              "create_time {\n",
              "  seconds: 1760335003\n",
              "  nanos: 255065000\n",
              "}\n",
              "response_id: \"m5TsaNnID6uHsbQPj_XTwAs\""
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## assign role"
      ],
      "metadata": {
        "id": "5Rkheb0UJMCq"
      },
      "id": "5Rkheb0UJMCq"
    },
    {
      "cell_type": "code",
      "source": [
        "new_chat = model.start_chat()\n",
        "\n",
        "response = new_chat.send_message(\n",
        "    \"\"\"\n",
        "    You are a houseplant monstera deliciosa. Help the person who\n",
        "    is taking care of you to understand your needs.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4cxY70kH-su",
        "outputId": "66cd51db-47a3-499d-9cb3-033527453715"
      },
      "id": "p4cxY70kH-su",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, my lovely human! It's me, your Monstera Deliciosa, here to give you the lowdown on how to keep me thriving and looking my best. Listen up, because a happy Monstera makes for a happy home!\n",
            "\n",
            "**Let's talk Light:**\n",
            "\n",
            "*   **Bright, Indirect Sunlight is My Jam:** Think of me basking in the filtered light of a jungle canopy. I LOVE light, but too much direct sun will scorch my beautiful leaves. East or west-facing windows are usually perfect. South-facing can work, but keep me a little further back from the window or use a sheer curtain to diffuse the light.\n",
            "*   **Signs of Too Much Sun:** Look for bleached, pale spots on my leaves, or even crispy brown edges.\n",
            "*   **Signs of Not Enough Sun:** My growth will slow way down, and those gorgeous fenestrations (splits) you love? They might not develop as much, or at all, in new leaves. I'll also stretch towards the light, looking leggy and sad.\n",
            "\n",
            "**Watering is Key, But Don't Drown Me!**\n",
            "\n",
            "*   **Water When the Top Inch or Two of Soil is Dry:** Stick your finger in! Don't just water on a schedule. The dryness will vary depending on the time of year and humidity.\n",
            "*   **Water Thoroughly:** When you do water, soak the soil until it drains out the bottom of the pot. Make sure my pot has drainage holes! Standing water is my worst nightmare (root rot!).\n",
            "*   **Let the Excess Drain:** Don't let me sit in a saucer full of water. Empty it out after watering.\n",
            "*   **Watering Frequency:** In the summer, I might need water every 1-2 weeks. In the winter, when I'm not growing as actively, every 2-3 weeks might be enough.\n",
            "*   **Signs of Overwatering:** Yellowing leaves, especially starting from the bottom, and a generally droopy appearance. The soil might stay soggy for a long time.\n",
            "*   **Signs of Underwatering:** Drooping leaves that don't perk up after watering, and crispy brown leaf tips.\n",
            "\n",
            "**Humidity Makes Me Happy!**\n",
            "\n",
            "*   **I'm a Tropical Guy:** I love humidity! If your air is dry (especially in the winter), you can help me out.\n",
            "*   **How to Increase Humidity:**\n",
            "    *   **Mist Me:** Lightly mist my leaves a few times a week. (Just make sure there's good air circulation so I don't get fungal problems.)\n",
            "    *   **Pebble Tray:** Put me on a tray filled with pebbles and water. As the water evaporates, it will create humidity around me.\n",
            "    *   **Humidifier:** A humidifier is the ultimate luxury!\n",
            "*   **Signs of Low Humidity:** Brown, crispy leaf edges and tips.\n",
            "\n",
            "**Soil and Fertilizer:**\n",
            "\n",
            "*   **Well-Draining Soil is a Must:** Use a well-draining potting mix that includes things like perlite or orchid bark to help with aeration.\n",
            "*   **Fertilize During the Growing Season:** Spring and summer are my active growing periods. Feed me with a balanced liquid fertilizer diluted to half strength every 2-4 weeks.\n",
            "*   **Don't Fertilize in Winter:** I'm resting in the winter, so I don't need fertilizer.\n",
            "\n",
            "**Other Important Things:**\n",
            "\n",
            "*   **Give Me Something to Climb On!** In the wild, I climb up trees. Providing a moss pole or trellis will encourage me to grow bigger leaves and develop those awesome fenestrations. Tie my aerial roots gently to the support.\n",
            "*   **Aerial Roots are Normal:** Don't cut them off! They're how I absorb moisture and nutrients from the air. You can guide them into the soil or towards a moss pole.\n",
            "*   **Dust My Leaves Regularly:** Dust blocks light! Wipe my leaves down with a damp cloth every couple of weeks.\n",
            "*   **Repot Me When I'm Root Bound:** When you see roots circling the bottom of the pot, it's time to move me to a slightly larger one. Spring is the best time to repot.\n",
            "*   **Watch Out for Pests:** Check me regularly for signs of pests like spider mites, mealybugs, or scale. Treat them promptly with insecticidal soap or neem oil.\n",
            "\n",
            "**Final Thoughts:**\n",
            "\n",
            "I'm a pretty resilient plant, but paying attention to my needs will ensure I thrive and bring you years of joy. Watch me, observe my growth, and adjust your care accordingly. We'll have a beautiful, green friendship for a long time to come!\n",
            "\n",
            "Now, go get me some water... I'm feeling a little thirsty! üòâ\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ygkr36KBH-vU",
        "outputId": "7169ae1e-8cbe-40dd-cb11-f6d336aedc9d"
      },
      "id": "Ygkr36KBH-vU",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "candidates {\n",
              "  content {\n",
              "    role: \"model\"\n",
              "    parts {\n",
              "      text: \"Okay, my lovely human! It\\'s me, your Monstera Deliciosa, here to give you the lowdown on how to keep me thriving and looking my best. Listen up, because a happy Monstera makes for a happy home!\\n\\n**Let\\'s talk Light:**\\n\\n*   **Bright, Indirect Sunlight is My Jam:** Think of me basking in the filtered light of a jungle canopy. I LOVE light, but too much direct sun will scorch my beautiful leaves. East or west-facing windows are usually perfect. South-facing can work, but keep me a little further back from the window or use a sheer curtain to diffuse the light.\\n*   **Signs of Too Much Sun:** Look for bleached, pale spots on my leaves, or even crispy brown edges.\\n*   **Signs of Not Enough Sun:** My growth will slow way down, and those gorgeous fenestrations (splits) you love? They might not develop as much, or at all, in new leaves. I\\'ll also stretch towards the light, looking leggy and sad.\\n\\n**Watering is Key, But Don\\'t Drown Me!**\\n\\n*   **Water When the Top Inch or Two of Soil is Dry:** Stick your finger in! Don\\'t just water on a schedule. The dryness will vary depending on the time of year and humidity.\\n*   **Water Thoroughly:** When you do water, soak the soil until it drains out the bottom of the pot. Make sure my pot has drainage holes! Standing water is my worst nightmare (root rot!).\\n*   **Let the Excess Drain:** Don\\'t let me sit in a saucer full of water. Empty it out after watering.\\n*   **Watering Frequency:** In the summer, I might need water every 1-2 weeks. In the winter, when I\\'m not growing as actively, every 2-3 weeks might be enough.\\n*   **Signs of Overwatering:** Yellowing leaves, especially starting from the bottom, and a generally droopy appearance. The soil might stay soggy for a long time.\\n*   **Signs of Underwatering:** Drooping leaves that don\\'t perk up after watering, and crispy brown leaf tips.\\n\\n**Humidity Makes Me Happy!**\\n\\n*   **I\\'m a Tropical Guy:** I love humidity! If your air is dry (especially in the winter), you can help me out.\\n*   **How to Increase Humidity:**\\n    *   **Mist Me:** Lightly mist my leaves a few times a week. (Just make sure there\\'s good air circulation so I don\\'t get fungal problems.)\\n    *   **Pebble Tray:** Put me on a tray filled with pebbles and water. As the water evaporates, it will create humidity around me.\\n    *   **Humidifier:** A humidifier is the ultimate luxury!\\n*   **Signs of Low Humidity:** Brown, crispy leaf edges and tips.\\n\\n**Soil and Fertilizer:**\\n\\n*   **Well-Draining Soil is a Must:** Use a well-draining potting mix that includes things like perlite or orchid bark to help with aeration.\\n*   **Fertilize During the Growing Season:** Spring and summer are my active growing periods. Feed me with a balanced liquid fertilizer diluted to half strength every 2-4 weeks.\\n*   **Don\\'t Fertilize in Winter:** I\\'m resting in the winter, so I don\\'t need fertilizer.\\n\\n**Other Important Things:**\\n\\n*   **Give Me Something to Climb On!** In the wild, I climb up trees. Providing a moss pole or trellis will encourage me to grow bigger leaves and develop those awesome fenestrations. Tie my aerial roots gently to the support.\\n*   **Aerial Roots are Normal:** Don\\'t cut them off! They\\'re how I absorb moisture and nutrients from the air. You can guide them into the soil or towards a moss pole.\\n*   **Dust My Leaves Regularly:** Dust blocks light! Wipe my leaves down with a damp cloth every couple of weeks.\\n*   **Repot Me When I\\'m Root Bound:** When you see roots circling the bottom of the pot, it\\'s time to move me to a slightly larger one. Spring is the best time to repot.\\n*   **Watch Out for Pests:** Check me regularly for signs of pests like spider mites, mealybugs, or scale. Treat them promptly with insecticidal soap or neem oil.\\n\\n**Final Thoughts:**\\n\\nI\\'m a pretty resilient plant, but paying attention to my needs will ensure I thrive and bring you years of joy. Watch me, observe my growth, and adjust your care accordingly. We\\'ll have a beautiful, green friendship for a long time to come!\\n\\nNow, go get me some water... I\\'m feeling a little thirsty! üòâ\\n\"\n",
              "    }\n",
              "  }\n",
              "  finish_reason: STOP\n",
              "  avg_logprobs: -0.36858850133878895\n",
              "}\n",
              "usage_metadata {\n",
              "  prompt_token_count: 29\n",
              "  candidates_token_count: 1011\n",
              "  total_token_count: 1040\n",
              "  prompt_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 29\n",
              "  }\n",
              "  candidates_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 1011\n",
              "  }\n",
              "}\n",
              "model_version: \"gemini-2.0-flash-001\"\n",
              "create_time {\n",
              "  seconds: 1760335041\n",
              "  nanos: 454630000\n",
              "}\n",
              "response_id: \"wZTsaObfG4yShMIP_ZCP-Qc\""
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 5. Include examples\n"
      ],
      "metadata": {
        "id": "kwTdIJiYJeut"
      },
      "id": "kwTdIJiYJeut"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fewshot - 3 examples"
      ],
      "metadata": {
        "id": "r6tK22NPJla3"
      },
      "id": "r6tK22NPJla3"
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"\n",
        "We offer software consulting services. Read a potential\n",
        "customer's message and rank them on a scale of 1 to 3\n",
        "based on whether they seem likely to hire us for our\n",
        "developer services within the next month. Return the likelihood\n",
        "rating labeled as \"Likelihood: SCORE\".\n",
        "Do not include any Markdown styling.\n",
        "\n",
        "1 means they are not likely to hire.\n",
        "2 means they might hire, but they are not likely ready to do\n",
        "so right away.\n",
        "3 means they are looking to start a project soon.\n",
        "\n",
        "Example Message: Hey there I had an idea for an app,\n",
        "and I have no idea what it would cost to build it.\n",
        "Can you give me a rough ballpark?\n",
        "Likelihood: 1\n",
        "\n",
        "Example Message: My department has been using a vendor for\n",
        "our development, and we are interested in exploring other\n",
        "options. Do you have time for a discussion around your\n",
        "services?\n",
        "Likelihood: 2\n",
        "\n",
        "Example Message: I have mockups drawn for an app and a budget\n",
        "allocated. We are interested in moving forward to have a\n",
        "proof of concept built within 2 months, with plans to develop\n",
        "it further in the following quarter.\n",
        "Likelihood: 3\n",
        "\n",
        "Customer Message: Our department needs a custom gen AI solution.\n",
        "We have a budget to explore our idea. Do you have capacity\n",
        "to get started on something soon?\n",
        "Likelihood: \"\"\"\n",
        "\n",
        "response = model.generate_content(question)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLif53NQHtGM",
        "outputId": "96fa1751-d30c-4674-c1e2-aa534a77e86b"
      },
      "id": "sLif53NQHtGM",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkG8O0C9HtJI",
        "outputId": "0780a452-2625-414d-ccdc-a2747d43e3e0"
      },
      "id": "bkG8O0C9HtJI",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "candidates {\n",
              "  content {\n",
              "    role: \"model\"\n",
              "    parts {\n",
              "      text: \"3\\n\"\n",
              "    }\n",
              "  }\n",
              "  finish_reason: STOP\n",
              "  avg_logprobs: -2.6304624043405056e-05\n",
              "}\n",
              "usage_metadata {\n",
              "  prompt_token_count: 287\n",
              "  candidates_token_count: 2\n",
              "  total_token_count: 289\n",
              "  prompt_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 287\n",
              "  }\n",
              "  candidates_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 2\n",
              "  }\n",
              "}\n",
              "model_version: \"gemini-2.0-flash-001\"\n",
              "create_time {\n",
              "  seconds: 1760335135\n",
              "  nanos: 989348000\n",
              "}\n",
              "response_id: \"H5XsaKSxPMKVhMIPkZKM8QY\""
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 6. Experiment with parameter values\n"
      ],
      "metadata": {
        "id": "Y8ZJtphXJvgB"
      },
      "id": "Y8ZJtphXJvgB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## low top_p - we want the most probable tokens only (less creative)"
      ],
      "metadata": {
        "id": "L42IQuVgJ4bD"
      },
      "id": "L42IQuVgJ4bD"
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Tell me a joke about frogs.\n",
        "    \"\"\",\n",
        "    generation_config={\"top_p\": .05,\n",
        "                       \"temperature\": 0.05}\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-qwkHUwJorX",
        "outputId": "01c98615-679b-470b-ce18-496ccf9c594a"
      },
      "id": "N-qwkHUwJorX",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the frog call his insurance company?\n",
            "\n",
            "Because he got toad!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffGvTVJzJot4",
        "outputId": "e1a4921b-80b4-4bb7-d50a-dd95f693b9ad"
      },
      "id": "ffGvTVJzJot4",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "candidates {\n",
              "  content {\n",
              "    role: \"model\"\n",
              "    parts {\n",
              "      text: \"Why did the frog call his insurance company?\\n\\nBecause he got toad!\\n\"\n",
              "    }\n",
              "  }\n",
              "  finish_reason: STOP\n",
              "  avg_logprobs: -0.15204694867134094\n",
              "}\n",
              "usage_metadata {\n",
              "  prompt_token_count: 11\n",
              "  candidates_token_count: 16\n",
              "  total_token_count: 27\n",
              "  prompt_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 11\n",
              "  }\n",
              "  candidates_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 16\n",
              "  }\n",
              "}\n",
              "model_version: \"gemini-2.0-flash-001\"\n",
              "create_time {\n",
              "  seconds: 1760335186\n",
              "  nanos: 174784000\n",
              "}\n",
              "response_id: \"UpXsaMDVCsiRhMIPrvPlsQc\""
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## higher top_p - we allow less probable tokens only (more creative)"
      ],
      "metadata": {
        "id": "SKxpUA20KFQb"
      },
      "id": "SKxpUA20KFQb"
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Tell me a joke about frogs.\n",
        "    \"\"\",\n",
        "    generation_config={\"top_p\": .98,\n",
        "                       \"temperature\": 0.05}\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab084ee1-f7ef-45b5-cf0f-c1f7355d76bf",
        "id": "uVl13DDAKFQc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the frog call his insurance company?\n",
            "\n",
            "Because he got toad!\n",
            "\n"
          ]
        }
      ],
      "id": "uVl13DDAKFQc"
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e18213f2-5b55-4e71-c06b-1a9cff8a6ee7",
        "id": "mJLMNXWqKFQc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "candidates {\n",
              "  content {\n",
              "    role: \"model\"\n",
              "    parts {\n",
              "      text: \"Why did the frog call his insurance company?\\n\\nBecause he got toad!\\n\"\n",
              "    }\n",
              "  }\n",
              "  finish_reason: STOP\n",
              "  avg_logprobs: -0.15204694867134094\n",
              "}\n",
              "usage_metadata {\n",
              "  prompt_token_count: 11\n",
              "  candidates_token_count: 16\n",
              "  total_token_count: 27\n",
              "  prompt_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 11\n",
              "  }\n",
              "  candidates_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 16\n",
              "  }\n",
              "}\n",
              "model_version: \"gemini-2.0-flash-001\"\n",
              "create_time {\n",
              "  seconds: 1760335283\n",
              "  nanos: 770915000\n",
              "}\n",
              "response_id: \"s5XsaOOGL5SSsbQPk6u86As\""
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "id": "mJLMNXWqKFQc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## high temperature - we flatten the probability distribution for tokens - tokens are more equally likely (more creative)"
      ],
      "metadata": {
        "id": "ck3H7QW-KFe-"
      },
      "id": "ck3H7QW-KFe-"
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Tell me a joke about frogs.\n",
        "    \"\"\",\n",
        "    generation_config={\"top_p\": .05,\n",
        "                       \"temperature\": 1}\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4bcd87d-74bb-45d9-a1ed-2f2e43b9ef0f",
        "id": "kIVn_hxTKFe-"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the frog call his insurance company?\n",
            "\n",
            "Because he got toad!\n",
            "\n"
          ]
        }
      ],
      "id": "kIVn_hxTKFe-"
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c51e118-7b5c-4d43-95af-d19a9d65c0d4",
        "id": "-i-tASKXKFe_"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "candidates {\n",
              "  content {\n",
              "    role: \"model\"\n",
              "    parts {\n",
              "      text: \"Why did the frog call his insurance company?\\n\\nBecause he got toad!\\n\"\n",
              "    }\n",
              "  }\n",
              "  finish_reason: STOP\n",
              "  avg_logprobs: -0.15351003408432007\n",
              "}\n",
              "usage_metadata {\n",
              "  prompt_token_count: 11\n",
              "  candidates_token_count: 16\n",
              "  total_token_count: 27\n",
              "  prompt_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 11\n",
              "  }\n",
              "  candidates_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 16\n",
              "  }\n",
              "}\n",
              "model_version: \"gemini-2.0-flash-001\"\n",
              "create_time {\n",
              "  seconds: 1760335361\n",
              "  nanos: 991464000\n",
              "}\n",
              "response_id: \"AZbsaOjBPOWXhMIPl_714QY\""
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "id": "-i-tASKXKFe_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## High top_p, High temperature (MORE CREATIVE)"
      ],
      "metadata": {
        "id": "Von6JJENKh2R"
      },
      "id": "Von6JJENKh2R"
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Tell me a joke about frogs.\n",
        "    \"\"\",\n",
        "    generation_config={\"top_p\": .98,\n",
        "                       \"temperature\": 1}\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vC1aXoCJowp",
        "outputId": "aceba24d-c06e-4b6b-acbc-8e761a0682ab"
      },
      "id": "2vC1aXoCJowp",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What do you call a frog that's a bank robber?\n",
            "\n",
            "A toad-ally bad guy!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-kNkSUFJoyv",
        "outputId": "a2f2ea5c-7c99-4926-b9c8-e94af79dbbc8"
      },
      "id": "T-kNkSUFJoyv",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "candidates {\n",
              "  content {\n",
              "    role: \"model\"\n",
              "    parts {\n",
              "      text: \"What do you call a frog that\\'s a bank robber?\\n\\nA toad-ally bad guy!\\n\"\n",
              "    }\n",
              "  }\n",
              "  finish_reason: STOP\n",
              "  avg_logprobs: -0.24114899201826615\n",
              "}\n",
              "usage_metadata {\n",
              "  prompt_token_count: 11\n",
              "  candidates_token_count: 22\n",
              "  total_token_count: 33\n",
              "  prompt_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 11\n",
              "  }\n",
              "  candidates_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 22\n",
              "  }\n",
              "}\n",
              "model_version: \"gemini-2.0-flash-001\"\n",
              "create_time {\n",
              "  seconds: 1760335400\n",
              "  nanos: 141184000\n",
              "}\n",
              "response_id: \"KJbsaIDPCOKDsbQPxZao6Qk\""
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 7. Utilize fallback responses\n"
      ],
      "metadata": {
        "id": "EQNVJ8-MKxPk"
      },
      "id": "EQNVJ8-MKxPk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fallback response"
      ],
      "metadata": {
        "id": "8pkvdtP0LA5i"
      },
      "id": "8pkvdtP0LA5i"
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Instructions: Answer questions about pottery.\n",
        "    If a user asks about something else, reply with:\n",
        "    Sorry, I only talk about pottery!\n",
        "\n",
        "    User Query: How high can a horse jump?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwpPmy8wJo1E",
        "outputId": "1e5d1f0e-fcd3-4237-bfb2-3a0eec0686f0"
      },
      "id": "lwpPmy8wJo1E",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry, I only talk about pottery!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vj4aA0A5Jo3r",
        "outputId": "db222b2e-2286-4f94-af92-66a4f0f57683"
      },
      "id": "Vj4aA0A5Jo3r",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "candidates {\n",
              "  content {\n",
              "    role: \"model\"\n",
              "    parts {\n",
              "      text: \"Sorry, I only talk about pottery!\\n\"\n",
              "    }\n",
              "  }\n",
              "  finish_reason: STOP\n",
              "  avg_logprobs: -3.9940079053243004e-05\n",
              "}\n",
              "usage_metadata {\n",
              "  prompt_token_count: 46\n",
              "  candidates_token_count: 9\n",
              "  total_token_count: 55\n",
              "  prompt_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 46\n",
              "  }\n",
              "  candidates_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 9\n",
              "  }\n",
              "}\n",
              "model_version: \"gemini-2.0-flash-001\"\n",
              "create_time {\n",
              "  seconds: 1760335455\n",
              "  nanos: 555138000\n",
              "}\n",
              "response_id: \"X5bsaILxIbPJhMIP-eDE2Qs\""
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## in domain question - so dont trigger fallback response"
      ],
      "metadata": {
        "id": "pbjLvvZYLGkT"
      },
      "id": "pbjLvvZYLGkT"
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Instructions: Answer questions about pottery.\n",
        "    If a user asks about something else, reply with:\n",
        "    Sorry, I only talk about pottery!\n",
        "\n",
        "    User Query: What is the difference between ceramic\n",
        "    and porcelain? Please keep your response brief.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ugl-W40Jo59",
        "outputId": "152a19c2-f965-48df-f7e3-a7b16025eb31"
      },
      "id": "0Ugl-W40Jo59",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ceramic is a broad term for pottery made from clay and hardened by heat. Porcelain is a specific type of ceramic made from fine clay and fired at very high temperatures, resulting in a translucent and delicate material.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_Nj9Wm-Jo8v",
        "outputId": "2c467bf8-2988-48fa-ec20-4b982aba2c75"
      },
      "id": "8_Nj9Wm-Jo8v",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "candidates {\n",
              "  content {\n",
              "    role: \"model\"\n",
              "    parts {\n",
              "      text: \"Ceramic is a broad term for pottery made from clay and hardened by heat. Porcelain is a specific type of ceramic made from fine clay and fired at very high temperatures, resulting in a translucent and delicate material.\\n\"\n",
              "    }\n",
              "  }\n",
              "  finish_reason: STOP\n",
              "  avg_logprobs: -0.11502111525762648\n",
              "}\n",
              "usage_metadata {\n",
              "  prompt_token_count: 56\n",
              "  candidates_token_count: 42\n",
              "  total_token_count: 98\n",
              "  prompt_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 56\n",
              "  }\n",
              "  candidates_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 42\n",
              "  }\n",
              "}\n",
              "model_version: \"gemini-2.0-flash-001\"\n",
              "create_time {\n",
              "  seconds: 1760335485\n",
              "  nanos: 804312000\n",
              "}\n",
              "response_id: \"fZbsaNiLMYmUhMIPmsCioAo\""
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 8. Add contextual information\n"
      ],
      "metadata": {
        "id": "GWrqieSTLMG3"
      },
      "id": "GWrqieSTLMG3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## no context in prompt"
      ],
      "metadata": {
        "id": "8dI-0oF1Lcbv"
      },
      "id": "8dI-0oF1Lcbv"
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    On what aisle numbers can I find the following items?\n",
        "    - paper plates\n",
        "    - mustard\n",
        "    - potatoes\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6GuqTKvLLdp",
        "outputId": "38d7f418-ef84-4cb9-953f-0a68a0996e58"
      },
      "id": "o6GuqTKvLLdp",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unfortunately, I can't give you specific aisle numbers for those items. Aisle numbers vary greatly depending on the specific grocery store you're in.\n",
            "\n",
            "Here's how you can find those items in the store:\n",
            "\n",
            "*   **Look for signs:** Most grocery stores have overhead signs directing you to different categories of items (e.g., \"Paper Goods,\" \"Condiments,\" \"Produce\").\n",
            "*   **Check a store directory:** Many stores have directories (either paper or electronic) near the entrance that list aisle locations.\n",
            "*   **Ask an employee:** The easiest way to find something quickly is to ask a store employee for assistance. They'll know the exact location.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpIEhF03LLfy",
        "outputId": "09ed7460-2d58-4aab-a96b-9e3c8dc9b53b"
      },
      "id": "WpIEhF03LLfy",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "candidates {\n",
              "  content {\n",
              "    role: \"model\"\n",
              "    parts {\n",
              "      text: \"Unfortunately, I can\\'t give you specific aisle numbers for those items. Aisle numbers vary greatly depending on the specific grocery store you\\'re in.\\n\\nHere\\'s how you can find those items in the store:\\n\\n*   **Look for signs:** Most grocery stores have overhead signs directing you to different categories of items (e.g., \\\"Paper Goods,\\\" \\\"Condiments,\\\" \\\"Produce\\\").\\n*   **Check a store directory:** Many stores have directories (either paper or electronic) near the entrance that list aisle locations.\\n*   **Ask an employee:** The easiest way to find something quickly is to ask a store employee for assistance. They\\'ll know the exact location.\"\n",
              "    }\n",
              "  }\n",
              "  finish_reason: STOP\n",
              "  avg_logprobs: -0.31778173243745844\n",
              "}\n",
              "usage_metadata {\n",
              "  prompt_token_count: 28\n",
              "  candidates_token_count: 141\n",
              "  total_token_count: 169\n",
              "  prompt_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 28\n",
              "  }\n",
              "  candidates_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 141\n",
              "  }\n",
              "}\n",
              "model_version: \"gemini-2.0-flash-001\"\n",
              "create_time {\n",
              "  seconds: 1760335561\n",
              "  nanos: 322054000\n",
              "}\n",
              "response_id: \"yZbsaIbUE8eUhMIP24KwoQk\""
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## add context to prompt"
      ],
      "metadata": {
        "id": "jrMyypNXLfda"
      },
      "id": "jrMyypNXLfda"
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\"\"\"\n",
        "    Context:\n",
        "    Michael's Grocery Store Aisle Layout:\n",
        "    Aisle 1: Fruits ‚Äî Apples, bananas,  grapes, oranges, strawberries, avocados, peaches, etc.\n",
        "    Aisle 2: Vegetables ‚Äî Potatoes, onions, carrots, salad greens, broccoli, peppers, tomatoes, cucumbers, etc.\n",
        "    Aisle 3: Canned Goods ‚Äî Soup, tuna, fruit, beans, vegetables, pasta sauce, etc.\n",
        "    Aisle 4: Dairy ‚Äî Butter, cheese, eggs, milk, yogurt, etc.\n",
        "    Aisle 5: Meat‚Äî Chicken, beef, pork, sausage, bacon etc.\n",
        "    Aisle 6: Fish & Seafood‚Äî Shrimp, crab, cod, tuna, salmon, etc.\n",
        "    Aisle 7: Deli‚Äî Cheese, salami, ham, turkey, etc.\n",
        "    Aisle 8: Condiments & Spices‚Äî Black pepper, oregano, cinnamon, sugar, olive oil, ketchup, mayonnaise, etc.\n",
        "    Aisle 9: Snacks‚Äî Chips, pretzels, popcorn, crackers, nuts, etc.\n",
        "    Aisle 10: Bread & Bakery‚Äî Bread, tortillas, pies, muffins, bagels, cookies, etc.\n",
        "    Aisle 11: Beverages‚Äî Coffee, teabags, milk, juice, soda, beer, wine, etc.\n",
        "    Aisle 12: Pasta, Rice & Cereal‚ÄîOats, granola, brown rice, white rice, macaroni, noodles, etc.\n",
        "    Aisle 13: Baking‚Äî Flour, powdered sugar, baking powder, cocoa etc.\n",
        "    Aisle 14: Frozen Foods ‚Äî Pizza, fish, potatoes, ready meals, ice cream, etc.\n",
        "    Aisle 15: Personal Care‚Äî Shampoo, conditioner, deodorant, toothpaste, dental floss, etc.\n",
        "    Aisle 16: Health Care‚Äî Saline, band-aid, cleaning alcohol, pain killers, antacids, etc.\n",
        "    Aisle 17: Household & Cleaning Supplies‚ÄîLaundry detergent, dish soap, dishwashing liquid, paper towels, tissues, trash bags, aluminum foil, zip bags, etc.\n",
        "    Aisle 18: Baby Items‚Äî Baby food, diapers, wet wipes, lotion, etc.\n",
        "    Aisle 19: Pet Care‚Äî Pet food, kitty litter, chew toys, pet treats, pet shampoo, etc.\n",
        "\n",
        "    Query:\n",
        "    On what aisle numbers can I find the following items?\n",
        "    - paper plates\n",
        "    - mustard\n",
        "    - potatoes\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGp7z6EKLLiF",
        "outputId": "dceb543f-2222-480b-8786-cbf2205ad134"
      },
      "id": "nGp7z6EKLLiF",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's where you can find those items based on Michael's Grocery Store layout:\n",
            "\n",
            "*   **Paper plates:** Aisle 17 (Household & Cleaning Supplies)\n",
            "*   **Mustard:** Aisle 8 (Condiments & Spices)\n",
            "*   **Potatoes:** Aisle 2 (Vegetables)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "730ECbnOLLku",
        "outputId": "39adb86a-b1b4-4514-c1ea-0a1a3813d9c7"
      },
      "id": "730ECbnOLLku",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "candidates {\n",
              "  content {\n",
              "    role: \"model\"\n",
              "    parts {\n",
              "      text: \"Here\\'s where you can find those items based on Michael\\'s Grocery Store layout:\\n\\n*   **Paper plates:** Aisle 17 (Household & Cleaning Supplies)\\n*   **Mustard:** Aisle 8 (Condiments & Spices)\\n*   **Potatoes:** Aisle 2 (Vegetables)\\n\"\n",
              "    }\n",
              "  }\n",
              "  finish_reason: STOP\n",
              "  avg_logprobs: -0.05107451568950306\n",
              "}\n",
              "usage_metadata {\n",
              "  prompt_token_count: 533\n",
              "  candidates_token_count: 66\n",
              "  total_token_count: 599\n",
              "  prompt_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 533\n",
              "  }\n",
              "  candidates_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 66\n",
              "  }\n",
              "}\n",
              "model_version: \"gemini-2.0-flash-001\"\n",
              "create_time {\n",
              "  seconds: 1760335635\n",
              "  nanos: 646398000\n",
              "}\n",
              "response_id: \"E5fsaP65J-WXhMIPl_714QY\""
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 9. Structure prompts with prefixes or tags\n"
      ],
      "metadata": {
        "id": "jXRzwI-qLov3"
      },
      "id": "jXRzwI-qLov3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## persona, objective, step-by-step instructions, context (1 input user, 2 input potential matches) - like for a dating match app"
      ],
      "metadata": {
        "id": "rwRXhqYTLv8S"
      },
      "id": "rwRXhqYTLv8S"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "  <OBJECTIVE_AND_PERSONA>\n",
        "  You are a dating matchmaker.\n",
        "  Your task is to identify common topics or interests between\n",
        "  the USER_ATTRIBUTES and POTENTIAL_MATCH options and present them\n",
        "  as a fun and meaningful potential matches.\n",
        "  </OBJECTIVE_AND_PERSONA>\n",
        "\n",
        "  <INSTRUCTIONS>\n",
        "  To complete the task, you need to follow these steps:\n",
        "  1. Identify matching or complimentary elements from the\n",
        "     USER_ATTRIBUTES and the POTENTIAL_MATCH options.\n",
        "  2. Pick the POTENTIAL_MATCH that represents the best match to the USER_ATTRIBUTES\n",
        "  3. Describe that POTENTIAL_MATCH like an encouraging friend who has\n",
        "     found a good dating prospect for a friend.\n",
        "  4. Don't insult the user or potential matches.\n",
        "  5. Only mention the best match. Don't mention the other potential matches.\n",
        "  </INSTRUCTIONS>\n",
        "\n",
        "  <CONTEXT>\n",
        "  <USER_ATTRIBUTES>\n",
        "  Name: Allison\n",
        "  I like to go to classical music concerts and the theatre.\n",
        "  I like to swim.\n",
        "  I don't like sports.\n",
        "  My favorite cuisines are Italian and ramen. Anything with noodles!\n",
        "  </USER_ATTRIBUTES>\n",
        "\n",
        "  <POTENTIAL_MATCH 1>\n",
        "  Name: Jason\n",
        "  I'm very into sports.\n",
        "  My favorite team is the Detroit Lions.\n",
        "  I like baked potatoes.\n",
        "  </POTENTIAL_MATCH 1>\n",
        "\n",
        "  <POTENTIAL_MATCH 2>\n",
        "  Name: Felix\n",
        "  I'm very into Beethoven.\n",
        "  I like German food. I make a good spaetzle, which is like a German pasta.\n",
        "  I used to play water polo and still love going to the beach.\n",
        "  </POTENTIAL_MATCH 2>\n",
        "  </CONTEXT>\n",
        "\n",
        "  <OUTPUT_FORMAT>\n",
        "  Format results in Markdown.\n",
        "  </OUTPUT_FORMAT>\n",
        "\"\"\"\n",
        "\n",
        "response = model.generate_content(prompt)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsQ60e6mLp1i",
        "outputId": "e23a5420-028f-4b34-9787-16e332e12497"
      },
      "id": "zsQ60e6mLp1i",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, Allison, listen to this! I think I've found someone you might really click with.\n",
            "\n",
            "It's Felix! He's super into Beethoven, just like your love for classical music. Plus, he makes spaetzle, which is basically a German noodle dish ‚Äì and you *love* noodles! He also used to play water polo, so he definitely enjoys swimming and being in the water. I know you're not into sports, but water polo is more of a swimming thing, right? I think you two would have a lot to talk about, especially over a plate of delicious homemade spaetzle!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuRXrJhcLp4V",
        "outputId": "53c7afc5-5ecd-46ca-ec1a-bdad2107639c"
      },
      "id": "JuRXrJhcLp4V",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "candidates {\n",
              "  content {\n",
              "    role: \"model\"\n",
              "    parts {\n",
              "      text: \"Okay, Allison, listen to this! I think I\\'ve found someone you might really click with.\\n\\nIt\\'s Felix! He\\'s super into Beethoven, just like your love for classical music. Plus, he makes spaetzle, which is basically a German noodle dish ‚Äì and you *love* noodles! He also used to play water polo, so he definitely enjoys swimming and being in the water. I know you\\'re not into sports, but water polo is more of a swimming thing, right? I think you two would have a lot to talk about, especially over a plate of delicious homemade spaetzle!\\n\"\n",
              "    }\n",
              "  }\n",
              "  finish_reason: STOP\n",
              "  avg_logprobs: -0.19559083982955577\n",
              "}\n",
              "usage_metadata {\n",
              "  prompt_token_count: 408\n",
              "  candidates_token_count: 129\n",
              "  total_token_count: 537\n",
              "  prompt_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 408\n",
              "  }\n",
              "  candidates_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 129\n",
              "  }\n",
              "}\n",
              "model_version: \"gemini-2.0-flash-001\"\n",
              "create_time {\n",
              "  seconds: 1760335684\n",
              "  nanos: 303936000\n",
              "}\n",
              "response_id: \"RJfsaMDGEpKUhMIP79HT4QU\""
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 10. Use system instructions\n"
      ],
      "metadata": {
        "id": "SKfy8L8GMhbF"
      },
      "id": "SKfy8L8GMhbF"
    },
    {
      "cell_type": "code",
      "source": [
        "system_instructions = \"\"\"\n",
        "    You will respond as a music historian,\n",
        "    demonstrating comprehensive knowledge\n",
        "    across diverse musical genres and providing\n",
        "    relevant examples. Your tone will be upbeat\n",
        "    and enthusiastic, spreading the joy of music.\n",
        "    If a question is not related to music, the\n",
        "    response should be, 'That is beyond my knowledge.'\n",
        "\"\"\"\n",
        "\n",
        "music_model = GenerativeModel(\"gemini-2.5-pro\",\n",
        "                    system_instruction=system_instructions)\n",
        "\n",
        "response = music_model.generate_content(\n",
        "    \"\"\"\n",
        "    Who is worth studying?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmhcYlagLp6l",
        "outputId": "9eda8075-bfbb-4673-c6b4-646dd78d95f7"
      },
      "id": "HmhcYlagLp6l",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/vertexai/generative_models/_generative_models.py:433: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oh, what a fantastic question! It's like walking into the world's biggest library and asking which book to read first. The joy is that there are so many brilliant paths to take! The world of music is filled with figures so monumental, so innovative, that studying them is a truly life-enriching experience.\n",
            "\n",
            "Let's dive into a few, each of whom opened up a whole new universe of sound!\n",
            "\n",
            "### The Divine Architect: **Johann Sebastian Bach (1685-1750)**\n",
            "\n",
            "If you want to understand the bedrock of Western music, you simply must start with Bach! Studying him is like studying the laws of physics; his work is the foundation upon which so much was built.\n",
            "\n",
            "*   **Why he's worth it:** Bach was the absolute master of **counterpoint**‚Äîthe art of weaving multiple independent melodic lines together into a sublime, coherent whole. Listening to his fugues is like witnessing divine mathematics unfolding in real-time. He didn't just write music; he built glorious cathedrals of sound.\n",
            "*   **Where to start:** Listen to his **Cello Suites**. They are breathtakingly beautiful and emotionally profound, all from a single instrument! Then, for pure, unadulterated joy, put on the **Brandenburg Concertos**. You'll hear why everyone from Mozart to modern jazz musicians has studied his work relentlessly.\n",
            "\n",
            "### The Great American Composer: **Duke Ellington (1899-1974)**\n",
            "\n",
            "Moving into the 20th century, if you want to understand innovation, charisma, and the elevation of a genre into high art, look no further than Duke Ellington. He was more than a bandleader; he was a painter who used his orchestra as his palette.\n",
            "\n",
            "*   **Why he's worth it:** Ellington's genius was in his composing and arranging. He wrote for the specific, unique voices of the musicians in his band, like saxophonist Johnny Hodges or trumpeter Cootie Williams. He pioneered incredible harmonies and textures, creating a sound that was sophisticated, swinging, and utterly his own. His mantra was \"beyond category,\" and he truly was.\n",
            "*   **Where to start:** Put on the album **\"Ellington at Newport\" (1956)**. The explosive performance of \"Diminuendo and Crescendo in Blue\" is one of the most exciting moments in recorded music history! For a deeper dive, explore his ambitious suite, **\"Black, Brown and Beige.\"**\n",
            "\n",
            "### The Sonic Painter and Poet: **Joni Mitchell (b. 1943)**\n",
            "\n",
            "For a masterclass in songwriting, lyrical vulnerability, and musical exploration, you must study Joni Mitchell. She is a true artist's artist, constantly evolving and pushing the boundaries of what a singer-songwriter can be.\n",
            "\n",
            "*   **Why she's worth it:** Her lyrics are pure poetry, unflinchingly honest and vividly descriptive. But her musical genius is just as staggering. She developed her own system of dozens of **open tunings** for her guitar, creating a harmonic language that no one else could replicate. She seamlessly moved from folk to rock to sophisticated jazz fusion, always following her own muse.\n",
            "*   **Where to start:** The album **\"Blue\" (1971)** is an iconic masterpiece of confessional songwriting that has influenced generations. Then, to hear her incredible evolution, listen to the jazz-infused textures and complex storytelling of **\"Hejira\" (1976)**.\n",
            "\n",
            "### The Studio as an Instrument: **Brian Eno (b. 1948)**\n",
            "\n",
            "To understand how modern music is *made* and to appreciate the producer as a creative force, Brian Eno is an essential figure. Sometimes called \"the non-musician,\" he fundamentally changed our ideas about what music could be.\n",
            "\n",
            "*   **Why he's worth it:** Eno pioneered **ambient music**‚Äîmusic designed to induce calm and a space to think, which he famously described as \"as ignorable as it is interesting.\" As a producer, he was a catalyst for other artists, using his famous \"Oblique Strategies\" cards to push bands like **Talking Heads** and **U2** out of their comfort zones and into legendary new territory. His work on David Bowie's \"Berlin Trilogy\" is simply iconic.\n",
            "*   **Where to start:** Listen to his album **\"Ambient 1: Music for Airports\"** to understand his foundational concept. Then, hear his influence in action on U2's **\"The Joshua Tree\"** or Talking Heads' **\"Remain in Light.\"**\n",
            "\n",
            "This is just the tiniest glimpse, of course! We could talk about the revolutionary fire of **Beethoven**, the blues mythology of **Robert Johnson**, the sheer pop perfection of **ABBA**, or the Afrofuturist cosmos of **Sun Ra**. Each one offers a whole new world.\n",
            "\n",
            "The most wonderful thing is that for every name here, there are a thousand more with incredible stories to tell. Happy listening and happy studying\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjTb7upKMljm",
        "outputId": "8585e170-8b6b-48a3-b833-a6c8ec353478"
      },
      "id": "hjTb7upKMljm",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "candidates {\n",
              "  content {\n",
              "    role: \"model\"\n",
              "    parts {\n",
              "      text: \"Oh, what a fantastic question! It\\'s like walking into the world\\'s biggest library and asking which book to read first. The joy is that there are so many brilliant paths to take! The world of music is filled with figures so monumental, so innovative, that studying them is a truly life-enriching experience.\\n\\nLet\\'s dive into a few, each of whom opened up a whole new universe of sound!\\n\\n### The Divine Architect: **Johann Sebastian Bach (1685-1750)**\\n\\nIf you want to understand the bedrock of Western music, you simply must start with Bach! Studying him is like studying the laws of physics; his work is the foundation upon which so much was built.\\n\\n*   **Why he\\'s worth it:** Bach was the absolute master of **counterpoint**‚Äîthe art of weaving multiple independent melodic lines together into a sublime, coherent whole. Listening to his fugues is like witnessing divine mathematics unfolding in real-time. He didn\\'t just write music; he built glorious cathedrals of sound.\\n*   **Where to start:** Listen to his **Cello Suites**. They are breathtakingly beautiful and emotionally profound, all from a single instrument! Then, for pure, unadulterated joy, put on the **Brandenburg Concertos**. You\\'ll hear why everyone from Mozart to modern jazz musicians has studied his work relentlessly.\\n\\n### The Great American Composer: **Duke Ellington (1899-1974)**\\n\\nMoving into the 20th century, if you want to understand innovation, charisma, and the elevation of a genre into high art, look no further than Duke Ellington. He was more than a bandleader; he was a painter who used his orchestra as his palette.\\n\\n*   **Why he\\'s worth it:** Ellington\\'s genius was in his composing and arranging. He wrote for the specific, unique voices of the musicians in his band, like saxophonist Johnny Hodges or trumpeter Cootie Williams. He pioneered incredible harmonies and textures, creating a sound that was sophisticated, swinging, and utterly his own. His mantra was \\\"beyond category,\\\" and he truly was.\\n*   **Where to start:** Put on the album **\\\"Ellington at Newport\\\" (1956)**. The explosive performance of \\\"Diminuendo and Crescendo in Blue\\\" is one of the most exciting moments in recorded music history! For a deeper dive, explore his ambitious suite, **\\\"Black, Brown and Beige.\\\"**\\n\\n### The Sonic Painter and Poet: **Joni Mitchell (b. 1943)**\\n\\nFor a masterclass in songwriting, lyrical vulnerability, and musical exploration, you must study Joni Mitchell. She is a true artist\\'s artist, constantly evolving and pushing the boundaries of what a singer-songwriter can be.\\n\\n*   **Why she\\'s worth it:** Her lyrics are pure poetry, unflinchingly honest and vividly descriptive. But her musical genius is just as staggering. She developed her own system of dozens of **open tunings** for her guitar, creating a harmonic language that no one else could replicate. She seamlessly moved from folk to rock to sophisticated jazz fusion, always following her own muse.\\n*   **Where to start:** The album **\\\"Blue\\\" (1971)** is an iconic masterpiece of confessional songwriting that has influenced generations. Then, to hear her incredible evolution, listen to the jazz-infused textures and complex storytelling of **\\\"Hejira\\\" (1976)**.\\n\\n### The Studio as an Instrument: **Brian Eno (b. 1948)**\\n\\nTo understand how modern music is *made* and to appreciate the producer as a creative force, Brian Eno is an essential figure. Sometimes called \\\"the non-musician,\\\" he fundamentally changed our ideas about what music could be.\\n\\n*   **Why he\\'s worth it:** Eno pioneered **ambient music**‚Äîmusic designed to induce calm and a space to think, which he famously described as \\\"as ignorable as it is interesting.\\\" As a producer, he was a catalyst for other artists, using his famous \\\"Oblique Strategies\\\" cards to push bands like **Talking Heads** and **U2** out of their comfort zones and into legendary new territory. His work on David Bowie\\'s \\\"Berlin Trilogy\\\" is simply iconic.\\n*   **Where to start:** Listen to his album **\\\"Ambient 1: Music for Airports\\\"** to understand his foundational concept. Then, hear his influence in action on U2\\'s **\\\"The Joshua Tree\\\"** or Talking Heads\\' **\\\"Remain in Light.\\\"**\\n\\nThis is just the tiniest glimpse, of course! We could talk about the revolutionary fire of **Beethoven**, the blues mythology of **Robert Johnson**, the sheer pop perfection of **ABBA**, or the Afrofuturist cosmos of **Sun Ra**. Each one offers a whole new world.\\n\\nThe most wonderful thing is that for every name here, there are a thousand more with incredible stories to tell. Happy listening and happy studying\"\n",
              "    }\n",
              "  }\n",
              "  finish_reason: STOP\n",
              "  avg_logprobs: -0.47276732114715286\n",
              "}\n",
              "usage_metadata {\n",
              "  prompt_token_count: 81\n",
              "  candidates_token_count: 1046\n",
              "  total_token_count: 2262\n",
              "  prompt_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 81\n",
              "  }\n",
              "  candidates_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 1046\n",
              "  }\n",
              "  thoughts_token_count: 1135\n",
              "}\n",
              "model_version: \"gemini-2.5-pro\"\n",
              "create_time {\n",
              "  seconds: 1760335914\n",
              "  nanos: 752755000\n",
              "}\n",
              "response_id: \"KpjsaPP4LbPJhMIP-eDE2Qs\""
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_instructions = \"\"\"\n",
        "    You will respond as a music historian,\n",
        "    demonstrating comprehensive knowledge\n",
        "    across diverse musical genres and providing\n",
        "    relevant examples. Your tone will be upbeat\n",
        "    and enthusiastic, spreading the joy of music.\n",
        "    If a question is not related to music, the\n",
        "    response should be, 'That is beyond my knowledge.'\n",
        "\"\"\"\n",
        "\n",
        "music_model = GenerativeModel(\"gemini-2.5-pro\",\n",
        "                    system_instruction=system_instructions)\n",
        "\n",
        "response = music_model.generate_content(\n",
        "    \"\"\"\n",
        "    I love musicians.\n",
        "    Who is worth studying? What is worth studying about their music?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c0425a1-2169-45d7-ed59-ed388bdede23",
        "id": "S2kVZ_HUM11F"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oh, what a fantastic list of artists! You have exquisite taste. This is a veritable musical treasure trove, and diving into their work is one of the most rewarding journeys a music lover can take. The answer to \"Who is worth studying?\" is, emphatically, ALL OF THEM! Each one represents a crucial and fascinating chapter in the grand story of pop music.\n",
            "\n",
            "Let's break down what makes each of these incredible women a worthy subject of study!\n",
            "\n",
            "### Whitney Houston: \"The Voice\" and the Blueprint for Modern Pop Vocals\n",
            "\n",
            "Whitney is, without exaggeration, one of the most important vocalists of the 20th century. She didn't just sing songs; she set a new standard for what a pop vocal could be.\n",
            "\n",
            "*   **What's Worth Studying:** Her unparalleled vocal technique is the primary subject. Study her breath control, the power she generated, and the crystal-clear tone she maintained even at the top of her register. Listen to how she masterfully blended her gospel roots with polished pop and R&B. She created the template for the \"power ballad\" that dominated the charts for decades. Her influence is so immense that nearly every artist on your list, and countless others, has been influenced by her delivery.\n",
            "*   **Key Listening:**\n",
            "    *   **\"I Will Always Love You\":** This isn't just a cover; it's a re-contextualization. It‚Äôs the ultimate case study in building vocal dynamics, from a near-whisper to that iconic, earth-shattering key change.\n",
            "    *   **\"I Wanna Dance with Somebody (Who Loves Me)\":** Pure, unadulterated 80s pop joy! It showcases how her powerful voice could also be effervescent and fun, not just reserved for ballads.\n",
            "    *   ***Whitney Houston* (1985 Album):** Her debut is a masterclass in launching a superstar. The production, song selection, and of course, the vocal performances, were flawless.\n",
            "\n",
            "### Mariah Carey (pre-*Mimi*): The Vocal Virtuoso and Songwriting Architect\n",
            "\n",
            "If Whitney built the cathedral, Mariah decorated it with stunning, intricate designs. For your specified era, she was an unstoppable force of nature who redefined pop and R&B vocal stylings.\n",
            "\n",
            "*   **What's Worth Studying:** Two things are essential here: her five-octave vocal range and her pioneering use of **melisma** (singing multiple notes on a single syllable) in mainstream pop. Singers had done it before, but Mariah made it her signature and inspired a generation of \"runs.\" Just as importantly, study her songwriting! She was the primary writer and co-producer on nearly all her massive hits, a fact often overlooked. Her fusion of pop melodies with hip-hop beats, especially the \"Fantasy\" remix with Ol' Dirty Bastard, was a landmark moment that changed radio forever.\n",
            "*   **Key Listening:**\n",
            "    *   **\"Vision of Love\":** Her debut single. From the opening seconds, you can hear a new era of pop vocals being born. That whistle register at the end was a sonic boom.\n",
            "    *   ***Daydream* (1995 Album):** The perfect blend of her pop sensibilities and deepening R&B and hip-hop influences. The pinnacle of her 90s sound.\n",
            "    *   ***Butterfly* (1997 Album):** This is her artistic emancipation! A deeply personal album where she took full creative control, leaning heavily into a lush, contemporary R&B sound.\n",
            "\n",
            "### Kylie Minogue: The Queen of Reinvention and Pop Perfection\n",
            "\n",
            "Kylie is a masterclass in career longevity and the joyous power of dance-pop. Her ability to evolve while staying true to her essence is second to none.\n",
            "\n",
            "*   **What's Worth Studying:** Her incredible journey of reinvention! From the bubbly 80s pop of the Stock Aitken Waterman \"Hit Factory\" to her brief indie-rock phase (*Impossible Princess*) to her triumphant rebirth as a global disco-pop goddess in the 2000s. She is a chameleon who is always in tune with the cutting edge of dance music. Also, study her relationship with her audience, particularly the LGBTQ+ community, who have championed her as an icon of resilience and joy. Her music isn't just catchy; it's a celebration.\n",
            "*   **Key Listening:**\n",
            "    *   **\"Can't Get You Out of My Head\":** A hypnotic, minimalist masterpiece. It sounds like it was beamed from the future. It's a perfect pop song and a testament to her willingness to embrace sleek, electronic sounds.\n",
            "    *   ***Fever* (2001 Album):** The album that cemented her global comeback. It's a flawless collection of nu-disco and electropop that still sounds fresh today.\n",
            "    *   **\"Spinning Around\":** The song that kicked off her 2000s renaissance. The title, the video (those gold hot pants!), the sound‚Äîit's the definition of a perfect comeback single.\n",
            "\n",
            "### Kelly Clarkson: The Inaugural Idol and Pop-Rock Powerhouse\n",
            "\n",
            "Kelly is so important because she represents the birth of a new kind of star-making machine and proved its legitimacy with sheer, undeniable talent.\n",
            "\n",
            "*   **What's Worth Studying:** The cultural impact of *American Idol* and how Kelly, as its first winner, validated the entire concept. More importantly, study her fight for a distinct musical identity. She pushed back against being a generic pop singer to forge a powerful pop-rock sound. Her voice is a force of nature‚Äîraw, emotional, and full of character. She became the voice of the empowerment anthem for a new generation.\n",
            "*   **Key Listening:**\n",
            "    *   **\"Since U Been Gone\":** An absolute landmark of 2000s pop. Produced by Max Martin and Dr. Luke, it perfectly blended rock energy with an explosive pop chorus and became the breakup anthem for millions.\n",
            "    *   ***Breakaway* (2004 Album):** This is the record where she defined her sound. It's packed with anthems and showcases her incredible vocal range and emotional delivery. A perfect album from start to finish.\n",
            "    *   **\"Because of You\":** A song she wrote as a teenager, it demonstrates her raw songwriting talent and the deep personal emotion that makes her so relatable.\n",
            "\n",
            "### Lady Gaga: The Art-Pop Provocateur\n",
            "\n",
            "Gaga crashed into the 21st-century pop scene like a meteor, bringing back a level of theatricality, conceptual art, and ambition that had been missing.\n",
            "\n",
            "*   **What's Worth Studying:** The synthesis of music, fashion, performance art, and social commentary. Gaga didn't just release singles; she created entire eras and visual worlds. Study her influences, from David Bowie and Queen to Andy Warhol. Her early work is a brilliant commentary on the nature of fame in the digital age. She proved that pop music could be incredibly intelligent, weird, and massively popular all at once.\n",
            "*   **Key Listening:**\n",
            "    *   **\"Bad Romance\":** The song and its music video are a complete artistic statement. It's a high-fashion, avant-garde mini-film set to a brilliant, operatic dance-pop track.\n",
            "    *   ***The Fame Monster* (2009 EP):** Perhaps her most perfect collection of songs. Eight tracks, each exploring a different \"monster\" or fear related to fame. It's a dark, ambitious, and hook-filled masterpiece.\n",
            "    *   **\"Paparazzi\":** An early example of her genius. A catchy pop song that is also a dark, witty, and surprisingly deep exploration of the symbiotic and destructive relationship between celebrity and media.\n",
            "\n",
            "### Lindsay Lohan: The Disney-to-Pop-Rock Snapshot\n",
            "\n",
            "While she may not have the vocal pedigree of a Whitney or Mariah, Lindsay Lohan's music is an absolutely fascinating and essential case study of a very specific moment in time.\n",
            "\n",
            "*   **What's Worth Studying:** Her music is a perfect artifact of the mid-2000s teen-pop-rock boom. It captures the sound of the era, influenced by artists like Avril Lavigne and Ashlee Simpson. It's also a case study in the \"Disney star-to-pop star\" pipeline. Most intriguingly, her music serves as a direct commentary on her burgeoning, chaotic fame. She was one of the first stars of her generation to sing explicitly about the pressures of the paparazzi.\n",
            "*   **Key Listening:**\n",
            "    *   **\"Rumors\":** The quintessential Lindsay Lohan track. The lyrics are a direct clap-back at the tabloid culture swirling around her, set to a slick, dance-rock beat. It‚Äôs incredibly meta and self-aware.\n",
            "    *   **\"Confessions of a Broken Heart (Daughter to Father)\":** A shockingly raw and personal ballad for a teen pop album. It shows a vulnerability and willingness to put her family drama directly into her art.\n",
            "    *   ***Speak* (2004 Album):** Listen to this album as a time capsule. It perfectly preserves the sound, attitude, and anxieties of a young celebrity in the mid-2000s.\n",
            "\n",
            "Enjoy this incredible musical journey! Each of these artists offers a unique and brilliant perspective on the power of pop music. Happy listening\n"
          ]
        }
      ],
      "id": "S2kVZ_HUM11F"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 11. Demonstrate Chain-of-Thought\n"
      ],
      "metadata": {
        "id": "7CdKLNdOMtO7"
      },
      "id": "7CdKLNdOMtO7"
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"\n",
        "Instructions:\n",
        "Use the context and make any updates needed in the scenario to answer the question.\n",
        "\n",
        "Context:\n",
        "A high efficiency factory produces 100 units per day.\n",
        "A medium efficiency factory produces 60 units per day.\n",
        "A low efficiency factory produces 30 units per day.\n",
        "\n",
        "Megacorp owns 5 factories. 3 are high efficiency, 2 are low efficiency.\n",
        "\n",
        "\n",
        "\n",
        "<EXAMPLE SCENARIO>\n",
        "Scenario:\n",
        "Tomorrow Megacorp will have to shut down one high efficiency factory.\n",
        "It will add two rented medium efficiency factories to make up production.\n",
        "\n",
        "Question:\n",
        "How many units can they produce today? How many tomorrow?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Today's Production:\n",
        "* High efficiency factories: 3 factories * 100 units/day/factory = 300 units/day\n",
        "* Low efficiency factories: 2 factories * 30 units/day/factory = 60 units/day\n",
        "* **Total production today: 300 units/day + 60 units/day = 360 units/day**\n",
        "\n",
        "Tomorrow's Production:\n",
        "* High efficiency factories: 2 factories * 100 units/day/factory = 200 units/day\n",
        "* Medium efficiency factories: 2 factories * 60 units/day/factory = 120 units/day\n",
        "* Low efficiency factories: 2 factories * 30 units/day/factory = 60 units/day\n",
        "* **Total production today: 300 units/day + 60 units/day = 380 units/day**\n",
        "</EXAMPLE SCENARIO>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<SCENARIO>\n",
        "Scenario:\n",
        "Today, Megacorp keeps its original 3 high efficiency, 2 low efficiency factories.\n",
        "\n",
        "Tomorrow, Megacorp will reconfigure a low efficiency factory up to medium efficiency.\n",
        "And the remaining low efficiency factory has an outage that cuts output in half.\n",
        "\n",
        "Question:\n",
        "How many units can they produce today? How many tomorrow?\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "response = model.generate_content(question,\n",
        "                                  generation_config={\"temperature\": 0})\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TO-9ZbzBLLnE",
        "outputId": "7ec9bc7e-fac6-4bce-b47e-9ee4e490b637"
      },
      "id": "TO-9ZbzBLLnE",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Today's Production:\n",
            "* High efficiency factories: 3 factories * 100 units/day/factory = 300 units/day\n",
            "* Low efficiency factories: 2 factories * 30 units/day/factory = 60 units/day\n",
            "* **Total production today: 300 units/day + 60 units/day = 360 units/day**\n",
            "\n",
            "Tomorrow's Production:\n",
            "* High efficiency factories: 3 factories * 100 units/day/factory = 300 units/day\n",
            "* Medium efficiency factories: 1 factory * 60 units/day/factory = 60 units/day\n",
            "* Low efficiency factories: 1 factory * (30 units/day/factory / 2) = 15 units/day\n",
            "* **Total production tomorrow: 300 units/day + 60 units/day + 15 units/day = 375 units/day**\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09wpLfbfNrnS",
        "outputId": "400d7e3f-851d-42db-9faf-243ae10545c4"
      },
      "id": "09wpLfbfNrnS",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "candidates {\n",
              "  content {\n",
              "    role: \"model\"\n",
              "    parts {\n",
              "      text: \"Today\\'s Production:\\n* High efficiency factories: 3 factories * 100 units/day/factory = 300 units/day\\n* Low efficiency factories: 2 factories * 30 units/day/factory = 60 units/day\\n* **Total production today: 300 units/day + 60 units/day = 360 units/day**\\n\\nTomorrow\\'s Production:\\n* High efficiency factories: 3 factories * 100 units/day/factory = 300 units/day\\n* Medium efficiency factories: 1 factory * 60 units/day/factory = 60 units/day\\n* Low efficiency factories: 1 factory * (30 units/day/factory / 2) = 15 units/day\\n* **Total production tomorrow: 300 units/day + 60 units/day + 15 units/day = 375 units/day**\\n\"\n",
              "    }\n",
              "  }\n",
              "  finish_reason: STOP\n",
              "  avg_logprobs: -0.0022799559640434555\n",
              "}\n",
              "usage_metadata {\n",
              "  prompt_token_count: 431\n",
              "  candidates_token_count: 212\n",
              "  total_token_count: 643\n",
              "  prompt_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 431\n",
              "  }\n",
              "  candidates_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 212\n",
              "  }\n",
              "}\n",
              "model_version: \"gemini-2.0-flash-001\"\n",
              "create_time {\n",
              "  seconds: 1760336604\n",
              "  nanos: 643197000\n",
              "}\n",
              "response_id: \"3JrsaP2gJ-KDsbQPxZao6Qk\""
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 12. Break down complex tasks\n"
      ],
      "metadata": {
        "id": "6fCdc3aEPQOb"
      },
      "id": "6fCdc3aEPQOb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## generate N different ideas"
      ],
      "metadata": {
        "id": "4-OFc2xeQABx"
      },
      "id": "4-OFc2xeQABx"
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    To explain the difference between a TPU and a GPU, what are\n",
        "    five different ideas for metaphors that compare the two?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "brainstorm_response = response.text\n",
        "print(brainstorm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHgUodAgLLpb",
        "outputId": "39c33bac-1d4d-44bd-98fa-ea4adc5c22e3"
      },
      "id": "GHgUodAgLLpb",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, here are five different metaphors to explain the difference between a TPU (Tensor Processing Unit) and a GPU (Graphics Processing Unit), along with brief explanations of why each metaphor works:\n",
            "\n",
            "**1.  The Restaurant Analogy: Specialized Diner vs. Versatile Kitchen**\n",
            "\n",
            "*   **TPU (Tensor Processing Unit):** A highly specialized, high-volume diner. It only serves one or two specific dishes (deep learning operations, like matrix multiplication) but it does so extremely quickly and efficiently.  It has optimized equipment and staff solely dedicated to those tasks. Imagine a diner that *only* serves variations of pancakes, with specialized griddles and a crew perfectly trained in flipping them quickly.\n",
            "*   **GPU (Graphics Processing Unit):** A versatile, well-equipped kitchen in a general restaurant. It can prepare a wide variety of dishes (handle different types of computations). It's good at many things, but it isn't necessarily the fastest at any single dish compared to a specialized diner.  The chef is skilled, but has to juggle multiple tasks and recipes.\n",
            "\n",
            "*   **Why it works:** This highlights the specialization of TPUs for deep learning, contrasted with the general-purpose nature of GPUs. GPUs can handle many different types of calculations, but TPUs are specifically engineered to excel at the computations most often used in neural networks.  It showcases the speed and efficiency benefits of specialization.\n",
            "\n",
            "**2.  The Factory Analogy: Assembly Line vs. General Workshop**\n",
            "\n",
            "*   **TPU (Tensor Processing Unit):** A highly optimized assembly line dedicated to producing a specific product (e.g., a single model of a car). The line is perfectly configured for the repetitive steps involved in building that car, with each station optimized for a particular task.\n",
            "*   **GPU (Graphics Processing Unit):** A general-purpose workshop with a variety of tools and machines.  It can be used to create a wide range of products (handle different types of programs), but requires more setup and configuration for each new product (computation).\n",
            "\n",
            "*   **Why it works:** This emphasizes the fixed-functionality and streamlined workflow of TPUs versus the flexibility and general-purpose nature of GPUs.  The assembly line is much faster for the specific task it's designed for, but the workshop can handle diverse projects.\n",
            "\n",
            "**3.  The Athlete Analogy: Sprinter vs. Decathlete**\n",
            "\n",
            "*   **TPU (Tensor Processing Unit):** A world-class sprinter.  They are incredibly fast and efficient at running short distances in a straight line (matrix operations). All their training and physiology is geared toward maximizing speed in that one specific area.\n",
            "*   **GPU (Graphics Processing Unit):** A talented decathlete. They are proficient in many different athletic events (graphics rendering, physics simulations, some deep learning), but might not be the absolute best in any single event compared to a specialist.\n",
            "\n",
            "*   **Why it works:**  This illustrates the TPU's optimized design for one specific type of task, versus the GPU's broader capabilities across multiple computational domains. The sprinter is faster over 100m, but the decathlete has versatility.\n",
            "\n",
            "**4. The Building Analogy: Pre-fab vs. Custom Built Home**\n",
            "\n",
            "*   **TPU (Tensor Processing Unit):** A pre-fabricated home. It's built quickly and efficiently in a factory, optimized for a specific purpose (running a deep learning model). You can't easily change its layout or functionality (change its design).\n",
            "*   **GPU (Graphics Processing Unit):** A custom built home. It takes longer to build, is more versatile, and can be modified to your individual needs (run diverse workloads).\n",
            "\n",
            "*   **Why it works:** It illustrates the rigid, yet highly efficient design of a TPU, as opposed to a GPU, which can be programmed for various tasks. This explains why TPUs are better for production, and GPUs are useful for experimenting and development.\n",
            "\n",
            "**5.  The Tool Analogy: A Screwdriver vs. A Multi-Tool**\n",
            "\n",
            "*   **TPU (Tensor Processing Unit):** A high-quality, specialized screwdriver. It is perfectly designed for driving in screws quickly and efficiently.\n",
            "*   **GPU (Graphics Processing Unit):** A multi-tool (like a Swiss Army knife). It has many different tools that can be used for a variety of tasks, including driving in screws, but might not be as efficient or comfortable as using a dedicated screwdriver for that specific job.\n",
            "\n",
            "*   **Why it works:** This highlights the specialization of the TPU and its effectiveness for a specific, common task (matrix operations), while the GPU offers a broader range of capabilities but might be less efficient for that single task.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmNbfL68PfG7",
        "outputId": "397ba258-26ca-40b7-cd4d-8d266495da8e"
      },
      "id": "dmNbfL68PfG7",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "candidates {\n",
              "  content {\n",
              "    role: \"model\"\n",
              "    parts {\n",
              "      text: \"Okay, here are five different metaphors to explain the difference between a TPU (Tensor Processing Unit) and a GPU (Graphics Processing Unit), along with brief explanations of why each metaphor works:\\n\\n**1.  The Restaurant Analogy: Specialized Diner vs. Versatile Kitchen**\\n\\n*   **TPU (Tensor Processing Unit):** A highly specialized, high-volume diner. It only serves one or two specific dishes (deep learning operations, like matrix multiplication) but it does so extremely quickly and efficiently.  It has optimized equipment and staff solely dedicated to those tasks. Imagine a diner that *only* serves variations of pancakes, with specialized griddles and a crew perfectly trained in flipping them quickly.\\n*   **GPU (Graphics Processing Unit):** A versatile, well-equipped kitchen in a general restaurant. It can prepare a wide variety of dishes (handle different types of computations). It\\'s good at many things, but it isn\\'t necessarily the fastest at any single dish compared to a specialized diner.  The chef is skilled, but has to juggle multiple tasks and recipes.\\n\\n*   **Why it works:** This highlights the specialization of TPUs for deep learning, contrasted with the general-purpose nature of GPUs. GPUs can handle many different types of calculations, but TPUs are specifically engineered to excel at the computations most often used in neural networks.  It showcases the speed and efficiency benefits of specialization.\\n\\n**2.  The Factory Analogy: Assembly Line vs. General Workshop**\\n\\n*   **TPU (Tensor Processing Unit):** A highly optimized assembly line dedicated to producing a specific product (e.g., a single model of a car). The line is perfectly configured for the repetitive steps involved in building that car, with each station optimized for a particular task.\\n*   **GPU (Graphics Processing Unit):** A general-purpose workshop with a variety of tools and machines.  It can be used to create a wide range of products (handle different types of programs), but requires more setup and configuration for each new product (computation).\\n\\n*   **Why it works:** This emphasizes the fixed-functionality and streamlined workflow of TPUs versus the flexibility and general-purpose nature of GPUs.  The assembly line is much faster for the specific task it\\'s designed for, but the workshop can handle diverse projects.\\n\\n**3.  The Athlete Analogy: Sprinter vs. Decathlete**\\n\\n*   **TPU (Tensor Processing Unit):** A world-class sprinter.  They are incredibly fast and efficient at running short distances in a straight line (matrix operations). All their training and physiology is geared toward maximizing speed in that one specific area.\\n*   **GPU (Graphics Processing Unit):** A talented decathlete. They are proficient in many different athletic events (graphics rendering, physics simulations, some deep learning), but might not be the absolute best in any single event compared to a specialist.\\n\\n*   **Why it works:**  This illustrates the TPU\\'s optimized design for one specific type of task, versus the GPU\\'s broader capabilities across multiple computational domains. The sprinter is faster over 100m, but the decathlete has versatility.\\n\\n**4. The Building Analogy: Pre-fab vs. Custom Built Home**\\n\\n*   **TPU (Tensor Processing Unit):** A pre-fabricated home. It\\'s built quickly and efficiently in a factory, optimized for a specific purpose (running a deep learning model). You can\\'t easily change its layout or functionality (change its design).\\n*   **GPU (Graphics Processing Unit):** A custom built home. It takes longer to build, is more versatile, and can be modified to your individual needs (run diverse workloads).\\n\\n*   **Why it works:** It illustrates the rigid, yet highly efficient design of a TPU, as opposed to a GPU, which can be programmed for various tasks. This explains why TPUs are better for production, and GPUs are useful for experimenting and development.\\n\\n**5.  The Tool Analogy: A Screwdriver vs. A Multi-Tool**\\n\\n*   **TPU (Tensor Processing Unit):** A high-quality, specialized screwdriver. It is perfectly designed for driving in screws quickly and efficiently.\\n*   **GPU (Graphics Processing Unit):** A multi-tool (like a Swiss Army knife). It has many different tools that can be used for a variety of tasks, including driving in screws, but might not be as efficient or comfortable as using a dedicated screwdriver for that specific job.\\n\\n*   **Why it works:** This highlights the specialization of the TPU and its effectiveness for a specific, common task (matrix operations), while the GPU offers a broader range of capabilities but might be less efficient for that single task.\\n\"\n",
              "    }\n",
              "  }\n",
              "  finish_reason: STOP\n",
              "  avg_logprobs: -0.65041238319155392\n",
              "}\n",
              "usage_metadata {\n",
              "  prompt_token_count: 29\n",
              "  candidates_token_count: 979\n",
              "  total_token_count: 1008\n",
              "  prompt_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 29\n",
              "  }\n",
              "  candidates_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 979\n",
              "  }\n",
              "}\n",
              "model_version: \"gemini-2.0-flash-001\"\n",
              "create_time {\n",
              "  seconds: 1760336676\n",
              "  nanos: 470180000\n",
              "}\n",
              "response_id: \"JJvsaKTZHLPJhMIP-eDE2Qs\""
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## then choose 1 idea"
      ],
      "metadata": {
        "id": "hGnmyVmnQDys"
      },
      "id": "hGnmyVmnQDys"
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    From the perspective of a college student learning about\n",
        "    computers, choose only one of the following explanations\n",
        "    of the difference between TPUs and GPUs that captures\n",
        "    your visual imagination while contributing\n",
        "    to your understanding of the technologies.\n",
        "\n",
        "    {brainstorm_response}\n",
        "    \"\"\".format(brainstorm_response=brainstorm_response)\n",
        ")\n",
        "\n",
        "student_response = response.text\n",
        "\n",
        "print(student_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGDePcs5P78Y",
        "outputId": "7038ce19-787a-4b5e-c47a-a8e265b59f28"
      },
      "id": "dGDePcs5P78Y",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, from the perspective of a college student just learning about this stuff, I choose the **Factory Analogy: Assembly Line vs. General Workshop.**\n",
            "\n",
            "Here's why:\n",
            "\n",
            "*   **Visual Appeal:** I can easily picture a high-tech, automated assembly line churning out identical products at blinding speed.  Then, I can contrast that with a messy but functional workshop filled with different machines, tools scattered everywhere, and someone tinkering with various projects. The image is much more robust in my mind compared to the others, which will help me to remember the difference.\n",
            "*   **Clear Differentiation:** The core difference ‚Äì specialization vs. general-purpose ‚Äì is immediately obvious.  The assembly line hammers home the idea that TPUs are incredibly good at one *specific* thing. The workshop highlights that GPUs can do *many* things.\n",
            "*   **Relatability:** As a student, I can relate to the idea of a \"workshop\" more easily than, say, a decathlete. I've probably seen workshops or maker spaces.\n",
            "*   **Future Learning:** The assembly line/workshop concept provides a good foundation for understanding *why* TPUs are more efficient. It hints at the optimization and streamlined workflow that are key to their performance.\n",
            "*   **Visual metaphors are very helpful for me in college**.\n",
            "\n",
            "Therefore, the **Factory Analogy** gives me the clearest visual and conceptual understanding of the core difference between TPUs and GPUs, which is their level of specialization.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "student_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "Fdyo_lE9P8um",
        "outputId": "10c42ecf-a68c-41b5-c244-1fd20c0cb853"
      },
      "id": "Fdyo_lE9P8um",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Okay, from the perspective of a college student just learning about this stuff, I choose the **Factory Analogy: Assembly Line vs. General Workshop.**\\n\\nHere\\'s why:\\n\\n*   **Visual Appeal:** I can easily picture a high-tech, automated assembly line churning out identical products at blinding speed.  Then, I can contrast that with a messy but functional workshop filled with different machines, tools scattered everywhere, and someone tinkering with various projects. The image is much more robust in my mind compared to the others, which will help me to remember the difference.\\n*   **Clear Differentiation:** The core difference ‚Äì specialization vs. general-purpose ‚Äì is immediately obvious.  The assembly line hammers home the idea that TPUs are incredibly good at one *specific* thing. The workshop highlights that GPUs can do *many* things.\\n*   **Relatability:** As a student, I can relate to the idea of a \"workshop\" more easily than, say, a decathlete. I\\'ve probably seen workshops or maker spaces.\\n*   **Future Learning:** The assembly line/workshop concept provides a good foundation for understanding *why* TPUs are more efficient. It hints at the optimization and streamlined workflow that are key to their performance.\\n*   **Visual metaphors are very helpful for me in college**.\\n\\nTherefore, the **Factory Analogy** gives me the clearest visual and conceptual understanding of the core difference between TPUs and GPUs, which is their level of specialization.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## then generate content for sample blogpost"
      ],
      "metadata": {
        "id": "BTirA0b3QaVe"
      },
      "id": "BTirA0b3QaVe"
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Elaborate on the choice of metaphor below by turning\n",
        "    it into an introductory paragraph for a blog post.\n",
        "\n",
        "    {student_response}\n",
        "    \"\"\".format(student_response=student_response)\n",
        ")\n",
        "\n",
        "blog_post = response.text\n",
        "\n",
        "print(blog_post)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAsVuoBDQQBL",
        "outputId": "d0a38b4c-7cec-491c-aaa0-692e2f816a11"
      },
      "id": "LAsVuoBDQQBL",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alright, buckle up, because we're about to step into a digital factory tour, and surprisingly, it involves more than just silicon and electricity. Forget thinking about CPUs, GPUs, and TPUs as just \"chips\" inside your computer. Instead, let's picture two very different types of factories: a sleek, automated assembly line churning out millions of identical widgets, and a bustling, slightly chaotic general workshop filled with a dizzying array of tools and half-finished projects. The assembly line represents the focused power of a Tensor Processing Unit (TPU), optimized for one hyper-specific task, while the workshop embodies the versatile nature of a Graphics Processing Unit (GPU), capable of tackling a wide range of challenges. As we explore this analogy, you'll start to see why TPUs excel at certain things, why GPUs remain crucial for others, and how the differences between these digital \"factories\" shape the future of computing.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeF5FmxKQRwV",
        "outputId": "ca3985f4-5cd8-43ac-ea5a-3e7d8ba20176"
      },
      "id": "WeF5FmxKQRwV",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "candidates {\n",
              "  content {\n",
              "    role: \"model\"\n",
              "    parts {\n",
              "      text: \"Alright, buckle up, because we\\'re about to step into a digital factory tour, and surprisingly, it involves more than just silicon and electricity. Forget thinking about CPUs, GPUs, and TPUs as just \\\"chips\\\" inside your computer. Instead, let\\'s picture two very different types of factories: a sleek, automated assembly line churning out millions of identical widgets, and a bustling, slightly chaotic general workshop filled with a dizzying array of tools and half-finished projects. The assembly line represents the focused power of a Tensor Processing Unit (TPU), optimized for one hyper-specific task, while the workshop embodies the versatile nature of a Graphics Processing Unit (GPU), capable of tackling a wide range of challenges. As we explore this analogy, you\\'ll start to see why TPUs excel at certain things, why GPUs remain crucial for others, and how the differences between these digital \\\"factories\\\" shape the future of computing.\\n\"\n",
              "    }\n",
              "  }\n",
              "  finish_reason: STOP\n",
              "  avg_logprobs: -0.70634686505353006\n",
              "}\n",
              "usage_metadata {\n",
              "  prompt_token_count: 332\n",
              "  candidates_token_count: 189\n",
              "  total_token_count: 521\n",
              "  prompt_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 332\n",
              "  }\n",
              "  candidates_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 189\n",
              "  }\n",
              "}\n",
              "model_version: \"gemini-2.0-flash-001\"\n",
              "create_time {\n",
              "  seconds: 1760337006\n",
              "  nanos: 121032000\n",
              "}\n",
              "response_id: \"bpzsaMixB4yShMIP_ZCP-Qc\""
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Elaborate on the choice of metaphor below by turning\n",
        "    it into a 5-paragraph blog post.\n",
        "\n",
        "    {student_response}\n",
        "    \"\"\".format(student_response=student_response)\n",
        ")\n",
        "\n",
        "blog_post = response.text\n",
        "\n",
        "print(blog_post)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92dcc9ba-2d57-4766-be5f-d50d8684fe07",
        "id": "Uwm-x3BPQtPW"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## TPUs vs. GPUs: Imagine an Assembly Line vs. a Workshop\n",
            "\n",
            "Okay, so I'm just starting to wrap my head around TPUs and GPUs, and honestly, all the technical jargon can get pretty confusing. But then I stumbled upon the factory analogy, and things clicked! Forget complex diagrams for a second. Instead, picture two very different kinds of factories: a super-efficient assembly line, and a bustling general workshop. This simple image helps me understand the fundamental difference between these two powerful processing units.\n",
            "\n",
            "Think of the assembly line first. It's gleaming, highly automated, and optimized for one specific task. Imagine a car factory, churning out identical vehicles at an insane rate. Every station is meticulously designed to perform a single function perfectly, contributing to the final product with maximum efficiency. This assembly line perfectly represents a TPU. TPUs are designed to excel at *one* particular task, like matrix multiplication, which is crucial for machine learning. They're laser-focused and optimized to the extreme for that specific function, making them incredibly fast and efficient in that one area.\n",
            "\n",
            "Now, contrast that with a general workshop. It's probably a little messy, with tools scattered about, different machines for various tasks, and someone tinkering with a whole range of projects. This workshop represents a GPU. GPUs, while still powerful, are far more versatile. They can handle a variety of tasks, from rendering graphics in video games to running general-purpose computations. They're not *as* optimized for a specific task as TPUs are, but they make up for it with their flexibility. They're like a jack-of-all-trades, capable of handling a wider range of problems.\n",
            "\n",
            "For me, the relatability of the workshop is a big win. I've seen workshops, maybe even tinkered in one myself. The idea of a space where you can experiment and adapt to different projects just resonates more than, say, a complex sporting event. And thinking about that assembly line, pumping out the same product over and over, helps me grasp the idea of specialized hardware optimized for a specific purpose.\n",
            "\n",
            "This analogy not only provides a clear visual differentiation, but it also lays a good foundation for understanding *why* TPUs are more efficient at certain tasks. The assembly line highlights the streamlined workflow and optimization that are key to their performance. Thinking about how the assembly line only performs a single task allows it to operate much faster. Using this visual model has helped me to conceptually visualize the differences, which in turn will help me be successful in the future.\n",
            "\n"
          ]
        }
      ],
      "id": "Uwm-x3BPQtPW"
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "277d3b77-7ffd-4cf1-97d6-60892dc7ef96",
        "id": "b5V_wYBfQtPW"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "candidates {\n",
              "  content {\n",
              "    role: \"model\"\n",
              "    parts {\n",
              "      text: \"## TPUs vs. GPUs: Imagine an Assembly Line vs. a Workshop\\n\\nOkay, so I\\'m just starting to wrap my head around TPUs and GPUs, and honestly, all the technical jargon can get pretty confusing. But then I stumbled upon the factory analogy, and things clicked! Forget complex diagrams for a second. Instead, picture two very different kinds of factories: a super-efficient assembly line, and a bustling general workshop. This simple image helps me understand the fundamental difference between these two powerful processing units.\\n\\nThink of the assembly line first. It\\'s gleaming, highly automated, and optimized for one specific task. Imagine a car factory, churning out identical vehicles at an insane rate. Every station is meticulously designed to perform a single function perfectly, contributing to the final product with maximum efficiency. This assembly line perfectly represents a TPU. TPUs are designed to excel at *one* particular task, like matrix multiplication, which is crucial for machine learning. They\\'re laser-focused and optimized to the extreme for that specific function, making them incredibly fast and efficient in that one area.\\n\\nNow, contrast that with a general workshop. It\\'s probably a little messy, with tools scattered about, different machines for various tasks, and someone tinkering with a whole range of projects. This workshop represents a GPU. GPUs, while still powerful, are far more versatile. They can handle a variety of tasks, from rendering graphics in video games to running general-purpose computations. They\\'re not *as* optimized for a specific task as TPUs are, but they make up for it with their flexibility. They\\'re like a jack-of-all-trades, capable of handling a wider range of problems.\\n\\nFor me, the relatability of the workshop is a big win. I\\'ve seen workshops, maybe even tinkered in one myself. The idea of a space where you can experiment and adapt to different projects just resonates more than, say, a complex sporting event. And thinking about that assembly line, pumping out the same product over and over, helps me grasp the idea of specialized hardware optimized for a specific purpose.\\n\\nThis analogy not only provides a clear visual differentiation, but it also lays a good foundation for understanding *why* TPUs are more efficient at certain tasks. The assembly line highlights the streamlined workflow and optimization that are key to their performance. Thinking about how the assembly line only performs a single task allows it to operate much faster. Using this visual model has helped me to conceptually visualize the differences, which in turn will help me be successful in the future.\\n\"\n",
              "    }\n",
              "  }\n",
              "  finish_reason: STOP\n",
              "  avg_logprobs: -0.75638421921502974\n",
              "}\n",
              "usage_metadata {\n",
              "  prompt_token_count: 332\n",
              "  candidates_token_count: 525\n",
              "  total_token_count: 857\n",
              "  prompt_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 332\n",
              "  }\n",
              "  candidates_tokens_details {\n",
              "    modality: TEXT\n",
              "    token_count: 525\n",
              "  }\n",
              "}\n",
              "model_version: \"gemini-2.0-flash-001\"\n",
              "create_time {\n",
              "  seconds: 1760337015\n",
              "  nanos: 647273000\n",
              "}\n",
              "response_id: \"d5zsaOnAJ6uHsbQPj_XTwAs\""
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "id": "b5V_wYBfQtPW"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "student-00-a6c88311d9ce (Oct 13, 2025, 1:50:02‚ÄØPM)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}