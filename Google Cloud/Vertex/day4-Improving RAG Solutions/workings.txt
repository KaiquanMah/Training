Develop a Retrieval Augmented Generation (RAG) system using embeddings and Large Language Models (LLMs). We'll explore common issues and techniques to improve the overall recall and accuracy of results. 
Our toolkit includes Google Cloud technologies such as Vertex AI Workbench, the latest Gemini LLM model, and the Google Cloud Embeddings APIs.




Objectives
- Set up a RAG system.
- Explore the distribution of embeddings with user queries.
- Implement query augmentation.
- Perform result re-ranking.
- Utilize Embedding Adapters.


======

Task 4. Query augmentation

Flow 1
orig qn
>>ask LLM to provide example ans
>>concat orig qn + LLM example ans
>>from concat_qn_ans find 5 nearest docs
>>concat 5 nearest docs AS 'context'
expected>> use 'context' to ans orig qn
actual>> lab didn't use 'context' to ans orig qn

Flow 2
orig qn
>>generate 5 new sub-qns
>>concat orig qn + 5 sub-qns
>>find 5 nearest docs for each qn/sub-qn
>>concat 5 nearest docs for each qn/sub-qn AS 'context'
expected>> use 'context' to ans orig qn AND 5 sub-qns
actual>>lab didn't use 'context' to ans orig qn AND 5 sub-qns





======


Task 5. Re-ranking results

From Task 4 Flow 2
Continue from [>>concat 5 nearest docs for each qn/sub-qn AS 'context']
>>convert from list to 'set of unique docs'
>>create (orig query, doc) pairs - 1 pair per unique doc retrieved
>>put into sentence_transformers.CrossEncoder to predict similarity score per pair
>>rank similarity scores in descending order (less neg -> ie more similar/relevant)
expected AND actual>> use re-ranked top 5 similar docs to ans orig qn



======

Task 6. Embedding Adapters
- Finetune embedding model on your dataset

Ideally>> Own dataset
Lab>> ask LLM generate 15 qns (dataset)
>>from 15 qns find 10 nearest docs -> get their 'documents' text AND 'embeddings'

side-task
>>create 'evaluate_results' fn to ask LLM to evaluate relevance btw 'qn' (for each of the 15 qns) vs 'nearest doc' (for each of the 10 nearest docs)
if relevant => output 1
if not relevant => output -1
[dont use 1st, use below later]

THEN
>> run embedding fn on 15-llm-generated-qns
>> now we have 15-llm-generated-qns embeddings + 10-nearest-docs text + 10-nearest-docs embeddings
>>create adapter_query_embeddings AND adapter_doc_embeddings AND adapter_labels SEPARATELY AT 1 GO/'for loop'
containing the query/doc embeddings AND evaluate_results_class (-1, 1)
(from the pair/combo of 15 qns x 10 nearest docs = 150 combos)

THEN
>> turn the 3 (adapter_query_embeddings AND adapter_doc_embeddings AND adapter_labels)
into separate PyTorch Tensors
>> create 'TensorDataset' containing ALL 3 items
>>..... initialise AND train adapter_matrix





Reflection
- Lab needs to be updated to retrieve the right PDF URL
- More time could be given to explore the various techniques and what is happening at each stage
- Task 4 can be better explained and help us understand how to use the 2 flows to answer original question, and original question + 5 sub-questions
- Task 6 could be better explained and help us understand how to use the tuned model on sample questions

