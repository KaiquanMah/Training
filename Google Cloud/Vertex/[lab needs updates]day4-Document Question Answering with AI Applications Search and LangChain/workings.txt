- ask and answer questions about your data by combining a AI Applications Search engine with LLMs. In particular, we focus on querying 'unstructured' data such as PDFs and HTML files.
- These patterns are useful if you have a AI Applications Search Engine pointed at a store of documents, such as a Google Cloud Storage bucket containing PDFs.


Use Gemini
- Ask a question about uploaded documents, receive answers with citations.
- Engage in follow-up questions for deeper understanding.
- Have the flexibility to customize prompts for tailored responses.



============================================

Task 1. Open Python Notebook and Install Packages

In your Google Cloud project, navigate to Vertex AI Workbench. In the top search bar of the Google Cloud console, enter Vertex AI Workbench, and click on the first result.

Under Instances, click on Open JupyterLab for generative-ai-jupyterlab instance.

The JupyterLab will run in a new tab.
On the Launcher, under Notebook, click on Python 3 to open a new python notebook.

Install the required packages by running the following command in the first cell of the notebook. Either click the play play button at the top or click SHIFT+ENTER keys on your keyboard to execute the cell.
```
! pip install -q --user google-cloud-aiplatform google-cloud-discoveryengine langchain-google-vertexai langchain-google-community
```
Output:

Dependencies
https://cdn.qwiklabs.com/KxtAg514s5URRM6YmLSFMw2JEL8Fj5o7CZm9jwn7e40%3D
Note: Rerun the same cell if you encounter a dependency resolver error when installing the packages.


To use the newly installed packages in this Jupyter runtime, it is recommended to restart the runtime. Restart the kernel by running the below code snippet or clicking the refresh button restart kernel at the top, followed by clicking Restart button.

# Restart kernel after packages are installed so that your environment can access the new packages
import IPython
import time

app = IPython.Application.instance()
app.kernel.do_shutdown(True)


Output:
kernel restart image
https://cdn.qwiklabs.com/y5Os7%2Fcsu6MIeNA8RQ2SaUUEjSShNc9cmhhFIRVUwoQ%3D
After the restart is complete, click Ok on the prompt to continue.














============================================


Task 2. Setup project information and populate the dataset into a data store
In this section, we will setup project environments and import the data in a pre-created data store.

Setup project info and initializing the vertex ai.
```
PROJECT_ID = "qwiklabs-gcp-01-aa7f664e4be9"
LOCATION = "us-west1"

import vertexai

vertexai.init(project=PROJECT_ID, location=LOCATION)
```



Navigate to AI Applications by searching for it at the top of the Cloud Console.

Go to Data Stores and select the pre-created datastore 'qna-unstructured-datastore', choose the Cloud Storage as data source.
https://cdn.qwiklabs.com/6JPMWD%2F5Umrv7ujF4Z8DUIf43QPmzC6iy%2BbJZQDKxFs%3D

Enter the pdf location as 'cloud-samples-data/gen-app-builder/search/alphabet-investor-pdfs', select the Unstructured Documents and click on Import.
https://cdn.qwiklabs.com/ggsgvpHqvGtM%2FR9%2ByTFzH6aS3k1tRgoY7M2PU5mlX6Y%3D

Note: This step takes around 5 to 10 minutes to complete.












============================================



Task 3. Import Datastore, Model information and Libraries.

Run the below code snippet to set the Datastore ID and location.
```
DATA_STORE_ID = "Data Store"  # @param {type:"string"}
DATA_STORE_LOCATION = "global"  # @param {type:"string"}

MODEL = "gemini-2.0-flash"  # @param {type:"string"}

if PROJECT_ID == "YOUR_PROJECT_ID" or DATA_STORE_ID == "YOUR_DATA_STORE_ID":
    raise ValueError(
        "Please set the PROJECT_ID, DATA_STORE_ID constants to reflect your environment."
    )
```


Import the libraries by the below commands.
```
from langchain.chains import RetrievalQA
from langchain.chains import RetrievalQAWithSourcesChain
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate

from langchain_google_vertexai import ChatVertexAI
from langchain_google_community import VertexAISearchRetriever
from langchain_google_community import VertexAIMultiTurnSearchRetriever
```













============================================


Task 4. LangChain retrieval Q&A chains
Here, we'll be having three LangChain retrieval Q&A chains:
- RetrivalQA
- RetrievalQAWithSourceChain
- ConversationalRetrivealChain




We begin by initializing a Vertex AI LLM and a LangChain 'retriever' to fetch documents from our AI Applications Search engine.
```
llm = ChatVertexAI(model_name=MODEL)

retriever = VertexAISearchRetriever(
    project_id=PROJECT_ID,
    location_id=DATA_STORE_LOCATION,
    data_store_id=DATA_STORE_ID,
    get_extractive_answers=True,
    max_documents=10,
    max_extractive_segment_count=1,
    max_extractive_answer_count=5,
)
```

RetrievalQA simplest document Q&A chain offered by LangChain.
- Here, we use the stuff type, which simply inserts all of the document chunks into the prompt.
- This has the advantage of only making a single LLM call, which is faster and more cost efficient
- However, if we have a large number of search results we run the risk of exceeding the token limit in our prompt, or truncating useful information.
- Other chain types such as map_reduce and refine use an iterative process that makes multiple LLM calls, taking individual document chunks at a time and refining the answer iteratively.
```
search_query = "What was Alphabet's Revenue in Q2 2021?"  # @param {type:"string"}

retrieval_qa = RetrievalQA.from_chain_type(
    llm=llm, chain_type="stuff", retriever=retriever
)
retrieval_qa.invoke(search_query)
```


Output:
{'query': "What was Alphabet's Revenue in Q2 2021?",
 'result': "Alphabet's revenue in Q2 2021 was $61.880 billion.\n"}





Now, we'll be inspecting the document, If we add return_source_documents=True we can inspect the document chunks that were returned by the retriever.

This is helpful for debugging, as these chunks may not always be relevant to the answer, or their relevance might not be obvious.
```
retrieval_qa = RetrievalQA.from_chain_type(
    llm=llm, chain_type="stuff", retriever=retriever, return_source_documents=True
)

results = retrieval_qa.invoke(search_query)

print("*" * 79)
print(results["result"])
print("*" * 79)
for doc in results["source_documents"]:
    print("-" * 79)
    print(doc.page_content)
```

Output:
*******************************************************************************
Alphabet's revenue in Q2 2021 was **$61.9 billion**. 

*******************************************************************************
-------------------------------------------------------------------------------
Our long-term investments in AI and Google Cloud are helping us drive significant improvements in everyone's digital experience.” “Our strong second quarter revenues of <b>$61.9 billion</b> reflect elevated consumer online activity and broad-based strength in advertiser spend.
-------------------------------------------------------------------------------
Alphabet Inc. CONSOLIDATED STATEMENTS OF INCOME (In millions, except share amounts which are reflected in thousands and per share amounts) Quarter Ended June 30, Year To Date June 30, 2020 2021 2020 2021 (unaudited) (unaudited) Revenues $ 38297 $ 61880 $ 79456 $ 117194 Costs and expenses: Cost of revenues 18553 26227 37535 50330 Research and development 6875 7675 13695 15160 Sales and marketing 3901 5276 8401 9792 General and administrative 2585 3341 5465 6114 Total costs and expenses 31914 42519 65096 81396 Income from operations 6383 19361 14360 35798 Other income (expense), net 1894 2624 1674 7470 Income before income taxes 8277 21985 16034 43268 Provision for income taxes 1318 3460 2239 6813 Net income $ 6959 $ 18525 $ 13795 $ 36455 Basic earnings per share of Class A and B common stock and Class C capital stock $ 10.21 $ 27.69 $ 20.16 $ 54.32 Diluted earnings per share of Class A and B common stock and Class C capital stock $ 10.13 $ 27.26 $ 20.00 $ 53.54 Number of shares used in basic earnings per share calculation 681768 668958 684117 671089 Number of shares used in diluted earnings per share calculation 687024 679612 689646 680842 5......and so on








RetrievalQAWithSourceChain variant returns an answer to the question alongside the source documents that were used to generate the answer.
```
retrieval_qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(
    llm=llm, chain_type="stuff", retriever=retriever
)

retrieval_qa_with_sources.invoke(search_query, return_only_outputs=True)
```

Output:
{'answer': "Alphabet's revenue in Q2 2021 was $61.9 billion.\n",
 'sources': 'gs://cloud-samples-data/gen-app-builder/search/alphabet-investor-pdfs/2021Q2_alphabet_earnings_release.pdf1, gs://cloud-samples-data/gen-app-builder/search/alphabet-investor-pdfs/2021Q2_alphabet_earnings_release.pdf2'}







ConversationalRetrievalChain remembers and uses previous questions so you can have a chat-like discovery process.

To use this chain we must provide a memory class to store and pass the previous messages to the LLM as context. Here we use the ConversationBufferMemory class that comes with LangChain.

VertexAIMultiTurnSearchRetriever uses multi-turn search (also called conversational search or search with followups) to preserve context between requests.

Now will work with both retrievers, and the multi-turn retriever can be substituted in any of the previous examples.
```
multi_turn_retriever = VertexAIMultiTurnSearchRetriever(
    project_id=PROJECT_ID, location_id=DATA_STORE_LOCATION, data_store_id=DATA_STORE_ID
)
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
conversational_retrieval = ConversationalRetrievalChain.from_llm(
    llm=llm, retriever=multi_turn_retriever, memory=memory
)

search_query = "What were alphabet revenues in 2022?"

result = conversational_retrieval.invoke(search_query)
print(result["answer"])
```

Output:
Alphabet revenues for the full year 2022 were $282.836 billion.



```
new_query = "What about costs and expenses?"
result = conversational_retrieval.invoke(new_query)
print(result["answer"])
```

Output:
Alphabet's costs and expenses are divided into:

* **Cost of revenues:** These are expenses directly related to delivering Alphabet's products and services, including traffic acquisition costs (TAC), content acquisition costs, data center expenses, and inventory costs.
* **Research and development (R&D):** These cover the costs of developing new products and services, including compensation for engineers and technical employees, depreciation, and professional service fees.
*   **Sales and marketing:** These include expenses for marketing and sales staff, advertising, and promotional activities.
* **General and administrative:** These cover administrative and support functions, professional service fees (like audit and legal), and outsourcing services.

In 2023, Alphabet also had some specific charges related to reducing their workforce and optimizing their office space, as well as accelerated rent and depreciation expenses. These charges are included in Alphabet-level activities.





```
new_query = "Is this more than in 2021?"

result = conversational_retrieval.invoke(new_query)
print(result["answer"])
```
Output:
Yes, R&D expenses increased by $5.9 billion from 2022 to 2023. This increase was primarily driven by a rise in compensation expenses of $2.9 billion, $870 million in charges related to office space optimization efforts, and an increase in depreciation expense of $722 million.















============================================





Task 5. Advanced: Modifying the default LangChain prompt
In all of the previous steps, we used the default prompt that comes with langchain.

We can inspect our chain object to discover the wording of the prompt template being used.

We may find that this is not suitable for our purposes, and we may wish to customize the prompt, for example to present our results in a different format, or to specify additional constraints.



Execute the below code snippet to get the result in the form of a template.
```
qa = RetrievalQA.from_chain_type(
    llm=llm, chain_type="stuff", retriever=retriever, return_source_documents=True
)

print(qa.combine_documents_chain.llm_chain.prompt.messages[0].prompt.template)
```

Output:
Use the following pieces of context to answer the user's question. 
If you don't know the answer, just say that you don't know, don't try to make up an answer.
----------------
{context}




Let's modify the prompt to return an answer in a single word (useful for yes/no questions). We will constrain the LLM to say 'I don't know' if it cannot answer.

We will create a new prompt_template and pass this in using the template argument.
```
prompt_template = """Use the context to answer the question at the end.
You must always use the context and context only to answer the question. Never try to make up an answer. If the context is empty or you do not know the answer, just say "I don't know".
The answer should consist of only 1 word and not a sentence.

Context: {context}

Question: {question}
Helpful Answer:
"""
prompt = PromptTemplate(
    template=prompt_template, input_variables=["context", "question"]
)
qa_chain = RetrievalQA.from_llm(
    llm=llm, prompt=prompt, retriever=retriever, return_source_documents=True
)
```

```
print(qa_chain.combine_documents_chain.llm_chain.prompt.template)
```
Output:
Use the context to answer the question at the end.
You must always use the context and context only to answer the question. Never try to make up an answer. If the context is empty or you do not know the answer, just say "I don't know".
The answer should consist of only 1 word and not a sentence.

Context: {context}

Question: {question}
Helpful Answer:






```
search_query = "Were 2020 EMEA revenues higher than 2020 APAC revenues?"

results = qa_chain.invoke(search_query)

print("*" * 79)
print(results["result"])
print("*" * 79)
for doc in results["source_documents"]:
    print("-" * 79)
    print(doc.page_content)
```
Output:
Year Ended December 31, % Change from 2020 2021 Prior Year EMEA revenues $ $ 43% EMEA constant currency revenues 38 % APAC revenues 32550 42% APAC constant currency revenues 40% Other Americas revenues 14404 53% Other Americas constant currency revenues 52% United States revenues 85014 39% Hedging gains (losses) 149 Total revenues $ $ 41% Revenues, excluding hedging effect $ 182351 $ Exchange rate effect (3330) Total constant currency revenues $ 254158 39% EMEA revenue growth from 2020 to 2021 was favorably affected by foreign currency exchange rates, primarily due to the US dollar weakening relative to the Euro and British pound.
-------------------------------------------------------------------------------
Google Cloud's infrastructure and platform services were the largest drivers of growth in GCP. Revenues by Geography The following table presents revenues by geography as a percentage of revenues, determined based on the addresses of our Year Ended December 31, 2020 2021 United States % % EMEA 30 % 31 % APAC 18 % 18 % Other Americas 5 % 5










==============


Reflections:
- The lab was last updated and tested in April 2025
- It seems that LangChain released v1 of its library in September 2025, which broke the code in the lab. The lab needs an update.


