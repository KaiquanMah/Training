{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95b70ed6-a4c7-46c0-af24-3b65f0dcb3a1",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "560f919c-277b-4591-92e5-44ee51442b9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  DEPRECATION: Building 'intervaltree' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'intervaltree'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-cloud-aiplatform 1.122.0 requires google-cloud-storage<3.0.0,>=1.32.0; python_version < \"3.13\", but you have google-cloud-storage 3.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-cloud-documentai-toolbox 0.14.2a0 requires pyarrow<16.0.0,>=15.0.0, but you have pyarrow 21.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Building 'gapic-google-longrunning' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'gapic-google-longrunning'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Building 'google-gax' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'google-gax'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Building 'ply' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'ply'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Building 'oauth2client' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'oauth2client'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/opt/conda/lib/python3.10/site-packages/google/~rotobuf'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/opt/conda/lib/python3.10/site-packages/~rpc_status'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kfp 2.14.4 requires protobuf<7.0,==6.31.1, but you have protobuf 5.29.5 which is incompatible.\n",
      "kfp-pipeline-spec 2.14.0 requires protobuf<7.0,==6.31.1, but you have protobuf 5.29.5 which is incompatible.\n",
      "langchain-google-vertexai 3.0.0 requires pyarrow<22.0.0,>=19.0.1, but you have pyarrow 15.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.24.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install google-cloud-aiplatform --upgrade --quiet\n",
    "! pip install google-cloud-discoveryengine --upgrade --quiet\n",
    "! pip install google-cloud-documentai google-cloud-documentai-toolbox --upgrade --quiet\n",
    "! pip install google-cloud-storage --upgrade --quiet\n",
    "\n",
    "! pip install langchain-google-community --upgrade --quiet\n",
    "! pip install langchain-google-vertexai --upgrade --quiet\n",
    "! pip install langchain-google-community[vertexaisearch] --upgrade --quiet\n",
    "! pip install langchain-google-community[docai] --upgrade --quiet\n",
    "\n",
    "! pip install rich --upgrade --quiet\n",
    "! pip install markdown --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e9e9163-3829-42df-b89a-6c6a5794c6a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.29.5 is exactly one major version older than the runtime version 6.31.1 at google/protobuf/empty.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex AI SDK initialized.\n",
      "Vertex AI SDK version = 1.122.0\n",
      "Document AI API version = 2.35.0\n",
      "Discovery Engine API version = 0.14.0\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "from google.cloud import documentai\n",
    "from google.cloud import discoveryengine\n",
    "\n",
    "PROJECT_ID = \"qwiklabs-gcp-04-b187b658bea8\"  \n",
    "REGION = \"us-central1\"  \n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "print(f\"Vertex AI SDK initialized.\")\n",
    "print(f\"Vertex AI SDK version = {vertexai.__version__}\")\n",
    "print(f\"Document AI API version = {documentai.__version__}\")\n",
    "print(f\"Discovery Engine API version = {discoveryengine.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0402e2-3d10-4d96-9f75-3cbec17bf523",
   "metadata": {},
   "source": [
    "# Task 2. Initialize variables, import a Python utility file, and create resources\n",
    "In this task, you initialize variables for cloud resources, download a Python utility file, and run code to create a Cloud Storage bucket, Vector Search index, and Document AI processor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c248b00b-97db-4315-8a87-103bce23696b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the following in a cell to intialize variables and set names for your cloud storage bucket, index, index endpoint, and docai processor.\n",
    "\n",
    "# Cloud storage buckets\n",
    "GCS_BUCKET_URI = \"gs://qwiklabs-gcp-04-b187b658bea8-bucket\"  \n",
    "GCS_OUTPUT_PATH = f\"{GCS_BUCKET_URI}\"  # DocAI Layout Parser Output Path\n",
    "GCS_BUCKET_NAME = GCS_BUCKET_URI.replace(\"gs://\", \"\")\n",
    "\n",
    "# Vertex AI Vector Search\n",
    "# parameter description here\n",
    "# https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.MatchingEngineIndex#google_cloud_aiplatform_MatchingEngineIndex_create_tree_ah_index\n",
    "VS_INDEX_NAME = \"qwiklabs-gcp-04-b187b658bea8-index\"  \n",
    "VS_INDEX_ENDPOINT_NAME = \"qwiklabs-gcp-04-b187b658bea8-endpoint\"  \n",
    "VS_CONTENTS_DELTA_URI = f\"{GCS_BUCKET_URI}/index/embeddings\"\n",
    "VS_DIMENSIONS = 768       # HERE\n",
    "VS_APPROX_NEIGHBORS = 150 # HERE\n",
    "VS_INDEX_UPDATE_METHOD = \"STREAM_UPDATE\"\n",
    "VS_INDEX_SHARD_SIZE = \"SHARD_SIZE_SMALL\"\n",
    "VS_LEAF_NODE_EMB_COUNT = 500\n",
    "VS_LEAF_SEARCH_PERCENT = 80\n",
    "VS_DISTANCE_MEASURE_TYPE = \"DOT_PRODUCT_DISTANCE\"    # HERE\n",
    "VS_MACHINE_TYPE = \"e2-standard-16\"                   # HERE\n",
    "VS_MIN_REPLICAS = 1\n",
    "VS_MAX_REPLICAS = 1\n",
    "VS_DESCRIPTION = \"Index for DIY RAG with Vertex AI APIs\"  \n",
    "\n",
    "# Models\n",
    "EMBEDDINGS_MODEL_NAME = \"text-embedding-004\"    # HERE\n",
    "LLM_MODEL_NAME = \"gemini-2.0-flash-001\"         # HERE\n",
    "\n",
    "# DocumentAI Processor\n",
    "DOCAI_LOCATION = \"us\"  \n",
    "DOCAI_PROCESSOR_NAME = \"qwiklabs-gcp-04-b187b658bea8-docai-processor\"  \n",
    "\n",
    "# Enable/disable flags\n",
    "# flag to create Google Cloud resources configured above\n",
    "# refer to the notes after this cell\n",
    "CREATE_RESOURCES = True  \n",
    "# flag to run data ingestion\n",
    "RUN_INGESTION = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d05acc6-887f-4534-9333-8829002732cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://qwiklabs-gcp-04-b187b658bea8/utils.py to file://./utils.py\n",
      "  Completed files 1/1 | 29.1kiB/29.1kiB                                        \n"
     ]
    }
   ],
   "source": [
    "# Run the following gcloud command in a new cell to download a Python file with some helpful utility functions.\n",
    "!gcloud storage cp gs://qwiklabs-gcp-04-b187b658bea8/utils.py ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e717e8-5276-4a77-b68a-83fb91438094",
   "metadata": {},
   "source": [
    "The file utils.py should appear inside the file explorer. A good time to examine the file content would be while step 4 below is running, as it will take some time to complete. The file contains the following:\n",
    "- DocAIParser, a class that uses the Document AI Layout Parser to parse PDF documents into chunks.\n",
    "- CustomGCSDirectoryLoader, a class that loads documents from a Cloud Storage Bucket\n",
    "- Utility methods for adding an index to Vector Search\n",
    "- Utility methods for displaying rich content results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b07b996-5551-44ba-bb8c-c9cc47918112",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the required library by adding the following to a new cell and running it with SHIFT+ENTER:\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af5a9145-da9e-43b8-98b3-d9e377029eb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new resources.\n",
      "Bucket qwiklabs-gcp-04-b187b658bea8-bucket created\n",
      "Creating Vector Search index qwiklabs-gcp-04-b187b658bea8-index ...\n",
      "Vector Search index qwiklabs-gcp-04-b187b658bea8-index created with resource name projects/319136220147/locations/us-central1/indexes/6258888577254424576\n",
      "Creating Vector Search index endpoint qwiklabs-gcp-04-b187b658bea8-endpoint ...\n",
      "Vector Search index endpoint qwiklabs-gcp-04-b187b658bea8-endpoint created with resource name projects/319136220147/locations/us-central1/indexEndpoints/3074887621168594944\n",
      "Deploying Vector Search index qwiklabs-gcp-04-b187b658bea8-index at endpoint qwiklabs-gcp-04-b187b658bea8-endpoint ...\n",
      "Vector Search index qwiklabs-gcp-04-b187b658bea8-index is deployed at endpoint qwiklabs-gcp-04-b187b658bea8-endpoint\n",
      "Creating Document AI processor qwiklabs-gcp-04-b187b658bea8-docai-processor of type LAYOUT_PARSER_PROCESSOR ...\n",
      "Document AI processor qwiklabs-gcp-04-b187b658bea8-docai-processor of type LAYOUT_PARSER_PROCESSOR is created.\n"
     ]
    }
   ],
   "source": [
    "# Run the following code in a new cell to create necessary resources, then examine the code, and read the explanation that follows the code sample:\n",
    "\n",
    "import hashlib\n",
    "import uuid\n",
    "\n",
    "from google.cloud import storage\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import documentai\n",
    "from google.api_core.client_options import ClientOptions\n",
    "from google.cloud.aiplatform import MatchingEngineIndex, MatchingEngineIndexEndpoint\n",
    "from typing import List, Optional\n",
    "\n",
    "def create_uuid(name: str) -> str:\n",
    "    hex_string = hashlib.md5(name.encode(\"UTF-8\")).hexdigest()\n",
    "    return str(uuid.UUID(hex=hex_string))\n",
    "\n",
    "def create_bucket(bucket_name: str) -> storage.Bucket:\n",
    "    # create Cloud Storage bucket if does not exist\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    if bucket.exists():\n",
    "        print(f\"Bucket {bucket.name} exists\")\n",
    "        return bucket\n",
    "\n",
    "    if not CREATE_RESOURCES:\n",
    "        return bucket\n",
    "\n",
    "    bucket = storage_client.create_bucket(bucket_name, project=PROJECT_ID)\n",
    "    print(f\"Bucket {bucket.name} created\")\n",
    "    return bucket\n",
    "\n",
    "def create_index() -> Optional[MatchingEngineIndex]:\n",
    "    index_names = [\n",
    "        index.resource_name\n",
    "        for index in MatchingEngineIndex.list(filter=f\"display_name={VS_INDEX_NAME}\")\n",
    "    ]\n",
    "\n",
    "    if len(index_names) > 0:\n",
    "        vs_index = MatchingEngineIndex(index_name=index_names[0])\n",
    "        print(\n",
    "            f\"Vector Search index {vs_index.display_name} exists with resource name {vs_index.resource_name}\"\n",
    "        )\n",
    "        return vs_index\n",
    "\n",
    "    if not CREATE_RESOURCES:\n",
    "        print(\n",
    "            f\"CREATE_RESOURCES flag set to {CREATE_RESOURCES}. Skip creating resources\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    print(f\"Creating Vector Search index {VS_INDEX_NAME} ...\")\n",
    "    vs_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "        display_name=VS_INDEX_NAME,\n",
    "        dimensions=VS_DIMENSIONS,\n",
    "        approximate_neighbors_count=VS_APPROX_NEIGHBORS,\n",
    "        distance_measure_type=VS_DISTANCE_MEASURE_TYPE,\n",
    "        leaf_node_embedding_count=VS_LEAF_NODE_EMB_COUNT,\n",
    "        leaf_nodes_to_search_percent=VS_LEAF_SEARCH_PERCENT,\n",
    "        description=VS_DESCRIPTION,\n",
    "        shard_size=VS_INDEX_SHARD_SIZE,\n",
    "        index_update_method=VS_INDEX_UPDATE_METHOD,\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "    )\n",
    "    print(\n",
    "        f\"Vector Search index {vs_index.display_name} created with resource name {vs_index.resource_name}\"\n",
    "    )\n",
    "    return vs_index\n",
    "\n",
    "def create_index_endpoint() -> Optional[MatchingEngineIndexEndpoint]:\n",
    "    endpoint_names = [\n",
    "        endpoint.resource_name\n",
    "        for endpoint in MatchingEngineIndexEndpoint.list(\n",
    "            filter=f\"display_name={VS_INDEX_ENDPOINT_NAME}\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    if len(endpoint_names) > 0:\n",
    "        vs_endpoint = MatchingEngineIndexEndpoint(index_endpoint_name=endpoint_names[0])\n",
    "        print(\n",
    "            f\"Vector Search index endpoint {vs_endpoint.display_name} exists with resource name {vs_endpoint.resource_name}\"\n",
    "        )\n",
    "        return vs_endpoint\n",
    "\n",
    "    if not CREATE_RESOURCES:\n",
    "        print(\n",
    "            f\"CREATE_RESOURCES flag set to {CREATE_RESOURCES}. Skip creating resources\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    print(f\"Creating Vector Search index endpoint {VS_INDEX_ENDPOINT_NAME} ...\")\n",
    "    vs_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "        display_name=VS_INDEX_ENDPOINT_NAME,\n",
    "        public_endpoint_enabled=True,\n",
    "        description=VS_DESCRIPTION,\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "    )\n",
    "    print(\n",
    "        f\"Vector Search index endpoint {vs_endpoint.display_name} created with resource name {vs_endpoint.resource_name}\"\n",
    "    )\n",
    "    return vs_endpoint\n",
    "\n",
    "\n",
    "\n",
    "# QUESTION\n",
    "# THE INDEX OBJECT AND INDEXENDPOINT OBJECT ARE CREATED SEPARATELY\n",
    "# THEN DEPLOYED TOGETHER USING 'endpoint.deploy_index',\n",
    "# WHY are these not in 1 single call?\n",
    "def deploy_index(\n",
    "    index: MatchingEngineIndex, endpoint: MatchingEngineIndexEndpoint\n",
    ") -> Optional[MatchingEngineIndexEndpoint]:\n",
    "    index_endpoints = []\n",
    "    if index is not None:\n",
    "        index_endpoints = [\n",
    "            (deployed_index.index_endpoint, deployed_index.deployed_index_id)\n",
    "            for deployed_index in index.deployed_indexes\n",
    "        ]\n",
    "\n",
    "    if len(index_endpoints) > 0:\n",
    "        vs_deployed_index = MatchingEngineIndexEndpoint(\n",
    "            index_endpoint_name=index_endpoints[0][0]\n",
    "        )\n",
    "        print(\n",
    "            f\"Vector Search index {index.display_name} is already deployed at endpoint {vs_deployed_index.display_name}\"\n",
    "        )\n",
    "        return vs_deployed_index\n",
    "\n",
    "    if not CREATE_RESOURCES:\n",
    "        print(\n",
    "            f\"CREATE_RESOURCES flag set to {CREATE_RESOURCES}. Skip creating resources\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    print(\n",
    "        f\"Deploying Vector Search index {index.display_name} at endpoint {endpoint.display_name} ...\"\n",
    "    )\n",
    "    deployed_index_id = (\n",
    "        f'{VS_INDEX_NAME}_{create_uuid(VS_INDEX_NAME).split(\"-\")[-1]}'.replace(\"-\", \"_\")\n",
    "    )\n",
    "    vs_deployed_index = endpoint.deploy_index(\n",
    "        index=index,\n",
    "        deployed_index_id=deployed_index_id,\n",
    "        display_name=VS_INDEX_NAME,\n",
    "        machine_type=VS_MACHINE_TYPE,\n",
    "        min_replica_count=VS_MIN_REPLICAS,\n",
    "        max_replica_count=VS_MAX_REPLICAS,\n",
    "    )\n",
    "    print(\n",
    "        f\"Vector Search index {index.display_name} is deployed at endpoint {vs_deployed_index.display_name}\"\n",
    "    )\n",
    "    return vs_deployed_index\n",
    "\n",
    "def create_docai_processor(\n",
    "    processor_display_name: str = DOCAI_PROCESSOR_NAME,\n",
    "    processor_type: str = \"LAYOUT_PARSER_PROCESSOR\",\n",
    ") -> Optional[documentai.Processor]:\n",
    "    # Set the api_endpoint if you use a location other than 'us'\n",
    "    opts = ClientOptions(api_endpoint=f\"{DOCAI_LOCATION}-documentai.googleapis.com\")\n",
    "    docai_client = documentai.DocumentProcessorServiceClient(client_options=opts)\n",
    "    parent = docai_client.common_location_path(PROJECT_ID, DOCAI_LOCATION)\n",
    "    # Check if processor exists\n",
    "    processor_list = docai_client.list_processors(parent=parent)\n",
    "    processors = [\n",
    "        processor.name\n",
    "        for processor in processor_list\n",
    "        if (\n",
    "            processor.display_name == processor_display_name\n",
    "            and processor.type_ == processor_type\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    if len(processors) > 0:\n",
    "        docai_processor = docai_client.get_processor(name=processors[0])\n",
    "        print(\n",
    "            f\"Document AI processor {docai_processor.display_name} is already created\"\n",
    "        )\n",
    "        return docai_processor\n",
    "\n",
    "    if not CREATE_RESOURCES:\n",
    "        print(\n",
    "            f\"CREATE_RESOURCES flag set to {CREATE_RESOURCES}. Skip creating resources\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    # Create a processor\n",
    "    print(\n",
    "        f\"Creating Document AI processor {processor_display_name} of type {processor_type} ...\"\n",
    "    )\n",
    "    docai_processor = docai_client.create_processor(\n",
    "        parent=parent,\n",
    "        processor=documentai.Processor(\n",
    "            display_name=processor_display_name, type_=processor_type\n",
    "        ),\n",
    "    )\n",
    "    print(\n",
    "        f\"Document AI processor {processor_display_name} of type {processor_type} is created.\"\n",
    "    )\n",
    "    return docai_processor\n",
    "\n",
    "\n",
    "# @title Utility methods for adding index to Vertex AI Vector Search\n",
    "def get_batches(items: List, n: int = 1000) -> List[List]:\n",
    "    n = max(1, n)\n",
    "    return [items[i : i + n] for i in range(0, len(items), n)]\n",
    "\n",
    "\n",
    "def add_data(vector_store, chunks) -> None:\n",
    "    if RUN_INGESTION:\n",
    "        batch_size = 1000\n",
    "        texts = get_batches([chunk.page_content for chunk in chunks], n=batch_size)\n",
    "        metadatas = get_batches([chunk.metadata for chunk in chunks], n=batch_size)\n",
    "\n",
    "        for i, (b_texts, b_metadatas) in enumerate(zip(texts, metadatas)):\n",
    "            print(f\"Adding {len(b_texts)} data points to index\")\n",
    "            is_complete_overwrite = bool(i == 0)\n",
    "            vector_store.add_texts(\n",
    "                texts=b_texts,\n",
    "                metadatas=b_metadatas,\n",
    "                is_complete_overwrite=is_complete_overwrite,\n",
    "            )\n",
    "    else:\n",
    "        print(\"Skipping ingestion. Enable `RUN_INGESTION` flag\")\n",
    "\n",
    "if CREATE_RESOURCES:\n",
    "    print(\"Creating new resources.\")\n",
    "else:\n",
    "    print(\"Resource creation is skipped.\")\n",
    "\n",
    "# Create bucket if not exists\n",
    "bucket = create_bucket(GCS_BUCKET_NAME)\n",
    "\n",
    "# Create vector search index if not exists else return index resource name\n",
    "vs_index = create_index()\n",
    "\n",
    "# Create vector search index endpoint if not exists else return index endpoint resource name\n",
    "vs_endpoint = create_index_endpoint()\n",
    "\n",
    "# Deploy index to the index endpoint\n",
    "deploy_index(vs_index, vs_endpoint)\n",
    "\n",
    "# Create Document Layout Processor\n",
    "docai_processor = create_docai_processor(processor_display_name=DOCAI_PROCESSOR_NAME)\n",
    "PROCESSOR_NAME = docai_processor.name  # DocAI Layout Parser Processor Name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55b9a37-cdd7-4d1f-9746-f6440fbd171c",
   "metadata": {},
   "source": [
    "This code creates a number of resources required by your pipeline. It uses Google Cloud libraries and the utility functions you imported earlier to create:\n",
    "- A Cloud Storage Bucket to store raw text\n",
    "- A Vector Search Index and endpoint to store and provide access to vectors\n",
    "- A Document AI processor to parse and chunk documents\n",
    "\n",
    "This would be a good time to examine the contents of utils.py, and to read the architecture and pipeline discussion at the beginning of the lab if you have not already done so.\n",
    "\n",
    "Important: The code that you executed in step 4 creates a number of resources, including a Vector Search index with endpoint, and can take more than 25 minutes to complete. Wait for the cell to complete and the resources to be created before continuing.\n",
    "\n",
    "Note: Resource creation is skipped if the CREATE_RESOURCES flag is set to False in the Initialize Variables section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbb0281-1c3e-4f55-8c4c-638aaefe7d26",
   "metadata": {},
   "source": [
    "# Task 3. Ingest data\n",
    "In this task, you read sample documents from a Cloud Storage bucket, parse them using the Document AI layout processor, extract chunks from the parsed document, generate embeddings using the Vertex AI Embeddings API, and add them to the Vertex AI Vector Search index.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9806e211-887e-4806-ba9f-b7ac91e15642",
   "metadata": {},
   "source": [
    "**utils.py functions are used from task 3 onwards**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9886dfd-56ea-408f-a4f1-9876998dcd64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "PackageNotFoundError",
     "evalue": "No package metadata was found for langchain",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run the following code in a new cell to read the sample documents from a public Cloud Storage bucket.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# This code uses a class defined inside utils.py to load Alphabet investor reports for the years 2021, 2022, and 2023 from the public bucket.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m loader \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mCustomGCSDirectoryLoader(\n\u001b[1;32m      5\u001b[0m     project_name\u001b[38;5;241m=\u001b[39mPROJECT_ID,\n\u001b[1;32m      6\u001b[0m     bucket\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcloud-samples-data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgen-app-builder/search/alphabet-investor-pdfs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m doc_blobs \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_pattern\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.*/202[1-3]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/utils.py:519\u001b[0m, in \u001b[0;36mCustomGCSDirectoryLoader.load\u001b[0;34m(self, file_pattern)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import google-cloud-storage python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    515\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install google-cloud-storage`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    516\u001b[0m     )\n\u001b[1;32m    517\u001b[0m client \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mClient(\n\u001b[1;32m    518\u001b[0m     project\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject_name,\n\u001b[0;32m--> 519\u001b[0m     client_info\u001b[38;5;241m=\u001b[39m\u001b[43mget_client_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle-cloud-storage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    520\u001b[0m )\n\u001b[1;32m    522\u001b[0m regex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_pattern:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_community/utilities/vertexai.py:102\u001b[0m, in \u001b[0;36mget_client_info\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import ClientInfo. Please, install it with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpip install google-api-core\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m langchain_version \u001b[38;5;241m=\u001b[39m \u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlangchain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m client_library_version \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlangchain_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;28;01melse\u001b[39;00m langchain_version\n\u001b[1;32m    105\u001b[0m )\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ClientInfo(\n\u001b[1;32m    107\u001b[0m     client_library_version\u001b[38;5;241m=\u001b[39mclient_library_version,\n\u001b[1;32m    108\u001b[0m     user_agent\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangchain/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclient_library_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    109\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/importlib/metadata/__init__.py:996\u001b[0m, in \u001b[0;36mversion\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mversion\u001b[39m(distribution_name):\n\u001b[1;32m    990\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \n\u001b[1;32m    992\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;124;03m    :return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;124;03m        \"Version\" metadata key.\u001b[39;00m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mversion\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/importlib/metadata/__init__.py:969\u001b[0m, in \u001b[0;36mdistribution\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdistribution\u001b[39m(distribution_name):\n\u001b[1;32m    964\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the ``Distribution`` instance for the named package.\u001b[39;00m\n\u001b[1;32m    965\u001b[0m \n\u001b[1;32m    966\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package as a string.\u001b[39;00m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;124;03m    :return: A ``Distribution`` instance (or subclass thereof).\u001b[39;00m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 969\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/importlib/metadata/__init__.py:548\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m dist\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 548\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PackageNotFoundError(name)\n",
      "\u001b[0;31mPackageNotFoundError\u001b[0m: No package metadata was found for langchain"
     ]
    }
   ],
   "source": [
    "# Run the following code in a new cell to read the sample documents from a public Cloud Storage bucket.\n",
    "# This code uses a class defined inside utils.py to load Alphabet investor reports for the years 2021, 2022, and 2023 from the public bucket.\n",
    "\n",
    "loader = utils.CustomGCSDirectoryLoader(\n",
    "    project_name=PROJECT_ID,\n",
    "    bucket=\"cloud-samples-data\",\n",
    "    prefix=\"gen-app-builder/search/alphabet-investor-pdfs\",\n",
    ")\n",
    "\n",
    "doc_blobs = loader.load(file_pattern=\".*/202[1-3]\")[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f869c9f1-5ff6-4795-a87e-c8c078df0889",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3148fd-2235-4193-b6ba-cd0c300cb776",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bbaf7c-264c-4a58-b7cc-6ab0066dc33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following code in a new cell to create a custom parser.\n",
    "# Note: The DocAIParser class is defined inside utils.py. It uses the Document AI Layout Parser to convert blobs into layout-aware chunks.\n",
    "# Layout Parser extracts document content elements like text, tables, and lists, and creates context-aware chunks that are useful for building RAG applications.\n",
    "\n",
    "parser = utils.DocAIParser(\n",
    "    project_id=PROJECT_ID,\n",
    "    location=DOCAI_LOCATION,\n",
    "    processor_name=PROCESSOR_NAME,\n",
    "    gcs_output_path=GCS_OUTPUT_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b084adf-151a-4666-967b-f0fe77172df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2e3777-9774-4460-a570-6461d1bddf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following code in a new cell to process the documents.\n",
    "# Note: This cell should take less than 2 minutes to run to completion. Wait for it to finish before continuing.\n",
    "\n",
    "docs = list(\n",
    "    parser.batch_parse(\n",
    "        doc_blobs,  # filter only last 40 for docs after 2020\n",
    "        chunk_size=500,\n",
    "        include_ancestor_headings=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "# At this point, you are ready to start converting chunks into embeddings. First, you examine two of the chunks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e93699-e721-4c97-94fd-8d3afb581d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb16d5b-6f24-4277-8f23-cb5421a48c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following code in a new cell to print the first and third chunks to the screen.\n",
    "\n",
    "print(\"THE FIRST CHUNK:\")\n",
    "print(docs[1].page_content)\n",
    "print()\n",
    "print(\"THE THIRD CHUNK:\")\n",
    "print(docs[3].page_content)\n",
    "\n",
    "# Note: Notice that the documents are parsed into different sections like title, subtitle, and even a Markdown table (an especially complex table with merged cells!).\n",
    "# This makes it easy for retrieval as well for the downstream generation tasks. For example, a large language model (LLM) can now reason more effectively and more accurately.\n",
    "# You have successfully chunked the document, but the chunks are still just text. Next, you create embeddings of the text chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f46f6dd-c65c-4d5e-9b59-1b07acd2010c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following code in a new cell to define the model for creating embeddings:\n",
    "\n",
    "from langchain_google_vertexai.embeddings import VertexAIEmbeddings\n",
    "\n",
    "embedding_model = VertexAIEmbeddings(model_name=EMBEDDINGS_MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822f5ae7-b9c0-4bf8-8491-64ddaf0b0a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following code to initialize the Vertex AI Vector Search retriever:\n",
    "\n",
    "from langchain_google_vertexai.vectorstores.vectorstores import VectorSearchVectorStore\n",
    "\n",
    "vector_store = VectorSearchVectorStore.from_components(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    "    gcs_bucket_name=GCS_BUCKET_NAME,\n",
    "    index_id=vs_index.resource_name,\n",
    "    endpoint_id=vs_endpoint.resource_name,\n",
    "    embedding=embedding_model,\n",
    "    stream_update=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e8af7f-3d3b-466b-8b8c-e5ad5bbdad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, run the following code to use the add_data method in utils to store chunks as embeddings in the Vector Search index, and raw texts in the Cloud Storage bucket:\n",
    "\n",
    "add_data(vector_store, docs)\n",
    "\n",
    "# You have now retrieved source documents, processed as well as chunked them, embedded them into vectors, and upserted them into Vector Search.\n",
    "\n",
    "# In the next task, you run searches against your vector store and generate grounded text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e867796-9c4a-44a7-8e0f-d2e5585aefd1",
   "metadata": {},
   "source": [
    "# Task 4. Retrieve and rank results\n",
    "\n",
    "In this task, you use Vertex AI Vector Search to retrieve the top-k relevant results, and then rerank them using the Vertex AI Ranking API based on chunk content and semantic similarity to the query.\n",
    "\n",
    "![RAG](https://cdn.qwiklabs.com/7SEpiASYyaQBABk34SnpQPGVHqBK0c6rzMjilbO9eag%3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf2dfbc-3d94-4481-95af-b5a976b94315",
   "metadata": {},
   "source": [
    "The Ranking API takes a list of documents and reranks them based on their relevance to a given query. Unlike embeddings, which focus on semantic similarity between a document and a query, the Ranking API provides precise relevance scores that indicate how well each document answers the query.\n",
    "\n",
    "The Ranking API is stateless, meaning you don’t need to index documents beforehand. You simply pass in the query and the list of documents. This makes it ideal for reranking results retrieved from Vector Search or other search solutions to improve the overall quality of search results.\n",
    "\n",
    "Note: For more information, refer to the Improve search and RAG quality with ranking API reference documentation.\n",
    "https://cloud.google.com/generative-ai-app-builder/docs/ranking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c4d006-79b8-4457-a090-5d4a9c1d7174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following code in a new cell to define and combine retrievers using Vector Search and the Vertex AI Ranking API:\n",
    "\n",
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_google_community import VertexAIRank\n",
    "\n",
    "# Instantiate the VertexAIReranker with the SDK manager\n",
    "reranker = VertexAIRank(\n",
    "    project_id=PROJECT_ID,\n",
    "    location_id=\"global\",\n",
    "    ranking_config=\"default_ranking_config\",\n",
    "    title_field=\"source\",  # metadata field to preserve with reranked results\n",
    "    top_n=5,\n",
    ")\n",
    "\n",
    "basic_retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"k\": 5}\n",
    ")  # fetch top 5 documents\n",
    "\n",
    "# Create the ContextualCompressionRetriever with the VertexAIRanker as a Reranker\n",
    "retriever_with_reranker = ContextualCompressionRetriever(\n",
    "    base_compressor=reranker, base_retriever=basic_retriever\n",
    ")\n",
    "\n",
    "# Note: By prioritizing semantically relevant documents, the Ranking API improves the LLM's context, leading to more accurate and well-reasoned answers.\n",
    "# In the next step, you compare the Retriever Results and the Reranked Results side-by-side to see the improvement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8254738c-f15f-45e0-908f-2f918c0558b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following code in a new cell to use the get_sxs_comparison method in utils to view original and reranked results side-by-side inside your notebook:\n",
    "\n",
    "reranked_results = utils.get_sxs_comparison(\n",
    "    simple_retriever=basic_retriever,\n",
    "    reranking_api_retriever=retriever_with_reranker,\n",
    "    query=\"what was google cloud revenue in 2023 ?\",\n",
    "    search_kwargs={\"k\": 5},\n",
    ")\n",
    "\n",
    "# Note: You have retrieved the most relevant facts from your indexed source data. Now you can configure a RAG chain that follows this pipeline: query -> vector search -> retrieve documents -> LLM -> rerank documents -> check grounding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874c06fb-e432-4d3c-8f51-6d25dee723c9",
   "metadata": {},
   "source": [
    "## Check Grounding API\n",
    "The Check Grounding API returns an overall support score between 0 and 1, indicating how well an answer candidate aligns with a given set of facts. It also provides citations for the facts supporting each claim in the text.\n",
    "\n",
    "A **claim is considered perfectly grounded** only if it is **fully supported by one or more facts. Partial support does not qualify as grounded**. For example, the claim Google was founded by Larry Page and Sergey Brin in 1975 is ungrounded—the founders are correct, but the date is wrong.\n",
    "**In this version of the API, each sentence is treated as a single claim.**\n",
    "\n",
    "You can use the Check Grounding API to evaluate any piece of text—human or AI-generated. A common use case is verifying LLM responses against a known fact set.\n",
    "\n",
    "The API is optimized for low latency (<500ms), making it suitable for real-time chatbot integration.\n",
    "It also returns citations and a support score, enabling applications to highlight grounded content and filter out hallucinations using a citation threshold.\n",
    "\n",
    "Note: For more information, refer to the Check grounding with RAG documentation. https://cloud.google.com/generative-ai-app-builder/docs/check-grounding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b37b19-5349-4660-94a0-3d4987d7511c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following code in a new cell to configure a retreiver from the vector store you defined earlier.\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "from langchain_google_vertexai import VertexAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain_google_community import VertexAICheckGroundingWrapper\n",
    "\n",
    "from rich import print\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156c5b2b-1289-4683-927b-ad488f6b9d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following code in a new cell to configure the LLM with a prompt template to generate the answer.\n",
    "\n",
    "llm = VertexAI(model_name=LLM_MODEL_NAME, max_output_tokens=1024)\n",
    "template = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "create_answer = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4612068-02fb-4123-a628-b72551aae4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following code in a new cell to define a wrapper to call Vertex AI Check Grounding API on the generated answer:\n",
    "\n",
    "output_parser = VertexAICheckGroundingWrapper(\n",
    "    project_id=PROJECT_ID,\n",
    "    location_id=\"global\",\n",
    "    grounding_config=\"default_grounding_config\",\n",
    "    top_n=3,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf219c8-fb1c-4216-a381-3adb14c129f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, complete the workflow by defining a QA chain with a check on the grounding of the result:\n",
    "\n",
    "@chain\n",
    "def check_grounding_output_parser(answer_candidate: str, documents: List[Document]):\n",
    "    return output_parser.with_config(configurable={\"documents\": documents}).invoke(\n",
    "        answer_candidate\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# this 'workflow' is used in the fn below\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"query\": RunnablePassthrough()}\n",
    ")\n",
    "\n",
    "@chain\n",
    "def qa_with_check_grounding(query):\n",
    "    docs = setup_and_retrieval.invoke(query)\n",
    "    answer_candidate = create_answer.invoke(docs)\n",
    "    check_grounding_output = check_grounding_output_parser.invoke(\n",
    "        answer_candidate, documents=docs[\"context\"]\n",
    "    )\n",
    "    return check_grounding_output\n",
    "\n",
    "# Your entire pipeline is now in place. You have ingested data, parsed it into chunks, converted chunks to embeddings and stored them in Vector Search. You have then created retrievers to rerank results and check their grounding. Finally, you created a chain that included a prompt template and called the Ranking and Check Grounding APIs, respectively, to get the most relevant result and rate the level of grounding.\n",
    "\n",
    "# With everything in place, it is time to run a query and invoke the chain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b259887-4218-47df-a429-9c902f9f3ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following code in a new cell to pass the query \"what was google cloud revenue in Q1 2021?\" to your qa_with_check_grounding chain:\n",
    "\n",
    "result = qa_with_check_grounding.invoke(\"what was google cloud revenue in Q1 2021?\")\n",
    "print(result)\n",
    "\n",
    "# Note: You should get a response from the check grounding service, including a support_score measuring grounding accuracy, as well as cited chunks, sources, and the answer to the query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7971989e-dba1-45a3-8e59-804d5f7a9f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following code in a new cell to view the answer to the query, the source link, and a citation:\n",
    "\n",
    "utils.display_grounded_generation(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d045ac4-8b4a-4b68-a963-1bb18283d659",
   "metadata": {},
   "source": [
    "Your query result should resemble the following.\n",
    "\n",
    "![Query Result](https://cdn.qwiklabs.com/Oq%2B9MJ2Q8jGhHLKzWbtLKd9SznHKfMG%2BokEcWMsfKOg%3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65442d07-5844-4912-bbda-93cd677c29b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following code to run a second query against the chain, and display grounded results:\n",
    "\n",
    "result = qa_with_check_grounding.invoke(\n",
    "    \"what are the main influencing factors on Alphabet revenue in Q1 2021 ?\"\n",
    ")\n",
    "utils.display_grounded_generation(result)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m134",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m134"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
