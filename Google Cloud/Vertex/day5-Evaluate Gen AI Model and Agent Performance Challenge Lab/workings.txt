Use the Google Gen AI evaluation service in the following scenarios:
Evaluate a model's performance in response to prompts
Evaluate and compare two model's performance to each other for model selection
Evaluate the performance of agents



Additional materials
1 Documentation
Gen AI evaluation service overview
https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview
Vertex AI SDK evaluation package
https://cloud.google.com/python/docs/reference/vertexai/latest
Evaluate an Agent
https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/evaluate#recall_1
Evaluation Metrics
https://cloud.google.com/vertex-ai/generative-ai/docs/models/determine-eval
2 Path
Build with Vertex AI: Evaluating Model and Agent Performance
https://partner.skills.google/paths/2311
3 Lab
[Lab with issues] Measure Gen AI performance with the Generative AI Evaluation Service https://partner.skills.google/course_templates/1130/labs/528775






region
us-west1
zone
us-west1-c



===================



Task 1. Initialize Vertex AI in a Colab Enterprise notebook
Note: The default region and zone for this lab are us-west1 and us-west1-c, respectively. Use this region and zone wherever applicable to ensure consistency with the lab environment.

In this task, you will be setting up a Colab Enterprise notebook and initializing Vertex AI to connect the notebook. You will use this notebook to perform two tasks:
1
EVALUATE a SINGLE MODEL's ability to provide summarization of product descriptions
2
Compare 2 MODELS' ability to provide summarization of product descriptions for model selection


In the Google Cloud Console, navigate to Vertex AI > Colab Enterprise.

If prompted, enable the required APIs.

Create a new notebook in the us-west1 region.

Rename the notebook to cymbal-genai-model-evaluations.ipynb.

Paste the following code into the top cell and run it.
```
!pip install --upgrade google-cloud-aiplatform google-cloud-logging --quiet
!pip install "google-cloud-aiplatform[evaluation]" --quiet
```

Note: If you don’t already have an active notebook runtime, running a cell in a Colab Enterprise notebook will trigger it to create one for you and connect the notebook to it. When a runtime is allocated for the first time, you may be presented with a pop-up window to authorize the environment to act as your Qwiklabs student account.
Note: If you encounter the error "Failed to execute cell. Could not execute the message to runtime. ChannelError", it is usually caused by a temporary disconnection from the runtime environment. Please wait a few seconds and then re-run the cell.

After the cell completes running, indicated by a checkmark to the left of the cell, the packages should be installed. To use them, we’ll restart the runtime. Restart the session now.

Click + Code to add a new code section and paste the following code below. Run the cell.
```
import pandas as pd
import logging
import google.cloud.logging
from IPython.display import display, Markdown

import vertexai
from vertexai.generative_models import GenerativeModel, GenerationConfig
from vertexai.evaluation import (
    MetricPromptTemplateExamples,
    EvalTask,
    PairwiseMetric,
    PointwiseMetric,
)

# Do not remove logging section
client = google.cloud.logging.Client()
client.setup_logging()

pd.set_option("display.max_colwidth", None)
```


In a new code block, initialize Vertex AI with vertexai.init(). Use the us-west1 location and run the cell.
```
PROJECT_ID = "qwiklabs-gcp-01-ecf3a270a90b"
LOCATION = "us-west1"

import vertexai

# Initialize vertexai
vertexai.init()

# Do not remove logging section
log_message = f"Vertex AI initialize: {vertexai}"
logging.info(log_message)
```

Run and save the notebook.










===================

For the remaining tasks, please refer to the 2 notebooks:
Task1 to 4 Pointwise, Pairwise Summary Evaluation - cymbal_genai_model_evaluations.ipynb
Task5 to 9 SingleTool, Trajectory, Safety-Coherence - cymbal_genai_agent_evaluations.ipynb


Reflections
- Alright challenge lab which tested concepts in the 2 labs
  1. [Lab has issues] Measure Gen AI performance with the Generative AI Evaluation Service
  2. Evaluate ADK agent performance using the Vertex AI Generative AI Evaluation Service
- Pairwise evaluation has issues from hallucinations - not sure whether this is expected

