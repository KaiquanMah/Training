Create a source connection and grant IAM permissions.
Generate embeddings and convert text data to vector embeddings.
Search the vector space and retrieve similar items.
Generate an improved answer by augmenting Gemini with the search results.

====

Concerned about AI hallucinations? While AI can be a valuable resource, it sometimes generates inaccurate, outdated, or overly general responses - a phenomenon known as "hallucination." This lab teaches you how to implement a Retrieval Augmented Generation (RAG) pipeline to address this issue. RAG improves large language models (LLMs) like Gemini by grounding their output in contextually relevant information from a specific dataset.

Assume you are helping Coffee-on-Wheels, a pioneering mobile coffee vendor, analyze customer feedback on its services. Without access to the latest data, Gemini's responses might be inaccurate. To solve this problem, you decide to build a RAG pipeline that includes three steps:
1 Generate embeddings: Convert customer feedback text into vector embeddings, which are numerical representations of data that capture semantic meaning.
2 Search vector space: Create an index of these vectors, search for similar items, and retrieve them.
3 Generate improved answers: Augment Gemini with the retrieved information to produce more accurate and relevant responses.

BigQuery allows seamless connection to remote generative AI models on Vertex AI. It also provides various functions for embeddings, vector search, and text generation directly through SQL queries or Python notebooks.




==========================================


Task 1. Create a source connection and grant IAM permissions
Create a source connection
To use remote generative AI models on Vertex AI in BigQuery, like Gemini and an embedding model, create a new external source connection.

In the Google Cloud console, on the Navigation menu (Navigation menu icon), click BigQuery.
Navigate to Explorer, click + Add, and select Connections to external data sources.

[like mine]
Note: Alternatively, if you do not see the option for + Add followed by Connections to external data sources, you can click + Add data, and then use the search bar for data sources to search for Vertex AI. Click on the result for Vertex AI > BigQuery Federation.


In the Connection type dropdown, select Vertex AI remote models, remote functions and BigLake (Cloud Resource).
In the Connection ID field, enter 'embedding_conn'.
Click Create connection.
Once the connection is created, click on Go to connection in the pop-up confirmation to navigate to the connection and copy the Service account id value. You need it later to assign permissions to this account.

Connection ID		projects/qwiklabs-gcp-02-e6470c0d9269/locations/us/connections/embedding_conn
Friendly name
Created			Oct 21, 2025, 2:31:37 PM UTC+8
Last modified		Oct 21, 2025, 2:31:37 PM UTC+8
Data location		us
Description
Connection type		Vertex AI remote models, remote functions, BigLake and Spanner (Cloud Resource)
Service account id	bqcx-184356814413-wu5c@gcp-sa-bigquery-condel.iam.gserviceaccount.com







Grant IAM permissions
To use BigQuery data and Vertex AI resources, grant the service account the necessary IAM permissions.

Next, you need to grant permissions via IAM. Perform the steps that follow:
In the Google Cloud console, on the Navigation menu (Navigation menu icon), navigate to IAM & Admin > IAM.
Click on Grant access.
In the Add principals section:
In the New principals text field, paste the Service account id value that you copied earlier.
Under Assign Role, select the following roles (search for them if you need to):
- BigQuery Data Owner - full access to datasets and contents
- Vertex AI User

Click Save to apply the changes.
Navigate to APIs and Services from the Navigation menu (Navigation menu icon), click + Enable APIs and services, search 'Vertex AI API', click the Enable button.






==========================================

Task 2. Generate embeddings
In the Google Cloud console, on the Navigation menu (Navigation menu icon), navigate to BigQuery.

In Explorer, navigate to the three dots besides the project, click 'Create dataset'. For Dataset ID, enter 'CustomerReview'. Keep the other option by default, and click Create dataset.

To connect to the embedding model, run the following SQL query in the query editor:
```
CREATE OR REPLACE MODEL `CustomerReview.Embeddings`
REMOTE WITH CONNECTION `us.embedding_conn`
OPTIONS (ENDPOINT = 'text-embedding-005');
```


To upload the dataset from a CSV file, run the following SQL query:
```
LOAD DATA OVERWRITE CustomerReview.customer_reviews
(
    customer_review_id INT64,
    customer_id INT64,
    location_id INT64,
    review_datetime DATETIME,
    review_text STRING,
    social_media_source STRING,
    social_media_handle STRING
)
FROM FILES (
    format = 'CSV',
    uris = ['gs://spls/gsp1249/customer_reviews.csv']
);
```

(optional) To check the uploaded data in the table, click Go to table. Find the schema of the table and preview the data.

To generate embeddings from recent customer feedback and store them in a table, run the following SQL query in the query editor:
```
CREATE OR REPLACE TABLE `CustomerReview.customer_reviews_embedded` AS
SELECT *
FROM ML.GENERATE_EMBEDDING(
    MODEL `CustomerReview.Embeddings`,
    (SELECT review_text AS content FROM `CustomerReview.customer_reviews`)
);
```
(Optional) To examine the embedding results, click Go to table. Find the schema of the table and preview the data. Note that the embedding results are floating-point numbers and may not be immediately interpretable.







==========================================



Task 3. Search the vector space and retrieve the similar items

To create an index of the vector search space, run the following SQL query:
Note: For datasets with fewer than 5,000 rows, as in this lab, creating an index is unnecessary. This step demonstrates the code required to create a vector space index when needed for larger datasets.
```
CREATE OR REPLACE VECTOR INDEX `CustomerReview.reviews_index`
ON `CustomerReview.customer_reviews_embedded`(ml_generate_embedding_result)
OPTIONS (distance_type = 'COSINE', index_type = 'IVF');
```
-- Total rows 50 is smaller than min allowed 5000 for CREATE VECTOR INDEX query with the IVF index type. Please use VECTOR_SEARCH table-valued function directly to perform the similarity search.




To search the vector space and retrieve the similar items, run the following SQL query:
```
CREATE OR REPLACE TABLE `CustomerReview.vector_search_result` AS
SELECT
    query.query,
    base.content
FROM
    VECTOR_SEARCH(
        TABLE `CustomerReview.customer_reviews_embedded`,
        'ml_generate_embedding_result',
        (
            SELECT
                ml_generate_embedding_result,
                content AS query
            FROM
                ML.GENERATE_EMBEDDING(
                    MODEL `CustomerReview.Embeddings`,
                    (SELECT 'service' AS content)     -- we want to search about 'service'
                )
        ),
        top_k => 5,
        options => '{"fraction_lists_to_search": 0.01}'
    );
```

(Optional) To check the query results, click Go to table. Find the schema of the table and preview the data.







==========================================



Task 4. Generate an improved answer
To connect to the Gemini model, run the following SQL query:
```
CREATE OR REPLACE MODEL `CustomerReview.Gemini`
REMOTE WITH CONNECTION `us.embedding_conn`
OPTIONS (ENDPOINT = 'gemini-2.0-flash');
```


To enhance Gemini's responses, provide it with relevant and recent data retrieved from the vector search by running the following query:
```
SELECT
    ml_generate_text_llm_result AS generated
FROM
    ML.GENERATE_TEXT(
        MODEL `CustomerReview.Gemini`,
        (
            SELECT
                CONCAT(
                    'Summarize what customers think about our services',
                    STRING_AGG(FORMAT('review text: %s', base.content), ',\n')
                ) AS prompt
            FROM
                `CustomerReview.vector_search_result` AS base
        ),
        STRUCT(
            0.4 AS temperature,
            300 AS max_output_tokens,
            0.5 AS top_p,
            5 AS top_k,
            TRUE AS flatten_json_output
        )
    );
```




Check the Gemini-generated results in the Query results section below the query editor.







Explore these questions with any remaining lab time. Good luck!
Questions for you:
1. How do you determine whether Gemini generates better answers with RAG than without it? Try testing it with code.
```
-- Without RAG: Summarize based on general knowledge
SELECT
    ml_generate_text_llm_result AS generated_summary_without_rag
FROM
    ML.GENERATE_TEXT(
        MODEL `CustomerReview.Gemini`,
        (
            SELECT 'Summarize what customers think about our services' AS prompt
        ),
        STRUCT(
            0.4 AS temperature,
            300 AS max_output_tokens,
            0.5 AS top_p,
            5 AS top_k,
            TRUE AS flatten_json_output
        )
    );
```
generated_summary_with_rag	
(Specific, data-driven summary of your reviews)	
vs
generated_summary_without_rag
(Generic, high-level summary about customer service in general)
(OR MODEL PROMPTS US FOR THE CONTEXT)





2.How can the code be improved? For example, instead of saving vector search results to a table (Task 3), could that process be embedded directly into answer generation (Task 4) for real-time retrieval?
```
-- Improved Query: Combines Vector Search and Text Generation in one step
WITH VectorSearchResults AS (
  -- Step 1: Perform the vector search in-memory (as a CTE)
  SELECT
    base.content,
    query.query
  FROM
    VECTOR_SEARCH(
      TABLE `CustomerReview.customer_reviews_embedded`,
      'ml_generate_embedding_result',
      (
        -- This subquery generates the embedding for the user's query in real-time
        SELECT
          ml_generate_embedding_result,
          content AS query
        FROM
          ML.GENERATE_EMBEDDING(
            MODEL `CustomerReview.Embeddings`,
            (SELECT 'service' AS content) -- <-- Your dynamic query goes here
          )
      ),
      top_k => 5,
      options => '{"fraction_lists_to_search": 0.01}'
    )
)
-- Step 2: Use the search results directly in the prompt for the LLM
SELECT
  ml_generate_text_llm_result AS generated_summary
FROM
  ML.GENERATE_TEXT(
    MODEL `CustomerReview.Gemini`,
    (
      SELECT
        CONCAT(
          'Summarize what customers think about our services based on these reviews: ',
          STRING_AGG(FORMAT('review text: %s', content), ',\n')
        ) AS prompt
      FROM
        VectorSearchResults
    ),
    STRUCT(
      0.4 AS temperature,
      300 AS max_output_tokens,
      0.5 AS top_p,
      5 AS top_k,
      TRUE AS flatten_json_output
    )
  );


```
Advantages of This Improved Approach
1 Real-Time & Dynamic: This is the biggest advantage. The entire process runs on-demand. If you change the search query from 'service' to 'food quality', the query will instantly retrieve the relevant documents and generate a new summary without needing to update any intermediate tables. This is essential for interactive applications like chatbots or live dashboards.
2 Efficiency: You are no longer performing a write operation (CREATE OR REPLACE TABLE), which saves processing time and cost. The search results are held in memory just long enough to be passed to the ML.GENERATE_TEXT function.
3 Simplicity & Maintainability: The logic is contained within a single query. This makes the workflow easier to understand, manage, and integrate into other systems (like a BI tool, a Cloud Function, or an application backend). There are fewer database objects to keep track of.
4 Statelessness: The process doesn't create a persistent state (the vector_search_result table). This is a cleaner design, as you don't have to worry about stale or outdated search results in your table. Every run is completely fresh.




So the CTE exists only in the SQL session, and is not commited to a table?
- Yes, that is exactly correct. You have perfectly understood the concept and its advantage.
- A Common Table Expression (CTE), defined with the WITH clause, is a temporary, named result set that exists only for the duration of a single query execution.

Let's break that down:
- It's Not Committed to a Table: Unlike CREATE OR REPLACE TABLE, a CTE does not write any data to persistent storage. It doesn't create a new table in your BigQuery dataset. It is a purely logical construct that exists in memory while the query is being planned and executed.
- Its Scope is a Single Statement: The CTE (VectorSearchResults in our example) is defined at the beginning of the statement and can be referenced one or more times within the main query that follows it (e.g., the SELECT FROM ML.GENERATE_TEXT(...) part). As soon as that single statement finishes running, the CTE and its results vanish completely.

Think of it like a variable in a programming script:
- CREATE OR REPLACE TABLE is like saving data to a file on your hard drive (results.csv). It stays there after the script finishes.
- A CTE is like creating a variable within a function (results = perform_search()). The variable holds the data, you use it within that function, and once the function returns, the variable is cleared from memory.
- This is precisely why the CTE approach is ideal for real-time RAG: it's a lightweight, efficient, and stateless way to "pipe" the results of your retrieval step directly into your generation step without the overhead of writing to and reading from disk.


