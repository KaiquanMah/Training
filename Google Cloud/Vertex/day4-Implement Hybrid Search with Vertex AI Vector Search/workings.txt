Objectives
- Install and explore the basic tools needed for hybrid search.
- Convert product information into two types of search-friendly formats:
- Dense embeddings (for meaning-based search)
- Sparse embeddings (for keyword-based search)
- Create a hybrid search index that supports both dense and sparse embeddings.
- Explore how combining both methods can improve search accuracy and relevance.





Moving beyond simple keyword matching to understanding search intent and the nuanced meanings behind customer queries.
In this lab, you create a hybrid index for both semantic and keyword search in Vertex AI Vector Search, and run hybrid queries against it for improved search quality.



This lab uses the following Google Cloud services:
- Vertex AI
- Vertex AI Workbench
- Vertex Search
- Text embeddings API
Note: The final step in Task 2 starts a process that can take more than 30 minutes to complete. You might prefer to review the following explanation while that step is running.




Why does hybrid search matter?
As described in the Overview of Vector Search, semantic search with Vector Search can find items with semantic similarity by using queries.
https://cloud.google.com/vertex-ai/docs/vector-search/overview

Embedding models, such as Vertex AI embeddings, build a vector space as a map of content meanings. Each text or multimodal embedding is a location in the map that represents the meaning of some content. As a simplified example, when an embedding model takes a text that discusses movies for 10%, music for 2%, and actors for 30%, it could represent this text with an embedding [0.1, 0.02, 0.3].

With Vector Search, you can quickly find other embeddings in its neighborhood. This searching by content meaning is called semantic search.
https://cdn.qwiklabs.com/wxVygQXmw1eCy2icFW5UxaZ8hpMhya0hRvLkZS8qbWA%3D


Semantic search with embeddings and vector search can help make IT systems as smart as experienced librarians or shop staff. Embeddings can be used for tying different business data with their meanings; for example, queries and search results, texts and images, user activities and recommended products, English texts and Japanese texts, or sensor data and alerting conditions. With this capability, there's a wide variety of use cases for embeddings.





Why combine semantic search with keyword-based search?
Semantic search doesn't cover all the possible requirements for information retrieval applications, such as Retrieval-Augmented Generation (RAG). Semantic search can only find data that the embedding model can make sense of.

For example, 
- queries or datasets with ARBITRARY product numbers or SKUs, 
- brand NEW product names that were added recently, and 
- corporate PROPRIETARY codenames
don't work with semantic search because they aren't included in the training dataset of the embedding model. This is called "out-of-domain" data.

In such cases, you would need to combine semantic search with keyword-based (token-based) search to form a hybrid search. With hybrid search, you can take advantage of both semantic and token-based search to achieve a higher search quality.

One of the most popular hybrid search systems, Google Search, incorporated semantic search in 2015 with its 'RankBrain model', in addition to its 'token-based keyword search algorithm'. With the introduction of hybrid search, Google Search was able to improve the search quality significantly by addressing the two requirements:
- search by meaning and 
- search by keyword.

In the past, building a hybrid search engine was a complex task. Just like with Google Search, you have to build and operate two different kinds of search engines (semantic search and token-based search) and merge and rank the results from each. 

However, with hybrid search support in Vector Search, you can build your hybrid search system with 1 single Vector Search index that's customized to your business requirements.











How token-based search works
How does token-based search in Vector Search work? After splitting the text into tokens (such as words or sub-words) https://huggingface.co/docs/transformers/en/tokenizer_summary, you can use popular sparse embedding algorithms such as TF-IDF, BM25, or SPLADE to generate a sparse embedding for the text.
https://en.wikipedia.org/wiki/Tf%E2%80%93idf
https://en.wikipedia.org/wiki/Okapi_BM25
https://en.wikipedia.org/wiki/Learned_sparse_retrieval


A simplified explanation of sparse embeddings is they are vectors that represent how many times each word or sub-word appears in the text. Typical sparse embeddings don't take the semantics of the text into account.

Sparse embeddings panel
https://cdn.qwiklabs.com/MHXff9t%2Fs%2B6tzNe6iQndF2Vaez7wO%2BxCXFH0G5OWiZQ%3D


There could be thousands of different words used in texts. Thus, this embedding usually has tens of thousands of dimensions, with only a few dimensions in them having non-zero values. This is why they're called "sparse" embeddings. The majority of their values are zeroes. This sparse embedding space works as a map of keywords, similar to an index of books.

In this sparse embedding space, you can find similar embeddings by looking at the neighborhood of a query embedding. These embeddings are similar in terms of the distribution of the keywords used in their texts.

Token search
https://cdn.qwiklabs.com/KQi8z%2FCur%2FCFHqLzHLVpEmOOVy2yXQHUuARaUhYVPlc%3D

This is the basic mechanism of the token-based search with sparse embeddings.

With hybrid search in Vector Search, you can mix both dense and sparse embeddings into a single vector index and run queries with dense embeddings, sparse embeddings, or both. The result is a combination of semantic search and token-based search results.

Hybrid search also provides shorter query latency compared to a token-based search engine with an inverted index design. Just like vector search for semantic search, each query with dense or sparse embeddings finishes within milliseconds, even with millions or billions of items.










============



Task 1. Prepare the environment in Vertex AI Workbench


Vertex AI > Dashboard
Click Enable all recommended APIs.




Vertex AI Workbench
JupyterLab
Create new notebook > hybrid_search.ipynb




For the remaining steps and tasks, please refer to the notebook

============


Reflections
- Good introduction to hybrid search using keyword/vocab search (with TF-IDF embeddings) and semantic search (using Gemini embedding model)
- This is a better and working lab, compared to the other Qwiklabs in this Day 4 vertex ai series

