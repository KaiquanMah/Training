Python

>Python List pop()
https://www.programiz.com/python-programming/methods/list/pop
list.pop(index)   => pop element at that index
list.pop(df.index('colName'))     => find col index # for a particular col name => pop that colName elemnent from your list of col names
list.pop()        => pop last element


>Python sorted()
https://www.programiz.com/python-programming/methods/built-in/sorted
Sort list, dictionary
sorted(dict.items(), key = lambda x: x[1])                          Sort on the value for each key-value pair in the dictionary => ascending order
sorted_dict = dict(sorted(dict.items(), key = lambda x: x[1]))      => assign to a 'new dict'



>.format  show dp
https://docs.python.org/3/tutorial/inputoutput.html
value = 13456.23456789
print("a is : {:,.5f}".format(value))
# Output: a is : 13,456.23457             => , comma for comma before the decimal point
                                          => 5f for number of digits behind decimal point; rounded up/down
                                          
value = 13456.23456489
print("a is : {:,.5f}".format(value))
# Output: a is : 13,456.23456








========================================================================

Pandas

>display # rows, cols in Jupyter notebook
pd.set_option("display.max_rows", #)
pd.set_option("display.max_columns", #)
pd.set_option("display.width", #)




>pandas.DataFrame.describe
https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html
df.describe(include = "all")              describe FOR ALL COLS in the df => min 25% 50% 75% max, count, unique count




>show percentage of NA values in df cols => then sort in descending order
percent = df.isnull().sum() / len(df) * 100     => percentage of NA values in df cols
percent_df = pd.DataFrame( {'col_name' : df.columns,
                            'percent_NA' : percent} )
percent_df.sort_values('percent_NA',
                       ascending = FALSE,
                       inplace = TRUE)



>value_counts
df.colName.value_counts(dropna = FALSE)




>pandas.DatetimeIndex
https://pandas.pydata.org/docs/reference/api/pandas.DatetimeIndex.html
https://stackoverflow.com/questions/27032052/how-do-i-properly-set-the-datetimeindex-for-a-pandas-datetime-object-in-a-datafr
# df = df.set_index('DatetimeColName')
pd.DatetimeIndex(df['dateColName']).year
pd.DatetimeIndex(df['dateColName']).month
pd.DatetimeIndex(df['dateColName']).day



>get hour, day, month from datetime column
https://pythontic.com/datetime/date/fromisoformat
https://docs.python.org/3/library/datetime.html#datetime.datetime.fromisoformat
from datetime import datetime
hour = pd.Series(map(lambda dt: datetime.fromisoformat(dt).hour,
                     df.loc[:, 'datetimeCol'].values))
df['hour'] = hour
print(datetime.fromisoformat("2022-06-15 17:53:29").year)       # 2022
print(datetime.fromisoformat("2022-06-15 17:53:29").month)       # 6
print(datetime.fromisoformat("2022-06-15 17:53:29").day)       # 15
print(datetime.fromisoformat("2022-06-15 17:53:29").hour)       # 17
print(datetime.fromisoformat("2022-06-15 17:53:29").minute)       # 53
print(datetime.fromisoformat("2022-06-15 17:53:29").second)       # 29





>pandas.tseries.offsets.DateOffset
https://pandas.pydata.org/docs/reference/api/pandas.tseries.offsets.DateOffset.html
# e.g. turn from dateoffsetcol into seconds
from pandas.tseries.offsets import DateOffset
type(DateOffset())
df["dateOffsetCol"].fillna(DateOffset(days = 0,
                                    microseconds = 0,
                                    nanoseconds = 0),
                         inplace = TRUE)
df["dateOffsetCol"] = pd.Series(map(lambda time: int(time.microseconds/1000000),         # micro is 1 x 10^-6 => so we are adjusting back (divide by 10^6) from microseconds to seconds
                             df.loc[:, "dateOffsetCol"].values
                             ))




>pandas.DataFrame.dropna
https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html
df.dropna()                   # Drop the rows where at least one element is missing.
df.dropna(axis='columns')     # Drop the columns where at least one element is missing.
df.dropna(subset = 'specificColName')     # drop rows with NA in specific col/s only



>pandas.DataFrame.mode
https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.mode.html
df.mode(axis='columns', numeric_only=True)               # To compute the mode over columns and not rows, use the axis parameter:
df_X_without_na.fillna(df_X_without_na.mode())           # 2 dataframes => start with 1 df with NA, 
                                                         # => then (1) dropna from Y/label col
                                                         # => (2) impute categorical cols' NA values with something else, e.g. Unknown, None
                                                         # now our intermediate df only has NAs in numeric X/feature cols
                                                         # => then in the numeric X/feature cols => fill NA values with the mode
                                                         # now our final df no longer has NAs in Y col (dropped), categorical X cols (imputed with another label), numeric X cols (imputed with mode)





>rename cols
df.rename(columns = {"origName1" : "newName1", ...},
          inplace = TRUE)





>.agg Aggregation and multiindex
https://pandas.pydata.org/docs/user_guide/advanced.html
df.groupby(['primaryKey']).agg({
                              "categoricalColName" : "count",
                              "numericColName1" : "sum",
                              "numericColName2" : "min",
                              "numericColName3" : [min, max, "mean", "median", "std", sum],         # supply a list of operations to perform on a col
                              "dateColName" : [min, max, "nunique"]
                              }).reset_index()
=> you will have 2-level/layer column indexes                               
e.g. ("categoricalColName", "count)
     ("numericColName3", "min")
     ("numericColName3", "max")




========================================================================

Sklearn

>sklearn.model_selection.train_test_split
https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html
https://stackoverflow.com/questions/34842405/parameter-stratify-from-method-train-test-split-scikit-learn

#stratify => SAME RELATIVE PERCENTAGE between (e.g. categorical/numeric) values in train and test set of Y
stratify parameter makes a split so that the proportion of values in the sample produced 
will be the same as the proportion of values provided to parameter stratify.
For example, if variable y is a binary categorical variable with values 0 and 1 
and there are 25% of zeros and 75% of ones, 
stratify=y will make sure that your random split has 25% of 0's and 75% of 1's.

X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    test_size = 0.3,
                                                    random_state,
                                                    stratify = y)



>sklearn.model_selection.StratifiedKFold
https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html











>sklearn.preprocessing.OrdinalEncoder
https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html
OrdinalEncoder().fit_transform(df[["categoricalCol1", ...]])



>sklearn.preprocessing.OneHotEncoder
https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html
OneHotEncoder(handle_unknown = 'ignore', sparse = FALSE)            # Sparse => return sparse matrix if set to True, else will return an array.
                                                                    # Handle_unknown => ‘ignore’ : When an unknown category is encountered during transform, the resulting one-hot encoded columns for this feature will be all zeros.
df_onehot_cols = pd.DataFrame( onehot_encoder.fit_transform(df[<variableListOfCategoricalCols>]) )
df_onehot_cols.columns = onehot_encoder.get_feature_names(variableListOfCategoricalCols)







>sklearn.ensemble.RandomForestRegressor
https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html
rf = RandomForestRegressor(max_depth, min_samples_leaf, min_samples_split, n_estimators)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
mean_absolute_error(y_pred, y_test)
print("MAE is: {:,.2f}".format( mean_absolute_error(y_pred, y_test) ))        # print MAE with 2 dp





>sklearn.cluster.KMeans clustering
https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html
kmeans = KMeans(n_clusters, random_state)
kmeans.fit_predict(df)

#Check optimal number of clusters
from matplotlib import pyplot as plt
list_sum_squared_distance = []
K = range(1, 11)      # list of integers 1 to 10

for n_cluster in K:
  kmeans = KMeans(n_clusters = k, random_state)
  kmeans.fit_predict(df)
  list_sum_squared_distance.append(kmeans.inertia_)

plt.plot(x = K, y = list_sum_squared_distance, '.-')          # . point marker
                                                              # - solid line style
                                                              # https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html
plt.xlabel('Number of clusters')
plt.ylabel('Sum of squared distance')
plt.title('Find optimal number of clusters - Elbow method')
plt.show()

# identify cluster centers
cluster_center_coordinates = pd.DataFrame(kmeans.cluster_centers_,
                                          columns = df.columns)
=> e.g. there were 3 clusters
      col1      col2      col3
0      #         #         #
1      #         #         #
2      #         #         #



========================================================================

lightgbm

>lightgbm.LGBMRegressor
https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html
https://www.programcreek.com/python/example/88794/lightgbm.LGBMRegressor
lgbm_model = LGBMRegressor(n_estimators, learning_rate = 0.001)
lgbm_model.fit(X_train, y_train)
y_pred = lgbm_model.predict(X_test)
mean_absolute_error(y_pred, y_test)
print("MAE is: {:,.2f}".format( mean_absolute_error(y_pred, y_test) ))        # print MAE with 2 dp










========================================================================
XGBoost

>xgboost.XGBRegressor
https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor
xgb = XGBRegressor(n_estimators, learning_rate)
xgb.fit(X_train, y_train)
y_pred = xgb.predict(X_test)
mean_absolute_error(y_pred, y_test)
print("MAE is: {:,.2f}".format( mean_absolute_error(y_pred, y_test) ))        # print MAE with 2 dp









===============================================================
Combine outputs from multiple models

df = df.DataFrame('model1': model1_prediction, 'model2': model2_prediction)
weighted_prediction = df.model1 * 0.3 + df.model2 * 0.7           30% to model 1, 70% to model 2







========================================================================

Matplotlib

plt.rcParams['figure.figsize'] = [15.0, 8.0]






========================================================================

Seaborn

# https://www.statology.org/seaborn-font-size/
# https://seaborn.pydata.org/generated/seaborn.plotting_context.html
sns.set(font_scale = 3)       

# plot 2x3 grid
plt.subplots(figsize = (30, 20))
plt.subplot(2,3,1)
sns.lineplot(...);
plt.subplot(2,3,2)
...
plt.subplot(2,3,6)
sns.lineplot(...)
plt.show()
# goes from left to right cols in row 1
# then move to the next row, and repeat



>seaborn.boxplot
https://seaborn.pydata.org/generated/seaborn.boxplot.html
sns.boxplot(x, 
            y, 
            data = df,
            showfliers = TRUE,                # showfliers = show outliers
                                              # https://www.mikulskibartosz.name/how-to-remove-outliers-from-seaborn-boxplot-charts/
            palette = <list of colour schemes>,
            order = [2, 1, 0]
            )





>horizontal (left to right) seaborn.countplot
https://seaborn.pydata.org/generated/seaborn.countplot.html
ax = sns.countplot(y = 'categoricalCol',
                   data = df,
                   palette = <list of colour schemes>)
ax.bar_label(ax.containers[0])      # add count of each categorical 'col' in the plot, to the right of each bar
                                    # https://stackoverflow.com/questions/69810130/seaborn-display-values-on-barplot
                                    # seaborn countplot ".containers"
ax.set(xlabel = "count", 
       ylabel = "categories")
plt.show()






 


>seaborn.pairplot
https://seaborn.pydata.org/generated/seaborn.pairplot.html
https://pythonbasics.org/seaborn-pairplot/
sns.pairplot(data = df,
             hue = 'categoricalColName')      # different colour for different category
plt.show()










>seaborn.heatmap, correlation matrix
https://seaborn.pydata.org/generated/seaborn.heatmap.html
https://towardsdatascience.com/clearly-explained-pearson-v-s-spearman-correlation-coefficient-ada2f473b8#:~:text=The%20fundamental%20difference%20between%20the,with%20monotonic%20relationships%20as%20well.
fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (30, 15))        # 1x2 grid
sns.heatmap(df.corr(method = "pearson/spearman"),
            vmin = -1,                              # vmin optional
                                                    # Values to anchor the colormap, otherwise they are inferred from the data and other keyword arguments.
                                                    # over here, vmin = -1 => uses the last col of the df as the 'anchor' for intensity of colour
            cmap = 'coolwarm',
            annot = TRUE,
            ax = ax1/ax2)
plt.tight_layout()
plt.show()

# Correlation
# Pearson => linear correlation between two variables X and Y. It has a value between +1 and −1
# Spearman => assesses how well the relationship between two variables can be described using a monotonic function
           => A monotonic relationship is a relationship that does one of the following:
               => (1) as the value of one variable increases, so does the value of the other variable, OR,
               => (2) as the value of one variable increases, the other variable value decreases.
               => BUT, NOT EXACTLY AT A CONSTANT RATE like in a linear relationship the rate of increase/decrease is constant.
               => i.e. NON LINEAR upward/downward sloping RELATIONSHIP








=================================================

Reduce bias in dataset

>imblearn.under_sampling.RandomUnderSampler
https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html
Under-sample the majority class(es) by randomly picking samples with or without replacement.    => consideration: you will lose those 'extra' observations you are 'throwing away'








