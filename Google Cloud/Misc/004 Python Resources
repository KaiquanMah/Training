Python

>Python List pop()
https://www.programiz.com/python-programming/methods/list/pop
list.pop(index)   => pop element at that index
list.pop(df.index('colName'))     => find col index # for a particular col name => pop that colName elemnent from your list of col names
list.pop()        => pop last element


>Python sorted()
https://www.programiz.com/python-programming/methods/built-in/sorted
Sort list, dictionary
sorted(dict.items(), key = lambda x: x[1])                          Sort on the value for each key-value pair in the dictionary => ascending order
sorted_dict = dict(sorted(dict.items(), key = lambda x: x[1]))      => assign to a 'new dict'



>.format  show dp
https://docs.python.org/3/tutorial/inputoutput.html
value = 13456.23456789
print("a is : {:,.5f}".format(value))
# Output: a is : 13,456.23457             => , comma for comma before the decimal point
                                          => 5f for number of digits behind decimal point; rounded up/down
                                          
value = 13456.23456489
print("a is : {:,.5f}".format(value))
# Output: a is : 13,456.23456











========================================================================

Pandas

>display # rows, cols in Jupyter notebook
pd.set_option("display.max_rows", #)
pd.set_option("display.max_columns", #)
pd.set_option("display.width", #)




>pandas.DataFrame.describe
https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html
df.describe(include = "all")              describe FOR ALL COLS in the df => min 25% 50% 75% max, count, unique count




>show percentage of NA values in df cols => then sort in descending order
percent = df.isnull().sum() / len(df) * 100     => percentage of NA values in df cols
percent_df = pd.DataFrame( {'col_name' : df.columns,
                            'percent_NA' : percent} )
percent_df.sort_values('percent_NA',
                       ascending = FALSE,
                       inplace = TRUE)



>value_counts
df.colName.value_counts(dropna = FALSE)



>pandas.DataFrame.sort_values
https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html
df.sort_values(by = ['col1', 'col2'])





>pandas.DataFrame.set_index
https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.set_index.html
df.set_index('dateCol')
df.set_index(['col1', 'col2'])      # multi-index, e.g. a 2-layer/level index





>pandas.DatetimeIndex
https://pandas.pydata.org/docs/reference/api/pandas.DatetimeIndex.html
https://stackoverflow.com/questions/27032052/how-do-i-properly-set-the-datetimeindex-for-a-pandas-datetime-object-in-a-datafr
# df = df.set_index('DatetimeColName')
pd.DatetimeIndex(df['dateColName']).year
pd.DatetimeIndex(df['dateColName']).month
pd.DatetimeIndex(df['dateColName']).day





>get hour, day, month from datetime column
https://pythontic.com/datetime/date/fromisoformat
https://docs.python.org/3/library/datetime.html#datetime.datetime.fromisoformat
from datetime import datetime
hour = pd.Series(map(lambda dt: datetime.fromisoformat(dt).hour,
                     df.loc[:, 'datetimeCol'].values))
df['hour'] = hour
print(datetime.fromisoformat("2022-06-15 17:53:29").year)       # 2022
print(datetime.fromisoformat("2022-06-15 17:53:29").month)       # 6
print(datetime.fromisoformat("2022-06-15 17:53:29").day)       # 15
print(datetime.fromisoformat("2022-06-15 17:53:29").hour)       # 17
print(datetime.fromisoformat("2022-06-15 17:53:29").minute)       # 53
print(datetime.fromisoformat("2022-06-15 17:53:29").second)       # 29





>pandas.tseries.offsets.DateOffset
https://pandas.pydata.org/docs/reference/api/pandas.tseries.offsets.DateOffset.html
# e.g. turn from dateoffsetcol into seconds
from pandas.tseries.offsets import DateOffset
type(DateOffset())
df["dateOffsetCol"].fillna(DateOffset(days = 0,
                                    microseconds = 0,
                                    nanoseconds = 0),
                         inplace = TRUE)
df["dateOffsetCol"] = pd.Series(map(lambda time: int(time.microseconds/1000000),         # micro is 1 x 10^-6 => so we are adjusting back (divide by 10^6) from microseconds to seconds
                             df.loc[:, "dateOffsetCol"].values
                             ))




>pandas.DataFrame.dropna
https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html
df.dropna()                   # Drop the rows where at least one element is missing.
df.dropna(axis='columns')     # Drop the columns where at least one element is missing.
df.dropna(subset = 'specificColName')     # drop rows with NA in specific col/s only



>pandas.DataFrame.mode
https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.mode.html
df.mode(axis='columns', numeric_only=True)               # To compute the mode over columns and not rows, use the axis parameter:
df_X_without_na.fillna(df_X_without_na.mode())           # 2 dataframes => start with 1 df with NA, 
                                                         # => then (1) dropna from Y/label col
                                                         # => (2) impute categorical cols' NA values with something else, e.g. Unknown, None
                                                         # now our intermediate df only has NAs in numeric X/feature cols
                                                         # => then in the numeric X/feature cols => fill NA values with the mode
                                                         # now our final df no longer has NAs in Y col (dropped), categorical X cols (imputed with another label), numeric X cols (imputed with mode)









>subset/slice dataframe
https://pypi.org/project/dataframe-expressions/
df_subset = df[d.col1 > 10 & d.col2 < 50 | d.col3 == "valueUWant"]








>rename cols
# original and new col name
df.rename(columns = {"origName1" : "newName1", ...},
          inplace = TRUE)
# original col index, new col name
df.rename(index = {0: "newName1", ...},
          inplace = TRUE)




>create binary/dummy variables from categorical cols
df["is_category1"] = df["categoricalCol"] == "category1"
df.is_category1 = df.is_category1.replace({TRUE : 1,
                                           FALSE : 0})








>.agg Aggregation and multiindex
https://pandas.pydata.org/docs/user_guide/advanced.html
df.groupby(['primaryKey']).agg({
                              "categoricalColName" : "count",
                              "numericColName1" : "sum",
                              "numericColName2" : "min",
                              "numericColName3" : [min, max, "mean", "median", "std", sum],         # supply a list of operations to perform on a col
                              "dateColName" : [min, max, "nunique"]
                              }).reset_index()
=> you will have 2-level/layer column indexes                               
e.g. ("categoricalColName", "count)
     ("numericColName3", "min")
     ("numericColName3", "max")











> exponential weighted moving average
https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.ewm.html
https://tedboy.github.io/pandas/computation/computation5.html
https://www.w3resource.com/pandas/series/series-ewm.php
https://www.w3resource.com/pandas/dataframe/dataframe-ewm.php
https://aleksandarhaber.com/exponential-moving-average-in-pandas-and-python/
# 200-day ewma
ewm_df = df['numericCol'].ewm(span = 200, adjust = FALSE).mean()








>correlation between X variables and y
numpy.corrcoef
https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html
np.corrcoef(X, y)





>IQR interquartile range
https://www.codegrepper.com/code-examples/python/how+to+calculate+iqr+in+pandas
q1 = df.quantile(0.25)
q3 = df.quantile(0.75)

iqr = q3 - q1

lower_bound = q1 -(1.5 * iqr) 
upper_bound = q3 +(1.5 * iqr)




========================================================================

Sklearn

>sklearn.model_selection.train_test_split
https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html
https://stackoverflow.com/questions/34842405/parameter-stratify-from-method-train-test-split-scikit-learn

#stratify => SAME RELATIVE PERCENTAGE between (e.g. categorical/numeric) values in train and test set of Y
stratify parameter makes a split so that the proportion of values in the sample produced 
will be the same as the proportion of values provided to parameter stratify.
For example, if variable y is a binary categorical variable with values 0 and 1 
and there are 25% of zeros and 75% of ones, 
stratify=y will make sure that your random split has 25% of 0's and 75% of 1's.

X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    test_size = 0.3,
                                                    random_state,
                                                    stratify = y)



>sklearn.model_selection.StratifiedKFold
https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html






>PowerTransformer for boxcox transformation
sklearn.preprocessing.PowerTransformer
Apply a power transform featurewise to make data more Gaussian-like.
Box-Cox requires input data to be strictly positive, 
while Yeo-Johnson supports both positive or negative data.

https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html
from sklearn.preprocessing import PowerTransformer
pt = PowerTransformer(method = ‘yeo-johnson/box-cox’)
pt.fit(yourDataset)








>Linear Regression
https://stackabuse.com/linear-regression-in-python-with-scikit-learn/
regressor.intercept_
regressor.coef_



>Linear Regression with interaction terms
https://towardsdatascience.com/multiple-linear-regression-with-interactions-unveiled-by-genetic-programming-4cc325ac1b65
import statsmodels.api as sm 
X = sm.add_constant(df[['x1','x2','x3','x4']])

from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(interaction_only = True)      # => const(i.e. x0), x1, ..., x4 and
                                                        # => x0x1, ..., x0x4 and
                                                        # ... x3x4
X_transformed = poly.fit_transform(X)
X_transformed = pd.concat([X, pd.DataFrame(X_transformed,
                                           columns = poly.get_feature_names()
                                           ).drop([‘1’,’x0',’x1',’x2',’x3',’x4'], 1)      # 1 => drop col
                           ], 1)                                                          # 1 => concat col
mod = sm.OLS(y, X)
res = mod.fit()
res.summary()








>sklearn.linear_model.Lasso
Linear Model trained with L1 regularisation (aka the Lasso).
https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html
clf = linear_model.Lasso(alpha = 0.1,
                         fit_intercept = True)
clf.fit(X_train, y_train)
clf.coef_





>sklearn.linear_model.Ridge
https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html
clf = linear_model.Ridge(alpha = 0.1,
                         fit_intercept = True)
clf.fit(X_train, y_train)
clf.coef_








>sklearn.preprocessing.OrdinalEncoder
https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html
OrdinalEncoder().fit_transform(df[["categoricalCol1", ...]])





>sklearn.preprocessing.OneHotEncoder
https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html
OneHotEncoder(handle_unknown = 'ignore', sparse = FALSE)            # Sparse => return sparse matrix if set to True, else will return an array.
                                                                    # Handle_unknown => ‘ignore’ : When an unknown category is encountered during transform, the resulting one-hot encoded columns for this feature will be all zeros.
df_onehot_cols = pd.DataFrame( onehot_encoder.fit_transform(df[<variableListOfCategoricalCols>]) )
df_onehot_cols.columns = onehot_encoder.get_feature_names(variableListOfCategoricalCols)

https://stackoverflow.com/questions/37292872/how-can-i-one-hot-encode-in-python
pd.get_dummies(df['categoricalCol'],
               drop_first = True)         # drop the original column at the same time











>sklearn.linear_model.LogisticRegression
https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html
c_space = np.linspace(0.001, 10, 100)       # from 0.001 up to and including 10; 100 numbers
param_grid = {'C' : c_space,
              'class_weight' : [None, 'balanced']}

logreg_initial = Logisticegression()
# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html
logreg = GridSearchCV(logreg_initial,
                      param_grid,
                      cv = 5)
# tuned parameters and score
logreg.best_params_     
logreg.best_score_

# from tuned params, fit the logistic regression model
logreg = LogisticRegression(C = yourBestC,
                            class_weight = "balanced/None")
logreg.fit(X_train, y_train)
# https://stats.stackexchange.com/questions/354709/sklearn-metrics-accuracy-score-vs-logisticregression-score
logreg.score(X_test, y_test)

#coefficients
logreg.coef_
# probability of 0/no, 1/yes
prob_0 = logreg.predict_proba(X)[:, 0]
prob_1 = logreg.predict_proba(X)[:, 1]
# predicted 0/no, 1/yes classification
y_pred = logreg.predict(X)

#confusion matrix
matrix = confusion_matrix(y, y_pred, labels = [0, 1])   # count
matrix = confusion_matrix(y, y_pred, labels = [0, 1]) / len(y)   # 0.xx percentages
#                 PREDICTED
#                 NEG             POS
# ACTUAL    NEG   TRUE NEG        FALSE POS
#           POS   FALSE NEG       TRUE POS
tn, fp, fn, tp = confusion_matrix(y, y_pred, labels = [0, 1]).reshape(-1)   # 1 line of output
# output: tn#, fp#, fn#, tp#
matrix_report = classification_report(y, y_pred, labels = [0,1])
#           precision       recall        f1-score            support
#    0        0.xx            0.xx          0.xx              # of actual neg labels
#    1        0.xx            0.xx          0.xx              # of actual pos labels

# accuracy                                  0.xx              # actual pos + neg labels















>sklearn.model_selection.GridSearchCV
https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html
clf = GridSearchCV(estimator = yourModel, 
                   param_grid = {'modelParam1' : ('choice1', 'choice2'),
                                 'modelParam2' : np.linspace(start = 0,             # https://numpy.org/doc/stable/reference/generated/numpy.linspace.html
                                                             stop = 1,
                                                             num = 100,             # count of numbers
                                                             endpoint = TRUE)},     #include stop #/endpoint
                    cv = 5,
                    verbose = 0     #>1 : computation time for each fold and parameter candidate is displayed;
                                    #>2 : score is also displayed;
                                    #>3 : the fold and candidate parameter indexes are also displayed together with the starting time of the computation.
                    )











>sklearn.tree.DecisionTreeClassifier
https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html
clf = DecisionTreeClassifier(max_depth,
                             min_samples_split,
                             min_samples_leaf,
                             random_state = 0)
clf.fit(X_train, y_train)
clf.predict(X_test)







>sklearn.ensemble.BaggingClassifier
https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html
clf = BaggingClassifier(base_estimator = yourModel,       # e.g. a Decision Tree Classifier
                        n_estimators = 10,        # # of model/estimator variations in the ensemble model
                        random_state = 0, 
                        verbose = 0)
clf.fit(X_train, y_train)
clf.predict(X_test)











>sklearn.ensemble.RandomForestRegressor
https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html
rf = RandomForestRegressor(max_depth, min_samples_leaf, min_samples_split, n_estimators)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
mean_absolute_error(y_pred, y_test)
print("MAE is: {:,.2f}".format( mean_absolute_error(y_pred, y_test) ))        # print MAE with 2 dp






>sklearn.ensemble.RandomForestClassifier
https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html
clf = RandomForestClassifier(n_estimators,
                             max_depth = 2, 
                             min_samples_split,       # min # samples/observations before splitting at a node
                             min_samples_leaf,        # min # samples/observations for a leaf node to exist
                             max_leaf_nodes,          # keep the best nodes first => based on relative reduction in impurity
                             random_state = 0)
clf.fit(X_train, y_train)
clf.predict(X_test)








>Randomforestclassifier Feature importance
https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html
importances = clf.feature_importances_













>sklearn.cluster.KMeans clustering
https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html
kmeans = KMeans(n_clusters,
                n_init = 10,      # #times to run k-means with different centroid seeds
                random_state)
kmeans.fit_predict(df)

#Check optimal number of clusters
from matplotlib import pyplot as plt
list_sum_squared_distance = []
K = range(1, 11)      # list of integers 1 to 10

for n_cluster in K:
  kmeans = KMeans(n_clusters = k, random_state)
  kmeans.fit_predict(df)
  list_sum_squared_distance.append(kmeans.inertia_)

plt.plot(x = K, y = list_sum_squared_distance, '.-')          # . point marker
                                                              # - solid line style
                                                              # https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html
plt.xlabel('Number of clusters')
plt.ylabel('Sum of squared distance')
plt.title('Find optimal number of clusters - Elbow method')
plt.show()

# identify cluster centers
cluster_center_coordinates = pd.DataFrame(kmeans.cluster_centers_,
                                          columns = df.columns)
=> e.g. there were 3 clusters
      col1      col2      col3
0      #         #         #
1      #         #         #
2      #         #         #












>sklearn.metrics.roc_auc_score
https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html
sklearn.metrics.roc_auc_score(y_test, 
                              yourClasssifier.predict_proba(X)[:, 1])
# multi_class = 'ovr/ovo'     # => ovr one vs rest => 1 class vs the rest
                              # => ovo one vs one  => calculate avg AUC of all possible combo of classes (class1-class2 combo of observations?)







>sklearn.metrics.r2_score
https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html
r2_score(y_test, y_pred)






========================================================================

lightgbm

>lightgbm.LGBMRegressor
https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html
https://www.programcreek.com/python/example/88794/lightgbm.LGBMRegressor
lgbm_model = LGBMRegressor(n_estimators, learning_rate = 0.001)
lgbm_model.fit(X_train, y_train)
y_pred = lgbm_model.predict(X_test)
mean_absolute_error(y_pred, y_test)
print("MAE is: {:,.2f}".format( mean_absolute_error(y_pred, y_test) ))        # print MAE with 2 dp










========================================================================
XGBoost

>xgboost.XGBRegressor
https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor
xgb = XGBRegressor(n_estimators, learning_rate)
xgb.fit(X_train, y_train)
y_pred = xgb.predict(X_test)
mean_absolute_error(y_pred, y_test)
print("MAE is: {:,.2f}".format( mean_absolute_error(y_pred, y_test) ))        # print MAE with 2 dp









===============================================================
Combine outputs from multiple models

df = df.DataFrame('model1': model1_prediction, 'model2': model2_prediction)
weighted_prediction = df.model1 * 0.3 + df.model2 * 0.7           30% to model 1, 70% to model 2







========================================================================

Matplotlib

plt.rcParams['figure.figsize'] = [15.0, 8.0]






========================================================================

Seaborn

# https://www.statology.org/seaborn-font-size/
# https://seaborn.pydata.org/generated/seaborn.plotting_context.html
sns.set(font_scale = 3)       

# plot 2x3 grid
plt.subplots(figsize = (30, 20))
plt.subplot(2,3,1)
sns.lineplot(...);
plt.subplot(2,3,2)
...
plt.subplot(2,3,6)
sns.lineplot(...)
plt.show()
# goes from left to right cols in row 1
# then move to the next row, and repeat









>seaborn.scatterplot
https://seaborn.pydata.org/generated/seaborn.scatterplot.html
sns,scatterplot(data = df,
                x = "col1", 
                y = "col2",
                hue = "categoricalCol3",      #colour
                style = "categoricalCol4")    # circle, 'x'






>seaborn.jointplot
https://seaborn.pydata.org/generated/seaborn.jointplot.html
#e.g. scatterplot with histogram of x/y variable on the sides
sns.jointplot(data = df,
              x = "col1",
              y = "col2",
              hue = "categoricalCol3")      #colour

#kind = "kde"    => kernel density estimate (KDE) plot = visualizing the distribution of observations in a dataset, analagous to a histogram
                 => looks like contour plots in the middle => center of x/y variable histograms will be at the peak of the respective contour






>seaborn.lineplot
https://seaborn.pydata.org/generated/seaborn.lineplot.html
sns.lineplot(data = df,
             x = "col1",
             y = "col2",
             hue = "categoricalCol3")      #colour









>seaborn.kdeplot
density of a variable
https://seaborn.pydata.org/generated/seaborn.kdeplot.html
sns.kdeplot(data = df, 
            x = "col1",
            hue = "categoricalCol", 
            multiple = "stack")           # multiple = "stack" => fill the area under the curve with colour

#cumulative distribution function plot
sns.kdeplot(
    data = df, 
    x = "col1", 
    hue = "categoricalCol",
    cumulative=True,              #CDF
    common_norm=False, 
    common_grid=True,
)












>seaborn.boxplot
https://seaborn.pydata.org/generated/seaborn.boxplot.html
sns.boxplot(x, 
            y, 
            data = df,
            showfliers = TRUE,                # showfliers = show outliers
                                              # https://www.mikulskibartosz.name/how-to-remove-outliers-from-seaborn-boxplot-charts/
            palette = <list of colour schemes>,
            order = [2, 1, 0]
            )









>seaborn.violinplot
https://seaborn.pydata.org/generated/seaborn.violinplot.html
import seaborn as sns
sns.set_theme(style="whitegrid")
ax = sns.violinplot(x = "numericCol",
                    data = df)

ax = sns.violinplot(x = "categoricalCol1",
                    y = "numericCol",
                    hue "categoricalCol2",
                    data = df,
                    split = TRUE,               # e.g. if categoricalCol2 has 2 unique values => left half of each violin will be category 1, right half will be category 2
                    order = ["categoricalCol1_cat2_first","categoricalCol1_cat1_later"],
                    inner = "quartile")         # show quartile lines in each violin









>horizontal (left to right) seaborn.countplot
https://seaborn.pydata.org/generated/seaborn.countplot.html
ax = sns.countplot(y = 'categoricalCol',
                   data = df,
                   palette = <list of colour schemes>)
ax.bar_label(ax.containers[0])      # add count of each categorical 'col' in the plot, to the right of each bar
                                    # https://stackoverflow.com/questions/69810130/seaborn-display-values-on-barplot
                                    # seaborn countplot ".containers"
ax.set(xlabel = "count", 
       ylabel = "categories")
plt.show()






 


>seaborn.pairplot
https://seaborn.pydata.org/generated/seaborn.pairplot.html
https://pythonbasics.org/seaborn-pairplot/
sns.pairplot(data = df,
             hue = 'categoricalColName')      # different colour for different category
plt.show()
# variables on x-axis, y-axis
# separate plots => 1 plot per variable combo e.g. variable 1 with variable 1/2/3/4
# variable 1 with variable 1 => show a distribution
# variable 1 with variable 2/3/4 => show scatterplot









>seaborn.heatmap, correlation matrix
https://seaborn.pydata.org/generated/seaborn.heatmap.html
https://towardsdatascience.com/clearly-explained-pearson-v-s-spearman-correlation-coefficient-ada2f473b8#:~:text=The%20fundamental%20difference%20between%20the,with%20monotonic%20relationships%20as%20well.
fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (30, 15))        # 1x2 grid
sns.heatmap(df.corr(method = "pearson/spearman"),
            vmin = -1,                              # vmin optional
                                                    # Values to anchor the colormap, otherwise they are inferred from the data and other keyword arguments.
                                                    # over here, vmin = -1 => uses the last col of the df as the 'anchor' for intensity of colour
            cmap = 'coolwarm',
            annot = TRUE,
            ax = ax1/ax2)
plt.tight_layout()
plt.show()

# Correlation
# Pearson => linear correlation between two variables X and Y. It has a value between +1 and −1
# Spearman => assesses how well the relationship between two variables can be described using a monotonic function
           => A monotonic relationship is a relationship that does one of the following:
               => (1) as the value of one variable increases, so does the value of the other variable, OR,
               => (2) as the value of one variable increases, the other variable value decreases.
               => BUT, NOT EXACTLY AT A CONSTANT RATE like in a linear relationship the rate of increase/decrease is constant.
               => i.e. NON LINEAR upward/downward sloping RELATIONSHIP










=================================================

Statsmodels, stats stuff


>statsmodels.distributions.empirical_distribution.ECDF
https://www.statsmodels.org/devel/generated/statsmodels.distributions.empirical_distribution.ECDF.html
https://machinelearningmastery.com/empirical-distribution-function-in-python/
https://stackoverflow.com/questions/3209362/how-to-plot-empirical-cdf-ecdf

Empirical Distribution Function, or EDF = Empirical Cumulative Distribution Function, or ECDF
-order all of the unique observations in the data sample
-calculate the cumulative probability for each unique observation => as the number of observations less than or equal to a given observation divided by the total number of observations.
-EDF(x) = number of observations <= x / nT

from statsmodels.distributions.empirical_distribution import ECDF
ECDF(<list of numbers>)
ECDF([3, 2, 1, 6])









> numpy.random.choice
https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html
random.choice(yourArray/integer,    # e.g. ['item1', ...]
              size = None, 
              replace = True,       # sample with (by default)/without replacement
              p = None)             # default assumes a uniform distribution for each value in input array, 
                                    # otherwise use the probabilities u provide


np.random.choice(5,                 # np.arange(5)
                                    # https://numpy.org/doc/stable/reference/generated/numpy.arange.html
                                    # start from 0, go up to and not including stop # '5'
                 size = 3)          # select 3 elements
# array([0, 3, 4]) # random


#Generate a non-uniform random sample from np.arange(5) of size 3:
np.random.choice(5, 
                 size = 3, 
                 p = [0.1, 0, 0.3, 0.6, 0])
# array([3, 3, 0]) # random






>numpy.random.sample
https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.sample.html
https://numpy.org/doc/stable/reference/random/generated/numpy.random.random_sample.html#numpy.random.random_sample
# Return random floats in the half-open interval [0.0, 1.0)     # => from 0.0, up to and not including 1.0
np.random.random_sample()       # 0.35

np.random.random_sample(size = (5,))
# array([ 0.30220482,  0.86820401,  0.1654503 ,  0.11659149,  0.54323428])

np.random.random_sample((3, 2))         # 3 rows, 2 cols
array([[-3.99149989, -0.52338984],
       [-2.99091858, -0.79479508],
       [-1.23204345, -1.75224494]])












>scipy.stats.norm
https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html
A normal continuous random variable








>scipy.stats.norm => create normal distribution
https://www.alphacodingskills.com/scipy/scipy-normal-distribution.php
#scipy.stats.norm.pdf(x, loc=0, scale=1)
#scipy.stats.norm.cdf(x, loc=0, scale=1)
#scipy.stats.norm.ppf(q, loc=0, scale=1)
#scipy.stats.norm.rvs(loc=0, scale=1, size=1)       # size = output shape

# probability density function (pdf)
x = np.arange(-10, 10, 0.1)
y = norm.pdf(x, 
             loc = 0,         # mean of the distribution
             scale = 2)       # SD; non-negative
plt.plot(x, y) 
plt.show()


# cumulative distribution function (cdf)
x = np.arange(-10, 10, 0.1)
y = norm.cdf(x, 
             loc = 0,         # mean of the distribution
             scale = 2)       # SD; non-negative
plt.plot(x, y) 
plt.show()


# norm.rvs
# generates an array containing specified number of random numbers of the given normal distribution
np.random.seed(8)
y = norm.rvs(loc = 0,             # mean
             scale = 1,           # sd
             shape = 10000)       # # observations
#create bins
bins = np.arange(-4,4,0.1)  
plt.hist(y, bins = bins) 
plt.show()
#histogram of normally distributed random variables generated















>binomial distribution
scipy.stats.binom
https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binom.html
https://www.alphacodingskills.com/scipy/scipy-binomial-distribution.php

scipy.stats.binom.pmf(k, n, p, loc=0)       # k	Required. Specify float or array_like of floats representing random variable.
                                            # n	Required. Specify number of trials, must be >= 0. Floats are also accepted, but they will be truncated to integers.
                                            # p	Required. Specify probability of success in each trial, must be in range [0, 1]. float or array_like of floats.
                                            # loc	Optional. Specify the mean/location of the distribution. Default is 0.
scipy.stats.binom.cdf(k, n, p, loc=0)
scipy.stats.binom.ppf(q, n, p, loc=0)       # q	Required. Specify float or array_like of floats representing probabilities.
scipy.stats.binom.rvs(n, p, loc=0, size=1)  # size	Optional. Specify output shape.

# probability mass function (pmf) 
x = np.arange(0, 20, 1)
y = binom.pmf(x, 
              20,             # 20 trials
              0.5)            # 50% chance of success
plt.plot(x, y, 'o')           # plot circles denoting points for the PMF
plt.show()

# cumulative distribution function (cdf) 
x = np.arange(0, 20, 1)
y = binom.cdf(x, 
              20,             # 20 trials
              0.5)            # 50% chance of success
plt.plot(x, y, 'o')           # plot circles denoting points for the PMF
plt.show()

# binom.rvs
# enerates an array containing the specified number of random values drawn from the given binomial distribution
np.random.seed(10)
#creating a vector containing 10,000
#random values from binomial distribution
y = binom.rvs(20,             # 20 trials
              0.5,            # 50% chance of success
              loc = 10,       # mean = 10
              10000)          # 10,000 observations
#creating bins
bins = np.arange(0,25,1)  
plt.hist(y, bins = bins) 
plt.show()
#histogram of random variables generated
















>scipy stats create hypothesis
scipy.stats.ttest_ind
https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html
stats.ttest_ind(random_variables_arr1, 
                random_variables_arr2,
                equal_var = TRUE/FALSE,                               # assume 2-sample test has equal population variances; default is TRUE
                random_state = 0,
                alternative = 'two-sided/less/greater')               # default is ‘two-sided’

# Test with sample with identical means
stats.ttest_ind(random_variables_arr1, 
                random_variables_arr2)
stats.ttest_ind(random_variables_arr1, 
                random_variables_arr2,
                equal_var = FALSE)








=================================================

Reduce bias in dataset

>imblearn.under_sampling.RandomUnderSampler
https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html
Under-sample the majority class(es) by randomly picking samples with or without replacement.    => consideration: you will lose those 'extra' observations you are 'throwing away'








