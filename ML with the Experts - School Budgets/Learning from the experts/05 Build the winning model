#part 1 Build the winning model
#You have arrived! This is where all of your hard work pays off. It's time to build the model that won DrivenData's competition.
#You've constructed a robust, powerful pipeline capable of processing training and testing data. Now that you understand the data and know all of the tools you need, you can essentially solve the whole problem in a relatively small number of lines of code. Wow!
#All you need to do is add the HashingVectorizer step to the pipeline to replace the CountVectorizer step.
#The parameters non_negative=True, norm=None, and binary=False make the HashingVectorizer perform similarly to the default settings on the CountVectorizer so you can just replace one with the other.



#Import HashingVectorizer from sklearn.feature_extraction.text.
# Import the hashing vectorizer
from sklearn.feature_extraction.text import HashingVectorizer

#Add a HashingVectorizer step to the pipeline.
#Name the step 'vectorizer'.
#Use the TOKENS_ALPHANUMERIC token pattern.
#Specify the ngram_range to be (1, 2)
# Instantiate the winning model pipeline: pl
pl = Pipeline([
        ('union', FeatureUnion(
            transformer_list = [
                ('numeric_features', Pipeline([
                    ('selector', get_numeric_data),
                    ('imputer', Imputer())
                ])),
                ('text_features', Pipeline([
                    ('selector', get_text_data),
                    ('vectorizer', HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC,
                                                     non_negative=True, norm=None, binary=False,
                                                     ngram_range=(1,2))),
                    ('dim_red', SelectKBest(chi2, chi_k))
                ]))
             ]
        )),
        ('int', SparseInteractions(degree=2)),
        ('scale', MaxAbsScaler()),
        ('clf', OneVsRestClassifier(LogisticRegression()))
    ])


#Log loss: 1.2258. Looks like the performance is about the same, but this is expected since the HashingVectorizer should work the same as the CountVectorizer. Try this pipeline out on the whole dataset on your local machine to see its full power!



#part 2
#What tactics got the winner the best score?
#Now you've implemented the winning model from start to finish. If you want to use this model locally, this Jupyter notebook contains all the code you've worked so hard on. You can now take that code and build on it!
#Let's take a moment to reflect on why this model did so well. What tactics got the winner the best score?
#https://github.com/datacamp/course-resources-ml-with-experts-budgets/blob/master/notebooks/1.0-full-model.ipynb

#The winner used skillful NLP, efficient computation, and simple but powerful stats tricks to master the budget data.
#Often times simpler is better, and understanding the problem in depth leads to simpler solutions!


