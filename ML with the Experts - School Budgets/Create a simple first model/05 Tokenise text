#part 1
#Tokenizing text
#As we talked about in the video, tokenization is the process of chopping up a character sequence into pieces called tokens.
#How do we determine what constitutes a token? Often, tokens are separated by whitespace. But we can specify other delimiters as well. For example, if we decided to tokenize on punctuation, then any punctuation mark would be treated like a whitespace. How we tokenize text in our DataFrame can affect the statistics we use in our model.
#A particular cell in our budget DataFrame may have the string content Title I - Disadvantaged Children/Targeted Assistance. The number of n-grams generated by this text data is sensitive to whether or not we tokenize on punctuation, as you'll show in the following exercise.

#How many tokens (1-grams) are in the string
#Title I - Disadvantaged Children/Targeted Assistance
#if we tokenize on punctuation?

#6
#Yes! Tokenizing on punctuation means that Children/Targeted becomes two tokens and - is dropped altogether. Nice work!


#part 2
#Testing your NLP credentials with n-grams
#You're well on your way to NLP superiority. Let's test your mastery of n-grams!
#In the workspace, we have the loaded a python list, one_grams, which contains all 1-grams of the string petro-vend fuel and fluids, tokenized on punctuation. Specifically,
#one_grams = ['petro', 'vend', 'fuel', 'and', 'fluids']

#In this exercise, your job is to determine the sum of the sizes of 1-grams, 2-grams and 3-grams generated by the string petro-vend fuel and fluids, tokenized on punctuation.
#Recall that the n-gram of a sequence consists of all ordered subsequences of length n.

#1-gram = 5
#2-gram = 4
#3-gram = 3
#sum of 1 to 3-grams = 5+4+3 = 12
#Bingo! The number of 1-grams + 2-grams + 3-grams is 5 + 4 + 3 = 12. NLP champion!

