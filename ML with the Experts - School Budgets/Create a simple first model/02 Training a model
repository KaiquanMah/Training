#With split data in hand, you're only a few lines away from training a model.
#In this exercise, you will import the logistic regression and one versus rest classifiers in order to fit a multi-class logistic regression model to the NUMERIC_COLUMNS of your feature data.
#Then you'll test and print the accuracy with the .score() method to see the results of training.
#Before you train! Remember, we're ultimately going to be using logloss to score our model, so don't worry too much about the accuracy here. Keep in mind that you're throwing away all of the text data in the dataset - that's by far most of the data! So don't get your hopes up for a killer performance just yet. We're just interested in getting things up and running at the moment.
#All data necessary to call multilabel_train_test_split() has been loaded into the workspace.


# Import classifiers
#Import LogisticRegression from sklearn.linear_model and OneVsRestClassifier from sklearn.multiclass.
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier

# Create the DataFrame: numeric_data_only
numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)

# Get labels and convert to dummy variables: label_dummies
label_dummies = pd.get_dummies(df[LABELS])

# Create training and test sets
X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only,
                                                               label_dummies,
                                                               size=0.2, 
                                                               seed=123)

# Instantiate the classifier: clf
#Instantiate the classifier clf by placing LogisticRegression() inside OneVsRestClassifier().
clf = OneVsRestClassifier(LogisticRegression())

# Fit the classifier to the training data
clf.fit(X_train, y_train)

# Print the accuracy
#Compute and print the accuracy of the classifier using its .score() method, which accepts two arguments: X_test and y_test.
print("Accuracy: {}".format(clf.score(X_test, y_test)))

#accuracy: 0.0
#The good news is that your workflow didn't cause any errors. The bad news is that your model scored the lowest possible accuracy: 0.0! But hey, you just threw away ALL of the text data in the budget. Later, you won't. Before you add the text data, let's see how the model does when scored by log loss.
