Snowpipe vs Snowpipe Streaming
Snowpipe ingests data from files (usually stored in cloud storage) as they land, using EVENT-BASED triggers.
Snowpipe STREAMING ingests data directly from an application in REAL-TIME, row-by-row, using an SDK, enabling sub-second latency.


Snowflake CANNOT AUTOmatically DETECT SCHEMA CHANGES IN my INCOMING DATA
- implement schema evolution strategies manually or 
- use with tools that support it
- For semi-structured data like JSON, you can use 'variant' columns to accommodate dynamic structures


How do I monitor whether my data ingestion is working correctly?
Snowflake provides several views
- LOAD_HISTORY
- PIPE_USAGE_HISTORY
- TASK_HISTORY 
to track ingestion events, errors, and performance. 
You can also integrate with observability tools for alerts and dashboards.


What file formats are supported by Snowflake for ingestion?
CSV, JSON, XML, Avro, ORC, Parquet
You define how the file should be interpreted using a file format object.


Can I ingest data into Snowflake from sources other than cloud storage or Kafka?
Yes. You can ingest data using 
- third-party tools (e.g., Fivetran, Matillion, Airbyte)
- Snowflakeâ€™s REST API
- Snowpipe Streaming
- custom scripts using connectors or SDKs

