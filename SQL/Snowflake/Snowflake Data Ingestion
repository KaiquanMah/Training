Snowpipe vs Snowpipe Streaming
Snowpipe ingests data from files (usually stored in cloud storage) as they land, using EVENT-BASED triggers.
Snowpipe STREAMING ingests data directly from an application in REAL-TIME, row-by-row, using an SDK, enabling sub-second latency.


Snowflake CANNOT AUTOmatically DETECT SCHEMA CHANGES IN my INCOMING DATA
- implement schema evolution strategies manually or 
- use with tools that support it
- For semi-structured data like JSON, you can use 'variant' columns to accommodate dynamic structures


How do I monitor whether my data ingestion is working correctly?
Snowflake provides several views
- LOAD_HISTORY
- PIPE_USAGE_HISTORY
- TASK_HISTORY 
to track ingestion events, errors, and performance. 
You can also integrate with observability tools for alerts and dashboards.


What file formats are supported by Snowflake for ingestion?
CSV, JSON, XML, Avro, ORC, Parquet
You define how the file should be interpreted using a file format object.


Can I ingest data into Snowflake from sources other than cloud storage or Kafka?
Yes. You can ingest data using 
- third-party tools (e.g., Fivetran, Matillion, Airbyte)
- Snowflakeâ€™s REST API
- Snowpipe Streaming
- custom scripts using connectors or SDKs






--CREATE STAGE for your files
/* specify the file format */
CREATE FILE FORMAT my_csv_format TYPE = 'CSV' 
/* this will help if there are double quotes or apostrophes in your data */
FIELD_OPTIONALLY_ENCLOSED_BY='"';

/* Stage the data using the credentials you have */
CREATE STAGE my_stage URL='s3://my-bucket/data/' CREDENTIALS=(AWS_KEY_ID='...' AWS_SECRET_KEY='...');


--COPY from STAGE INTO table
COPY INTO my_table FROM @my_stage FILE_FORMAT = (FORMAT_NAME = 'my_csv_format');
SELECT * FROM my_table;



