Having seen an example of l1 regularization in the video, you'll now vary the l2 regularization penalty - also known as "lambda" - and see its effect on overall model performance on the Ames housing dataset.

# Create the DMatrix: housing_dmatrix
housing_dmatrix = xgb.DMatrix(data=X, label=y)

reg_params = [1, 10, 100]


#Create an initial parameter dictionary specifying an "objective" of "reg:linear" and "max_depth" of 3.
# Create the initial parameter dictionary for varying l2 strength: params
params = {"objective":"reg:linear","max_depth":3}

# Create an empty list for storing rmses as a function of l2 complexity
rmses_l2 = []

# Iterate over reg_params
for reg in reg_params:

    # Update l2 strength
    params["lambda"] = reg
    
    
    #Use xgb.cv() inside of a for loop and systematically vary the "lambda" value by passing in the current l2 value (reg).
    # Pass this updated param dictionary into cv
    cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2, num_boost_round=5, metrics="rmse", as_pandas=True, seed=123)
    
    
    #Append the "test-rmse-mean" from the last boosting round for each cross-validated xgboost model.
    # Append best rmse (final round) to rmses_l2
    rmses_l2.append(cv_results_rmse["test-rmse-mean"].tail(1).values[0])

# Look at best rmse per l2 param
print("Best rmse as a function of l2:")
print(pd.DataFrame(list(zip(reg_params, rmses_l2)), columns=["l2", "rmse"]))




    Best rmse as a function of l2:
        l2          rmse
    0    1  52275.355468
    1   10  57746.064453
    2  100  76624.628907
Nice work! It looks like as as the value of 'lambda' increases, so does the RMSE.

