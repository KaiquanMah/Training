Build Out An ALS Model
Let's specify your first ALS model. Complete the code below to build your first ALS model.

Recall that you can use the .columns method on the ratings data frame to see what the names of the columns are that contain user, movie, and ratings data. Spark needs to know the names of these columns in order to perform ALS correctly.


In [1]: ratings.columns
Out[1]: ['userId', 'movieId', 'rating']


#Before building our ALS model, we need to split the data into training data and test data. Use the randomSplit() method to split the ratings dataframe into training_data and test_data using an 0.8/0.2 split respectively and a seed for the random number generator of 1234.
# Split the ratings dataframe into training and test data
(training_data, test_data) = ratings.randomSplit([0.8, 0.2], seed=1234)



#Tell Spark which columns contain the userCol, itemCol and ratingCol. Use the .columns method if needed. Complete the hyperparameters. Set the rank to 10, the maxIter to 15, the regParam or lambda to .1, the coldStartStrategy to "drop", the nonnegative argument should be set to True, and since our data contains explicit ratings, set the implicitPrefs argument to False.
# Set the ALS hyperparameters
from pyspark.ml.recommendation import ALS
als = ALS(userCol="userId", 
        itemCol="movieId", 
        ratingCol="rating", 
        rank =10, 
        maxIter =15, 
        regParam =0.1,
          coldStartStrategy="drop", 
          nonnegative =True, 
          implicitPrefs = False)


# Fit the model to the training_data
model = als.fit(training_data)

# Generate predictions on the test_data
test_predictions = model.transform(test_data)
test_predictions.show()


<script.py> output:
    +------+-------+------+----------+
    |userId|movieId|rating|prediction|
    +------+-------+------+----------+
    +------+-------+------+----------+

You just built our your first ALS model and generated some test predictions. It's a toy dataset, so the results aren't particularly meaningful, but you now know how to do this.















Build RMSE Evaluator
Now that you know how to fit a model to training data and generate test predictions, you need a way to evaluate how well your model performs. For this we'll build an evaluator. Evaluators in Spark can be built out in various ways. For our purposes, we want a regressionEvaluator that calculates the RMSE. After we build our regressionEvaluator, we can fit the model to our data and generate predictions.



#Import the required RegressionEvaluator package from the pyspark.ml.evaluation class.
# Import RegressionEvaluator
from pyspark.ml.evaluation import RegressionEvaluator

#Complete the evaluator code, specifying the metric name to be "rmse". Set the labelCol to the name of the column in our ratings data that contains our ratings (use the ratings.columns method to see column names) and set the prediction column name to "prediction".
# Complete the evaluator code
evaluator = RegressionEvaluator(metricName="rmse", labelCol="rating", predictionCol="prediction")

# Extract the 3 parameters
print(evaluator.getMetricName())
print(evaluator.getLabelCol())
print(evaluator.getPredictionCol())
<script.py> output:
    rmse
    rating
    prediction












Get RMSE
Now that you know how to build a model and generate predictions, and have an evaluator to tell us how well it predicts ratings, we can calculate the RMSE to see how well an ALS model performed. We'll use the evaluator that we built in the previous exercise to calculate and print the rmse.#Call the .evaluate() method on our evaluator to calculate our RMSE on the test_predictions dataframe. Call the result RMSE.

# Evaluate the "test_predictions" dataframe
RMSE = evaluator.evaluate(test_predictions)

# Print the RMSE
print (RMSE)
    0.16853197489754093

This RMSE means that on average, the model's test predictions are about .16 off from the true values.
