Let's take a look at a new dataset - this one is a bit less-clean than what you've seen before.

As always, you'll first start by visualizing the raw data. Take a close look and try to find datapoints that could be problematic for fitting models.

The data has been loaded into a DataFrame called prices.



# Visualize the dataset
prices.plot(legend=False)
plt.tight_layout()
plt.show()

# Count the missing values of each time series
missing_values = prices.isna().sum()
print(missing_values)
<script.py> output:
    symbol
    EBAY    273
    NVDA    502
    YHOO    232
    dtype: int64





prices
symbol           EBAY        NVDA       YHOO
date                                        
2010-01-04  23.900000   18.490000  17.100000
2010-01-05  23.650000   18.760000  17.230000
2010-01-06  23.500000   18.879999  17.170000
2010-01-07  23.229998   18.510000  16.700001
2010-01-08  23.509999   18.549999  16.700001
2010-01-11  23.450000   18.290001  16.740000
2010-01-12  23.190000   17.670000  16.680000
2010-01-13  23.600000   17.910000  16.900000
2010-01-14  22.880000   17.629999  17.120001
2010-01-15  22.469998   17.110001  16.820000
2010-01-19  23.259999   17.430000  16.750000
2010-01-20  22.230000   17.360001  16.379999
2010-01-21  24.129998   17.049999  16.200001
2010-01-22  23.579998   16.459999  15.880000
2010-01-25  23.690000   16.740000  15.860000
2010-01-26  24.069999   16.209999  15.990000
2010-01-27  23.969999   16.650000  15.980000
2010-01-28  23.329998   16.090000  15.440000
2010-01-29  23.019999   15.390000  15.010000
2010-02-01  23.169999   16.570000  15.050000
2010-02-02  23.079998   16.740000  15.170000
2010-02-03  23.229998   16.879999  15.460000
2010-02-04  22.449999   15.900000  15.010000
2010-02-05  22.709999   16.219999  15.190000
2010-02-08  22.490000   16.190001  14.990000
2010-02-09  22.369998   16.059999  15.070000
2010-02-10  21.959999   16.370001  14.800000
2010-02-11  22.130000   17.120001  15.220000
2010-02-12  21.769998   17.350000  15.170000
2010-02-16  22.519998   17.670000  15.410000
...               ...         ...        ...
2016-11-17  28.870001   92.389999  41.450001
2016-11-18  28.690001   93.360001  41.189999
2016-11-21  29.000000   92.980003  41.110001
2016-11-22  29.059999   93.650002  41.009998
2016-11-23  28.900000   93.970001  40.959999
2016-11-25  28.950001   94.160004  40.869999
2016-11-28  28.570000   94.110001  41.450001
2016-11-29  28.510000   93.250000  41.599998
2016-11-30  27.809999   92.199997  41.020000
2016-12-01  27.389999   87.639999  39.630001
2016-12-02  28.420000   88.449997  40.070000
2016-12-05  28.350000   91.879997  40.200001
2016-12-06  28.209999   93.389999  39.970001
2016-12-07  28.910000   95.070000  40.520000
2016-12-08  29.420000   93.480003  41.410000
2016-12-09  29.969999   91.820000  41.759998
2016-12-12  30.209999   89.589996  41.299999
2016-12-13  29.820000   91.169998  41.470001
2016-12-14  29.820000   96.449997  40.910000
2016-12-15  29.730000   98.709999  38.410000
2016-12-16  29.610001  100.410004  38.610001
2016-12-19  29.379999  101.629997  38.419998
2016-12-20  29.260000  105.169998  39.160000
2016-12-21  29.360001  105.830002  39.150002
2016-12-22  29.540001  107.110001  38.500000
2016-12-23  29.790001  109.779999  38.660000
2016-12-27  30.240000  117.320000  38.919998
2016-12-28  30.010000  109.250000  38.730000
2016-12-29  29.980000  111.430000  38.639999
2016-12-30  29.690001  106.739998  38.669998

[1762 rows x 3 columns]



In the plot, you can see there are clearly missing chunks of time in your data. There also seem to be a few 'jumps' in the data. How can you deal with this?
