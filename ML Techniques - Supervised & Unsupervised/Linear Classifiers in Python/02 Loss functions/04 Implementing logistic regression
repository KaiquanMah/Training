This is very similar to the earlier exercise where you implemented linear regression "from scratch" using scipy.optimize.minimize. However, this time we'll minimize the logistic loss and compare with scikit-learn's LogisticRegression (we've set C to a large value to disable regularization; more on this in Chapter 3!).

The log_loss() function from the previous exercise is already defined in your environment, and the sklearn breast cancer prediction dataset (first 10 features, standardized) is loaded into the variables X and y.





# The logistic loss, summed over training examples
def my_loss(w):
    s = 0
    
    #range(___)
    #There are several ways to get the number of training examples, such as y.size, len(y), or len(X).
    for i in range(y.size):
        raw_model_output = w@X[i]
        s = s + log_loss(raw_model_output * y[i])
    return s

# Returns the w that makes my_loss(w) smallest
w_fit = minimize(my_loss, X[0]).x
print(w_fit)

    [ 1.03592182 -1.65378492  4.08331342 -9.40923002 -1.06786489  0.07892114
     -0.85110344 -2.44103305 -0.45285671  0.43353448]
     
     
     
     
     
# Compare with scikit-learn's LogisticRegression
lr = LogisticRegression(fit_intercept=False, C=1000000).fit(X,y)
print(lr.coef_)



    [[ 1.03731085 -1.65339037  4.08143924 -9.40788356 -1.06757746  0.07895582
      -0.85072003 -2.44079089 -0.45271     0.43334997]]
      
      
      
      

Great job! As you can see, logistic regression is just minimizing the loss function we've been looking at. Much more on logistic regression in the next chapter!

