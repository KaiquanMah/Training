Standardizing data
Some models, like K-nearest neighbors (KNN) & neural networks, work better with scaled data -- so we'll standardize our data.

We'll also remove unimportant variables (day of week), according to feature importances, by indexing the features DataFrames with .iloc[]. KNN uses distances to find similar points for predictions, so big features outweigh small ones. Scaling data fixes that.

sklearn's scale() will standardize data, which sets the mean to 0 and standard deviation to 1. Ideally we'd want to use StandardScaler with fit_transform() on the training data, and fit() on the test data, but we are limited to 15 lines of code here.

Once we've scaled the data, we'll check that it worked by plotting histograms of the data.



from sklearn.preprocessing import scale

# Remove unimportant features (weekdays) in the last 4 cols
train_features = train_features.iloc[:, :-4]
test_features = test_features.iloc[:, :-4]

# Standardize the train and test features
scaled_train_features = scale(train_features)
scaled_test_features = scale(test_features)

# Plot histograms of the 14-day SMA RSI before and after scaling
f, ax = plt.subplots(nrows=2, ncols=1)
train_features.iloc[:, 2].hist(ax=ax[0])
ax[1].hist(scaled_train_features[:, 2])
plt.show()

Next we're going to optimize n_neighbors for improved performance.














Optimize n_neighbors
Now that we have scaled data, we can try using a KNN model. To maximize performance, we should tune our model's hyperparameters. For the k-nearest neighbors algorithm, we only have one hyperparameter: n, the number of neighbors. We set this hyperparameter when we create the model with KNeighborsRegressor. The argument for the number of neighbors is n_neighbors.

We want to try a range of values that passes through the setting with the best performance. Usually we will start with 2 neighbors, and increase until our scoring metric starts to decrease. We'll use the R2 value from the .score() method on the test set (scaled_test_features and test_targets) to optimize n here. We'll use the test set scores to determine the best n.





from sklearn.neighbors import KNeighborsRegressor

for n in range(2,13,1):
    # Create and fit the KNN model
    knn = KNeighborsRegressor(n_neighbors=n)
    
    # Fit the model to the training data
    knn.fit(scaled_train_features, train_targets)
    
    # Print number of neighbors and the score to find the best value of n
    print("n_neighbors =", n)
    print('train, test scores')
    print(knn.score(scaled_train_features, train_targets))
    print(knn.score(scaled_test_features, test_targets))
    print()  # prints a blank line






<script.py> output:
    n_neighbors = 2
    train, test scores
    0.7086590518110245
    -0.24570512723742513
    
    n_neighbors = 3
    train, test scores
    0.6161299695003466
    -0.028247987527901145
    
    n_neighbors = 4
    train, test scores
    0.5698590844708643
    0.05406963728898184
    
    n_neighbors = 5
    train, test scores
    0.5306669823361658
    0.09562673296186885
    
    n_neighbors = 6
    train, test scores
    0.4924157634083257
    0.06493624818165344
    
    n_neighbors = 7
    train, test scores
    0.4638477451246331
    0.018590670460287284
    
    n_neighbors = 8
    train, test scores
    0.4305841272960338
    0.03430514088288383
    
    n_neighbors = 9
    train, test scores
    0.39035273655318137
    -0.05499368593229881
    
    n_neighbors = 10
    train, test scores
    0.3585431044577594
    -0.04569165134882858
    
    n_neighbors = 11
    train, test scores
    0.31002206869733073
    -0.08074815512838707
    
    n_neighbors = 12
    train, test scores
    0.2742940406863563
    -0.07302787030122526


#See how 5 is the best number of neighbors based on the test scores?















Evaluate KNN performance
We just saw a few things with our KNN scores. For one, the training scores started high and decreased with increasing n, which is typical. The test set performance reached a peak at 5 though, and we will use that as our setting in the final KNN model.

As we have done a few times now, we will check our performance visually. This helps us see how well the model is predicting on different regions of actual values. We will get predictions from our knn model using the .predict() method on our scaled features. Then we'll use matplotlib's plt.scatter() to create a scatter plot of actual versus predicted values.


# Create the model with the best-performing n_neighbors of 5
knn = KNeighborsRegressor(n_neighbors=5)

# Fit the model
knn.fit(scaled_train_features, train_targets)

# Get predictions for train and test sets
train_predictions = knn.predict(scaled_train_features)
test_predictions = knn.predict(scaled_test_features)

# Plot the actual vs predicted values
plt.scatter(train_predictions, train_targets, label='train')
plt.scatter(test_predictions,test_targets, label='test')
plt.legend()
plt.show()
