Gradient accumulation with Trainer
You're setting up Trainer for your language translation model to use gradient accumulation, so that you can effectively train on larger batches. Your model will simplify translations by training on paraphrases from the MRPC dataset. Configure the training arguments to accumulate gradients! The exercise will take some time to run with the call to trainer.train().
The model, dataset, and compute_metrics() function have been pre-defined.


Set the number of gradient accumulation steps to two.
Pass in the training arguments to Trainer.


training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    # Set the number of gradient accumulation steps to two
    gradient_accumulation_steps=2
)
trainer = Trainer(
    model=model,
    # Pass in the training arguments to Trainer
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    compute_metrics=compute_metrics,
)
trainer.train()


<script.py> output:
    {'eval_loss': 0.7681589722633362, 'eval_accuracy': 0.3333333333333333, 'eval_f1': 0.5, 'eval_runtime': 1.552, 'eval_samples_per_second': 1.933, 'eval_steps_per_second': 0.644, 'epoch': 1.0}
    {'eval_loss': 0.7908749580383301, 'eval_accuracy': 0.3333333333333333, 'eval_f1': 0.5, 'eval_runtime': 1.4744, 'eval_samples_per_second': 2.035, 'eval_steps_per_second': 0.678, 'epoch': 2.0}
    {'eval_loss': 0.8041989803314209, 'eval_accuracy': 0.3333333333333333, 'eval_f1': 0.5, 'eval_runtime': 1.2894, 'eval_samples_per_second': 2.327, 'eval_steps_per_second': 0.776, 'epoch': 3.0}
    {'train_runtime': 20.8874, 'train_samples_per_second': 0.431, 'train_steps_per_second': 0.144, 'train_loss': 0.2711409529050191, 'epoch': 3.0}


