Set up the Trainer
Your model will replace complex phrases with simpler, more accessible paraphrases to improve the readability of translations. For example, it can simplify the original phrase "The inclement weather conditions precipitated the postponement of the outdoor event" with "The bad weather caused the outdoor event to be delayed." Build the Trainer to prepare training your language translation service! The exercise will take some time to run with the call to trainer.train().

Some data has been pre-loaded:
model is a Transformer model
dataset contains the MRPC dataset of sentence paraphrases
compute_metrics() returns accuracy and F1 score
You've defined training_args in a prior exercise



Pass in the model to the Trainer() class.
Input the training arguments in the Trainer() class.
Pass in a function to compute metrics to the Trainer() class.
Print the device that the trainer chooses


trainer = Trainer(
    # Pass in the model
    model=model,
    # Input the training arguments
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    # Pass in a function to compute metrics
    compute_metrics=compute_metrics,
)

trainer.train()

# Print the device that the trainer chooses
print(trainer.args.device)



<script.py> output:
    {'eval_loss': 0.7015256881713867, 'eval_accuracy': 0.3333333333333333, 'eval_f1': 0.5, 'eval_runtime': 1.2614, 'eval_samples_per_second': 2.378, 'eval_steps_per_second': 0.793, 'epoch': 1.0}
    {'eval_loss': 0.708066999912262, 'eval_accuracy': 0.3333333333333333, 'eval_f1': 0.5, 'eval_runtime': 1.3042, 'eval_samples_per_second': 2.3, 'eval_steps_per_second': 0.767, 'epoch': 2.0}
    {'train_runtime': 19.2163, 'train_samples_per_second': 0.312, 'train_steps_per_second': 0.104, 'train_loss': 0.6703586578369141, 'epoch': 2.0}
    cpu
