Set up the 8-bit Adam optimizer
You're finding that training your Transformer model for real-time language translation isn't learning effectively with Adafactor. As an alternative, you decide to try an 8-bit Adam optimizer to reduce memory by approximately 75% compared to Adam.
The bitsandbytes library has been imported as bnb, TrainingArguments has been defined as args, and optimizer_grouped_parameters has been pre-loaded. Note the exercise prints a warning message about libbitsandbytes_cpu.so, but you can ignore this warning to complete the exercise.


Instantiate the 8-bit Adam optimizer from the bitsandbytes library.
Pass in the beta1 and beta2 parameters to the 8-bit Adam optimizer.
Pass in the epilson parameter to the 8-bit Adam optimizer.
Print the input parameters from the 8-bit Adam optimizer.


# Instantiate the 8-bit Adam optimizer
adam_bnb_optim = bnb.optim.Adam8bit(optimizer_grouped_parameters,
                                # Pass in the beta1 and beta2 parameters
                                betas=(args.adam_beta1, args.adam_beta2),
                                # Pass in the epilson parameter
                                eps=args.adam_epsilon,
                                lr=args.learning_rate)

# Print the input parameters
print(f"beta1 = {args.adam_beta1}")
print(f"beta2 = {args.adam_beta2}")
print(f"epsilon = {args.adam_epsilon}")
print(f"learning_rate = {args.learning_rate}")


<script.py> output:
    beta1 = 0.9
    beta2 = 0.999
    epsilon = 1e-08
    learning_rate = 5e-05

    
