AdamW with Trainer
You're beginning to train a Transformer model to simplify language translations. As a first step, you decide to use the AdamW optimizer as a benchmark and the Trainer interface for quick setup. Set up Trainer to use the AdamW optimizer.
AdamW has been pre-imported from torch.optim. Some training objects have been pre-loaded: model, training_args, train_dataset, validation_dataset, compute_metrics.


Pass the model parameters to the AdamW optimizer.
Pass the optimizer to Trainer.

# Pass the model parameters to the AdamW optimizer
optimizer = AdamW(params=model.parameters())

# Pass the optimizer to Trainer
trainer = Trainer(model=model,
                  args=training_args,
                  train_dataset=train_dataset,
                  eval_dataset=validation_dataset,
                  optimizers=(optimizer, None),
                  compute_metrics=compute_metrics)

trainer.train()


<script.py> output:
    {'eval_loss': 0.09045790135860443, 'eval_accuracy': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.7902, 'eval_samples_per_second': 1.265, 'eval_steps_per_second': 1.265, 'epoch': 1.0}
    {'eval_loss': 0.056325092911720276, 'eval_accuracy': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.6701, 'eval_samples_per_second': 1.492, 'eval_steps_per_second': 1.492, 'epoch': 2.0}
    {'eval_loss': 0.004764513578265905, 'eval_accuracy': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.6648, 'eval_samples_per_second': 1.504, 'eval_steps_per_second': 1.504, 'epoch': 3.0}
    {'train_runtime': 8.9631, 'train_samples_per_second': 0.335, 'train_steps_per_second': 0.335, 'train_loss': 0.20651914676030478, 'epoch': 3.0}



