Logging evaluation metrics
Tracking performance metrics allows you to monitor degradations, and you can make decisions about when to update your model to maintain a high level of accuracy. You decide you will log metrics after your model finishes an evaluation loop.

Some data has been pre-loaded:
accelerator is an instance of Accelerator
eval_metric is a dictionary of metrics like accuracy and f1
num_epochs is the number of epochs


Call a method to log evaluation metrics of the model.
Log "accuracy" and "f1" score as evaluation metrics.
Track the epoch number using epoch of the training loop.



accelerator = Accelerator(project_dir=".", log_with="all")
accelerator.init_trackers("my_project")

for epoch in range(num_epochs):
    # Training loop is here
    # Evaluation loop is here
    # Call a method to log metrics
    accelerator.log({
        # Log accuracy and F1 score as metrics
        "accuracy": eval_metric["accuracy"],
        "f1": eval_metric["f1"],
    # Track the epoch number
    }, step=epoch)

accelerator.end_training()




