Prepare datasets for distributed training
You've preprocessed a dataset for a precision agriculture system to help farmers monitor crop health. Now you'll load the data by creating a DataLoader and place the data on GPUs for distributed training, if GPUs are available. Note the exercise actually uses a CPU, but the code is the same for CPUs and GPUs.

Some data has been pre-loaded:
A sample dataset with agricultural imagery
The Accelerator class from the accelerate library
The DataLoader class




Initialize an accelerator object.
Create a dataloader for the pre-defined dataset.
Place the dataloader on available GPUs.
Print the device that Accelerator selected.




# Initialize an accelerator object
accelerator = Accelerator()

# Create a dataloader for the pre-defined dataset
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

# Place the dataloader on available GPUs
dataloader = accelerator.prepare(dataloader)

# Print the device that Accelerator selected
print(accelerator.device)



<script.py> output:
    cpu


