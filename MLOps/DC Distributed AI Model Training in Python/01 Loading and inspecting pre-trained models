Loading and inspecting pre-trained models
You're building a conversational AI assistant that can engage in human-like dialog across a wide range of topics, leveraging the powerful BERT model that has been pre-trained on a large corpus of text data.
You'll print the configuration to verify that you've loaded a conversational AI model with certain parameters like model_type: bert, num_attention_heads: 12, and num_hidden_layers: 12.


Load a pre-trained bert-base-uncased model.
Print the model's parameters and configuration


from transformers import AutoModelForSequenceClassification

# Load a pre-trained bert-base-uncased model
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased")

# Print the model's parameters and configuration
print(model.config)



<script.py> output:
    BertConfig {
      "_attn_implementation_autoset": true,
      "_name_or_path": "bert-base-uncased",
      "architectures": [
        "BertForMaskedLM"
      ],
      "attention_probs_dropout_prob": 0.1,
      "classifier_dropout": null,
      "gradient_checkpointing": false,
      "hidden_act": "gelu",
      "hidden_dropout_prob": 0.1,
      "hidden_size": 768,
      "initializer_range": 0.02,
      "intermediate_size": 3072,
      "layer_norm_eps": 1e-12,
      "max_position_embeddings": 512,
      "model_type": "bert",
      "num_attention_heads": 12,
      "num_hidden_layers": 12,
      "pad_token_id": 0,
      "position_embedding_type": "absolute",
      "transformers_version": "4.46.2",
      "type_vocab_size": 2,
      "use_cache": true,
      "vocab_size": 30522
    }



