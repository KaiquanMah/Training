Cross-validation scoring
Now, you should use cross-validation scoring with cross_val_score() to check the overall performance.
This is exercise presents an excellent opportunity to test out the use of the hyperparameters learning_rate and max_depth. Remember, hyperparameters are like settings which can help create optimum performance.
The data sets cr_loan_prep, X_train, and y_train have already been loaded in the workspace.

Create a gradient boosted tree with a learning rate of 0.1 and a max depth of 7. Store the model as gbt.
Calculate the cross validation scores against the X_train and y_train data sets with 4 folds. Store the results as cv_scores.
Print the cross validation scores.
Print the average accuracy score and standard deviation with formatting.

# Create a gradient boosted tree model using two hyperparameters
gbt = xgb.XGBClassifier(learning_rate = 0.1, max_depth = 7)

# Calculate the cross validation scores for 4 folds
cv_scores = cross_val_score(gbt, X_train, np.ravel(y_train), cv = 4)

# Print the cross validation scores
print(cv_scores)
<script.py> output:
    [0.94095023 0.93369541 0.93186962 0.92462653]


# Print the average accuracy and standard deviation of the scores
print("Average accuracy: %0.2f (+/- %0.2f)" % (cv_scores.mean(),
                                              cv_scores.std() * 2))
<script.py> output:
    Average accuracy: 0.93 (+/- 0.01)



With only a couple of hyperparameters and cross-validation, we can get the average accuracy up to 93%. This is a great way to validate how robust the model is.

